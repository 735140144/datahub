"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"overviewSidebar":[{"type":"category","label":"What is DataHub?","items":[{"type":"link","label":"Features","href":"/docs/features","docId":"docs/features"},{"type":"link","label":"Concepts","href":"/docs/what-is-datahub/datahub-concepts","docId":"docs/what-is-datahub/datahub-concepts"},{"type":"link","label":"See Datahub In Action","href":"https://demo.datahubproject.io/"},{"type":"category","label":"Architecture","items":[{"type":"link","label":"Overview","href":"/docs/architecture/architecture","docId":"docs/architecture/architecture"},{"type":"link","label":"Components","href":"/docs/components","docId":"docs/components"},{"type":"link","label":"Ingestion Framework","href":"/docs/architecture/metadata-ingestion","docId":"docs/architecture/metadata-ingestion"},{"type":"link","label":"Serving Tier","href":"/docs/architecture/metadata-serving","docId":"docs/architecture/metadata-serving"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Managed DataHub","href":"/docs/saas","docId":"docs/saas"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Get Started","collapsed":false,"items":[{"type":"category","label":"Self-Hosted DataHub","items":[{"type":"link","label":"Quickstart Guide","href":"/docs/quickstart","docId":"docs/quickstart"},{"type":"link","label":"Onboarding Users to DataHub","href":"/docs/authentication/guides/add-users","docId":"docs/authentication/guides/add-users"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Managed DataHub","items":[{"type":"link","label":"Onboarding Users to DataHub","href":"/docs/authentication/guides/add-users","docId":"docs/authentication/guides/add-users"},{"type":"link","label":"Configure Slack Notifications","href":"/docs/managed-datahub/saas-slack-setup","className":"saasOnly","docId":"docs/managed-datahub/saas-slack-setup"},{"type":"link","label":"Approval Workflows","href":"/docs/managed-datahub/approval-workflows","className":"saasOnly","docId":"docs/managed-datahub/approval-workflows"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Ingestion Quickstart Guides","items":[{"type":"category","label":"BigQuery","items":[{"type":"link","label":"Overview","href":"/docs/quick-ingestion-guides/bigquery/overview","docId":"docs/quick-ingestion-guides/bigquery/overview"},{"type":"link","label":"Setup","href":"/docs/quick-ingestion-guides/bigquery/setup","docId":"docs/quick-ingestion-guides/bigquery/setup"},{"type":"link","label":"Configuration","href":"/docs/quick-ingestion-guides/bigquery/configuration","docId":"docs/quick-ingestion-guides/bigquery/configuration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Redshift","items":[{"type":"link","label":"Overview","href":"/docs/quick-ingestion-guides/redshift/overview","docId":"docs/quick-ingestion-guides/redshift/overview"},{"type":"link","label":"Setup","href":"/docs/quick-ingestion-guides/redshift/setup","docId":"docs/quick-ingestion-guides/redshift/setup"},{"type":"link","label":"Configuration","href":"/docs/quick-ingestion-guides/redshift/configuration","docId":"docs/quick-ingestion-guides/redshift/configuration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Snowflake","items":[{"type":"link","label":"Overview","href":"/docs/quick-ingestion-guides/snowflake/overview","docId":"docs/quick-ingestion-guides/snowflake/overview"},{"type":"link","label":"Setup","href":"/docs/quick-ingestion-guides/snowflake/setup","docId":"docs/quick-ingestion-guides/snowflake/setup"},{"type":"link","label":"Configuration","href":"/docs/quick-ingestion-guides/snowflake/configuration","docId":"docs/quick-ingestion-guides/snowflake/configuration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Tableau","items":[{"type":"link","label":"Overview","href":"/docs/quick-ingestion-guides/tableau/overview","docId":"docs/quick-ingestion-guides/tableau/overview"},{"type":"link","label":"Setup","href":"/docs/quick-ingestion-guides/tableau/setup","docId":"docs/quick-ingestion-guides/tableau/setup"},{"type":"link","label":"Configuration","href":"/docs/quick-ingestion-guides/tableau/configuration","docId":"docs/quick-ingestion-guides/tableau/configuration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"PowerBI","items":[{"type":"link","label":"Overview","href":"/docs/quick-ingestion-guides/powerbi/overview","docId":"docs/quick-ingestion-guides/powerbi/overview"},{"type":"link","label":"Setup","href":"/docs/quick-ingestion-guides/powerbi/setup","docId":"docs/quick-ingestion-guides/powerbi/setup"},{"type":"link","label":"Configuration","href":"/docs/quick-ingestion-guides/powerbi/configuration","docId":"docs/quick-ingestion-guides/powerbi/configuration"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true}],"collapsible":true,"href":"/docs/get-started-with-datahub"},{"type":"category","label":"Ingest Metadata","items":[{"type":"category","label":"Overview","items":[{"type":"link","label":"Introduction to Metadata Ingestion","href":"/docs/metadata-ingestion","docId":"metadata-ingestion/README"},{"type":"link","label":"UI Ingestion Guide","href":"/docs/ui-ingestion","docId":"docs/ui-ingestion"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Sources","items":[{"type":"link","label":"Airflow","href":"/docs/lineage/airflow","docId":"docs/lineage/airflow"},{"type":"link","label":"Spark","href":"/docs/metadata-integration/java/spark-lineage","docId":"metadata-integration/java/spark-lineage/README"},{"type":"link","label":"Great Expectations","href":"/docs/metadata-ingestion/integration_docs/great-expectations","docId":"metadata-ingestion/integration_docs/great-expectations"},{"type":"link","label":"Protobuf Schemas","href":"/docs/metadata-integration/java/datahub-protobuf","docId":"metadata-integration/java/datahub-protobuf/README"},{"type":"link","label":"Athena","href":"/docs/generated/ingestion/sources/athena","docId":"docs/generated/ingestion/sources/athena"},{"type":"link","label":"Azure AD","href":"/docs/generated/ingestion/sources/azure-ad","docId":"docs/generated/ingestion/sources/azure-ad"},{"type":"link","label":"BigQuery","href":"/docs/generated/ingestion/sources/bigquery","docId":"docs/generated/ingestion/sources/bigquery"},{"type":"link","label":"Business Glossary","href":"/docs/generated/ingestion/sources/business-glossary","docId":"docs/generated/ingestion/sources/business-glossary"},{"type":"link","label":"ClickHouse","href":"/docs/generated/ingestion/sources/clickhouse","docId":"docs/generated/ingestion/sources/clickhouse"},{"type":"link","label":"CSV","href":"/docs/generated/ingestion/sources/csv","docId":"docs/generated/ingestion/sources/csv"},{"type":"link","label":"Databricks","href":"/docs/generated/ingestion/sources/databricks","docId":"docs/generated/ingestion/sources/databricks"},{"type":"link","label":"dbt","href":"/docs/generated/ingestion/sources/dbt","docId":"docs/generated/ingestion/sources/dbt"},{"type":"link","label":"Delta Lake","href":"/docs/generated/ingestion/sources/delta-lake","docId":"docs/generated/ingestion/sources/delta-lake"},{"type":"link","label":"Demo Data","href":"/docs/generated/ingestion/sources/demo-data","docId":"docs/generated/ingestion/sources/demo-data"},{"type":"link","label":"Druid","href":"/docs/generated/ingestion/sources/druid","docId":"docs/generated/ingestion/sources/druid"},{"type":"link","label":"Elasticsearch","href":"/docs/generated/ingestion/sources/elasticsearch","docId":"docs/generated/ingestion/sources/elasticsearch"},{"type":"link","label":"Feast","href":"/docs/generated/ingestion/sources/feast","docId":"docs/generated/ingestion/sources/feast"},{"type":"link","label":"File","href":"/docs/generated/ingestion/sources/file","docId":"docs/generated/ingestion/sources/file"},{"type":"link","label":"File Based Lineage","href":"/docs/generated/ingestion/sources/file-based-lineage","docId":"docs/generated/ingestion/sources/file-based-lineage"},{"type":"link","label":"Glue","href":"/docs/generated/ingestion/sources/glue","docId":"docs/generated/ingestion/sources/glue"},{"type":"link","label":"Hive","href":"/docs/generated/ingestion/sources/hive","docId":"docs/generated/ingestion/sources/hive"},{"type":"link","label":"Iceberg","href":"/docs/generated/ingestion/sources/iceberg","docId":"docs/generated/ingestion/sources/iceberg"},{"type":"link","label":"JSON Schemas","href":"/docs/generated/ingestion/sources/json-schema","docId":"docs/generated/ingestion/sources/json-schema"},{"type":"link","label":"Kafka","href":"/docs/generated/ingestion/sources/kafka","docId":"docs/generated/ingestion/sources/kafka"},{"type":"link","label":"Kafka Connect","href":"/docs/generated/ingestion/sources/kafka-connect","docId":"docs/generated/ingestion/sources/kafka-connect"},{"type":"link","label":"LDAP","href":"/docs/generated/ingestion/sources/ldap","docId":"docs/generated/ingestion/sources/ldap"},{"type":"link","label":"Looker","href":"/docs/generated/ingestion/sources/looker","docId":"docs/generated/ingestion/sources/looker"},{"type":"link","label":"MariaDB","href":"/docs/generated/ingestion/sources/mariadb","docId":"docs/generated/ingestion/sources/mariadb"},{"type":"link","label":"Metabase","href":"/docs/generated/ingestion/sources/metabase","docId":"docs/generated/ingestion/sources/metabase"},{"type":"link","label":"Microsoft SQL Server","href":"/docs/generated/ingestion/sources/mssql","docId":"docs/generated/ingestion/sources/mssql"},{"type":"link","label":"Mode","href":"/docs/generated/ingestion/sources/mode","docId":"docs/generated/ingestion/sources/mode"},{"type":"link","label":"MongoDB","href":"/docs/generated/ingestion/sources/mongodb","docId":"docs/generated/ingestion/sources/mongodb"},{"type":"link","label":"MySQL","href":"/docs/generated/ingestion/sources/mysql","docId":"docs/generated/ingestion/sources/mysql"},{"type":"link","label":"NiFi","href":"/docs/generated/ingestion/sources/nifi","docId":"docs/generated/ingestion/sources/nifi"},{"type":"link","label":"Okta","href":"/docs/generated/ingestion/sources/okta","docId":"docs/generated/ingestion/sources/okta"},{"type":"link","label":"OpenAPI","href":"/docs/generated/ingestion/sources/openapi","docId":"docs/generated/ingestion/sources/openapi"},{"type":"link","label":"Oracle","href":"/docs/generated/ingestion/sources/oracle","docId":"docs/generated/ingestion/sources/oracle"},{"type":"link","label":"Postgres","href":"/docs/generated/ingestion/sources/postgres","docId":"docs/generated/ingestion/sources/postgres"},{"type":"link","label":"PowerBI","href":"/docs/generated/ingestion/sources/powerbi","docId":"docs/generated/ingestion/sources/powerbi"},{"type":"link","label":"Presto","href":"/docs/generated/ingestion/sources/presto","docId":"docs/generated/ingestion/sources/presto"},{"type":"link","label":"Presto on Hive","href":"/docs/generated/ingestion/sources/presto-on-hive","docId":"docs/generated/ingestion/sources/presto-on-hive"},{"type":"link","label":"Pulsar","href":"/docs/generated/ingestion/sources/pulsar","docId":"docs/generated/ingestion/sources/pulsar"},{"type":"link","label":"Redash","href":"/docs/generated/ingestion/sources/redash","docId":"docs/generated/ingestion/sources/redash"},{"type":"link","label":"Redshift","href":"/docs/generated/ingestion/sources/redshift","docId":"docs/generated/ingestion/sources/redshift"},{"type":"link","label":"S3 Data Lake","href":"/docs/generated/ingestion/sources/s3","docId":"docs/generated/ingestion/sources/s3"},{"type":"link","label":"SageMaker","href":"/docs/generated/ingestion/sources/sagemaker","docId":"docs/generated/ingestion/sources/sagemaker"},{"type":"link","label":"Salesforce","href":"/docs/generated/ingestion/sources/salesforce","docId":"docs/generated/ingestion/sources/salesforce"},{"type":"link","label":"SAP HANA","href":"/docs/generated/ingestion/sources/hana","docId":"docs/generated/ingestion/sources/hana"},{"type":"link","label":"Snowflake","href":"/docs/generated/ingestion/sources/snowflake","docId":"docs/generated/ingestion/sources/snowflake"},{"type":"link","label":"SQLAlchemy","href":"/docs/generated/ingestion/sources/sqlalchemy","docId":"docs/generated/ingestion/sources/sqlalchemy"},{"type":"link","label":"Superset","href":"/docs/generated/ingestion/sources/superset","docId":"docs/generated/ingestion/sources/superset"},{"type":"link","label":"Tableau","href":"/docs/generated/ingestion/sources/tableau","docId":"docs/generated/ingestion/sources/tableau"},{"type":"link","label":"Trino","href":"/docs/generated/ingestion/sources/trino","docId":"docs/generated/ingestion/sources/trino"},{"type":"link","label":"Vertica","href":"/docs/generated/ingestion/sources/vertica","docId":"docs/generated/ingestion/sources/vertica"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Sinks","items":[{"type":"link","label":"Console","href":"/docs/metadata-ingestion/sink_docs/console","docId":"metadata-ingestion/sink_docs/console"},{"type":"link","label":"DataHub","href":"/docs/metadata-ingestion/sink_docs/datahub","docId":"metadata-ingestion/sink_docs/datahub"},{"type":"link","label":"File","href":"/docs/metadata-ingestion/sink_docs/file","docId":"metadata-ingestion/sink_docs/file"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Transformers","items":[{"type":"link","label":"Introduction","href":"/docs/metadata-ingestion/docs/transformer/intro","docId":"metadata-ingestion/docs/transformer/intro"},{"type":"link","label":"Dataset","href":"/docs/metadata-ingestion/docs/transformer/dataset_transformer","docId":"metadata-ingestion/docs/transformer/dataset_transformer"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Advanced Guides","items":[{"type":"category","label":"Scheduling Ingestion","items":[{"type":"link","label":"Introduction to Scheduling Metadata Ingestion","href":"/docs/metadata-ingestion/schedule_docs/intro","docId":"metadata-ingestion/schedule_docs/intro"},{"type":"link","label":"Using Cron","href":"/docs/metadata-ingestion/schedule_docs/cron","docId":"metadata-ingestion/schedule_docs/cron"},{"type":"link","label":"Using Airflow","href":"/docs/metadata-ingestion/schedule_docs/airflow","docId":"metadata-ingestion/schedule_docs/airflow"},{"type":"link","label":"Using Kubernetes","href":"/docs/metadata-ingestion/schedule_docs/kubernetes","docId":"metadata-ingestion/schedule_docs/kubernetes"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Working With Platform Instances","href":"/docs/platform-instances","docId":"docs/platform-instances"},{"type":"link","label":"Stateful Ingestion","href":"/docs/metadata-ingestion/docs/dev_guides/stateful","docId":"metadata-ingestion/docs/dev_guides/stateful"},{"type":"link","label":"Classification","href":"/docs/metadata-ingestion/docs/dev_guides/classification","docId":"metadata-ingestion/docs/dev_guides/classification"},{"type":"link","label":"Adding Stateful Ingestion to a Source","href":"/docs/metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source","docId":"metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source"},{"type":"link","label":"SQL Profiling","href":"/docs/metadata-ingestion/docs/dev_guides/sql_profiles","docId":"metadata-ingestion/docs/dev_guides/sql_profiles"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Enrich Metadata","items":[{"type":"link","label":"Domains","href":"/docs/domains","docId":"docs/domains"},{"type":"link","label":"Business Glossary","href":"/docs/glossary/business-glossary","docId":"docs/glossary/business-glossary"},{"type":"link","label":"Tags","href":"/docs/tags","docId":"docs/tags"},{"type":"link","label":"Lineage","href":"/docs/lineage/lineage-feature-guide","docId":"docs/lineage/lineage-feature-guide"}],"collapsed":true,"collapsible":true,"href":"/docs/enrich-metadata"},{"type":"category","label":"Act on Metadata","items":[{"type":"category","label":"Actions Framework","items":[{"type":"link","label":"Introduction","href":"/docs/actions","docId":"docs/actions/README"},{"type":"link","label":"Quickstart","href":"/docs/actions/quickstart","docId":"docs/actions/quickstart"},{"type":"link","label":"Concepts","href":"/docs/actions/concepts","docId":"docs/actions/concepts"},{"type":"category","label":"Sources","items":[{"type":"link","label":"Kafka Event Source","href":"/docs/actions/sources/kafka-event-source","docId":"docs/actions/sources/kafka-event-source"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Events","items":[{"type":"link","label":"Entity Change Event V1","href":"/docs/actions/events/entity-change-event","docId":"docs/actions/events/entity-change-event"},{"type":"link","label":"Metadata Change Log Event V1","href":"/docs/actions/events/metadata-change-log-event","docId":"docs/actions/events/metadata-change-log-event"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Actions","items":[{"type":"link","label":"Ingestion Executor","href":"/docs/actions/actions/executor","docId":"docs/actions/actions/executor"},{"type":"link","label":"Hello World","href":"/docs/actions/actions/hello_world","docId":"docs/actions/actions/hello_world"},{"type":"link","label":"Slack","href":"/docs/actions/actions/slack","docId":"docs/actions/actions/slack"},{"type":"link","label":"Microsoft Teams","href":"/docs/actions/actions/teams","docId":"docs/actions/actions/teams"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Guides","items":[{"type":"link","label":"Developing a Transformer","href":"/docs/actions/guides/developing-a-transformer","docId":"docs/actions/guides/developing-a-transformer"},{"type":"link","label":"Developing an Action","href":"/docs/actions/guides/developing-an-action","docId":"docs/actions/guides/developing-an-action"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"link","label":"About Metadata Tests","href":"/docs/tests/metadata-tests","className":"saasOnly","docId":"docs/tests/metadata-tests"},{"type":"link","label":"Lineage Impact Analysis","href":"/docs/act-on-metadata/impact-analysis","docId":"docs/act-on-metadata/impact-analysis"}],"collapsed":true,"collapsible":true,"href":"/docs/act-on-metadata"},{"type":"category","label":"Deploy DataHub","items":[{"type":"link","label":"Deploying to AWS","href":"/docs/deploy/aws","docId":"docs/deploy/aws"},{"type":"link","label":"Deploying to GCP","href":"/docs/deploy/gcp","docId":"docs/deploy/gcp"},{"type":"link","label":"Deploying with Docker","href":"/docs/docker","docId":"docker/README"},{"type":"link","label":"Deploying with Kubernetes","href":"/docs/deploy/kubernetes","docId":"docs/deploy/kubernetes"},{"type":"category","label":"Authentication","items":[{"type":"link","label":"Overview","href":"/docs/authentication","docId":"docs/authentication/README"},{"type":"link","label":"Concepts & Key Components","href":"/docs/authentication/concepts","docId":"docs/authentication/concepts"},{"type":"category","label":"Frontend Authentication","items":[{"type":"link","label":"JaaS Authentication","href":"/docs/authentication/guides/jaas","docId":"docs/authentication/guides/jaas"},{"type":"category","label":"OIDC Authentication","items":[{"type":"link","label":"Overview","href":"/docs/authentication/guides/sso/configure-oidc-react","docId":"docs/authentication/guides/sso/configure-oidc-react"},{"type":"link","label":"Configuring Google Authentication for React App (OIDC)","href":"/docs/authentication/guides/sso/configure-oidc-react-google","docId":"docs/authentication/guides/sso/configure-oidc-react-google"},{"type":"link","label":"Configuring Okta Authentication for React App (OIDC)","href":"/docs/authentication/guides/sso/configure-oidc-react-okta","docId":"docs/authentication/guides/sso/configure-oidc-react-okta"},{"type":"link","label":"Configuring Azure Authentication for React App (OIDC)","href":"/docs/authentication/guides/sso/configure-oidc-react-azure","docId":"docs/authentication/guides/sso/configure-oidc-react-azure"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"link","label":"Metadata Service Authentication","href":"/docs/authentication/introducing-metadata-service-authentication","docId":"docs/authentication/introducing-metadata-service-authentication"},{"type":"link","label":"Personal Access Tokens","href":"/docs/authentication/personal-access-tokens","docId":"docs/authentication/personal-access-tokens"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Authorization","items":[{"type":"link","label":"Overview","href":"/docs/authorization","docId":"docs/authorization/README"},{"type":"link","label":"Roles","href":"/docs/authorization/roles","docId":"docs/authorization/roles"},{"type":"link","label":"Policies Guide","href":"/docs/authorization/policies","docId":"docs/authorization/policies"},{"type":"link","label":"Authorization using Groups","href":"/docs/authorization/groups","docId":"docs/authorization/groups"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Advanced Guides","items":[{"type":"link","label":"Removing Metadata from DataHub","href":"/docs/how/delete-metadata","docId":"docs/how/delete-metadata"},{"type":"link","label":"Configuring Authorization with Apache Ranger","href":"/docs/how/configuring-authorization-with-apache-ranger","docId":"docs/how/configuring-authorization-with-apache-ranger"},{"type":"link","label":"Taking backup of DataHub","href":"/docs/how/backup-datahub","docId":"docs/how/backup-datahub"},{"type":"link","label":"Restoring Search and Graph Indices from Local Database","href":"/docs/how/restore-indices","docId":"docs/how/restore-indices"},{"type":"link","label":"Configuring Database Retention","href":"/docs/advanced/db-retention","docId":"docs/advanced/db-retention"},{"type":"link","label":"Monitoring DataHub","href":"/docs/advanced/monitoring","docId":"docs/advanced/monitoring"},{"type":"link","label":"How to Extract Logs from DataHub Containers","href":"/docs/how/extract-container-logs","docId":"docs/how/extract-container-logs"},{"type":"link","label":"Telemetry","href":"/docs/deploy/telemetry","docId":"docs/deploy/telemetry"},{"type":"link","label":"Configuring Kafka","href":"/docs/how/kafka-config","docId":"docs/how/kafka-config"},{"type":"link","label":"Integrating with Confluent Cloud","href":"/docs/deploy/confluent-cloud","docId":"docs/deploy/confluent-cloud"},{"type":"link","label":"No Code Upgrade (In-Place Migration Guide)","href":"/docs/advanced/no-code-upgrade","docId":"docs/advanced/no-code-upgrade"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Updating DataHub","href":"/docs/how/updating-datahub","docId":"docs/how/updating-datahub"}],"collapsed":true,"collapsible":true},{"type":"category","label":"DataHub API","items":[{"type":"link","label":"Which DataHub API is for me?","href":"/docs/api/datahub-apis","docId":"docs/api/datahub-apis"},{"type":"category","label":"GraphQL API","items":[{"type":"link","label":"Overview","href":"/docs/api/graphql/overview","docId":"docs/api/graphql/overview"},{"type":"category","label":"Reference","items":[{"type":"link","label":"Queries","href":"/docs/graphql/queries","docId":"graphql/queries"},{"type":"link","label":"Mutations","href":"/docs/graphql/mutations","docId":"graphql/mutations"},{"type":"link","label":"Objects","href":"/docs/graphql/objects","docId":"graphql/objects"},{"type":"link","label":"Inputs","href":"/docs/graphql/inputObjects","docId":"graphql/inputObjects"},{"type":"link","label":"Interfaces","href":"/docs/graphql/interfaces","docId":"graphql/interfaces"},{"type":"link","label":"Unions","href":"/docs/graphql/unions","docId":"graphql/unions"},{"type":"link","label":"Enums","href":"/docs/graphql/enums","docId":"graphql/enums"},{"type":"link","label":"Scalars","href":"/docs/graphql/scalars","docId":"graphql/scalars"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Guides","items":[{"type":"link","label":"How To Set Up GraphQL","href":"/docs/api/graphql/how-to-set-up-graphql","docId":"docs/api/graphql/how-to-set-up-graphql"},{"type":"link","label":"Getting Started With GraphQL","href":"/docs/api/graphql/getting-started","docId":"docs/api/graphql/getting-started"},{"type":"link","label":"Access Token Management","href":"/docs/api/graphql/token-management","docId":"docs/api/graphql/token-management"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"OpenAPI","items":[{"type":"link","label":"OpenAPI Guide","href":"/docs/api/openapi/openapi-usage-guide","docId":"docs/api/openapi/openapi-usage-guide"},{"type":"link","label":"Timeline API","href":"/docs/dev-guides/timeline","docId":"docs/dev-guides/timeline"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Rest.li","items":[{"type":"link","label":"Rest.li API","href":"/docs/api/restli/restli-overview","docId":"docs/api/restli/restli-overview"},{"type":"link","label":"Restore Indices","href":"/docs/api/restli/restore-indices","docId":"docs/api/restli/restore-indices"},{"type":"link","label":"Aspect Versioning and Rest.li Modeling","href":"/docs/advanced/aspect-versioning","docId":"docs/advanced/aspect-versioning"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Getting Started: APIs & SDKs","items":[{"type":"link","label":"Modifying Tags on Datasets","href":"/docs/api/tutorials/modifying-dataset-tags","docId":"docs/api/tutorials/modifying-dataset-tags"},{"type":"link","label":"Modifying Terms on Datasets","href":"/docs/api/tutorials/modifying-dataset-terms","docId":"docs/api/tutorials/modifying-dataset-terms"},{"type":"link","label":"Modifying Owners","href":"/docs/api/tutorials/modifying-dataset-owners","docId":"docs/api/tutorials/modifying-dataset-owners"},{"type":"link","label":"Modifying Domains On Dataset","href":"/docs/api/tutorials/modifying-dataset-domains","docId":"docs/api/tutorials/modifying-dataset-domains"},{"type":"link","label":"Modifying Dataset","href":"/docs/api/tutorials/modifying-datasets","docId":"docs/api/tutorials/modifying-datasets"},{"type":"link","label":"Modifying Lineages","href":"/docs/api/tutorials/modifying-dataset-lineage","docId":"docs/api/tutorials/modifying-dataset-lineage"},{"type":"link","label":"Modifying Deprecation","href":"/docs/api/tutorials/modifying-dataset-deprecation","docId":"docs/api/tutorials/modifying-dataset-deprecation"},{"type":"link","label":"Modifying Description","href":"/docs/api/tutorials/modifying-dataset-descriptions","docId":"docs/api/tutorials/modifying-dataset-descriptions"},{"type":"link","label":"Modifying Custom Properties on Datasets","href":"/docs/api/tutorials/modifying-dataset-custom-properties","docId":"docs/api/tutorials/modifying-dataset-custom-properties"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Tools","items":[{"type":"link","label":"CLI","href":"/docs/cli","docId":"docs/cli"},{"type":"category","label":"SDKs","items":[{"type":"link","label":"Python Emitter","href":"/docs/metadata-ingestion/as-a-library","docId":"metadata-ingestion/as-a-library"},{"type":"link","label":"Java Emitter","href":"/docs/metadata-integration/java/as-a-library","docId":"metadata-integration/java/as-a-library"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Python SDK Reference","items":[{"type":"link","label":"Builder","href":"/docs/python-sdk/builder","docId":"python-sdk/builder"},{"type":"link","label":"Client","href":"/docs/python-sdk/clients","docId":"python-sdk/clients"},{"type":"link","label":"Models","href":"/docs/python-sdk/models","docId":"python-sdk/models"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Lite (Experimental)","href":"/docs/datahub_lite","docId":"docs/datahub_lite"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Developer Guides","items":[{"type":"category","label":"DataHub Metadata Model","items":[{"type":"link","label":"The Metadata Model","href":"/docs/metadata-modeling/metadata-model","docId":"docs/modeling/metadata-model"},{"type":"link","label":"Extending the Metadata Model","href":"/docs/metadata-modeling/extending-the-metadata-model","docId":"docs/modeling/extending-the-metadata-model"},{"type":"link","label":"Metadata Events","href":"/docs/what/mxe","docId":"docs/what/mxe"},{"type":"category","label":"Entities","items":[{"type":"link","label":"Data Platform","href":"/docs/generated/metamodel/entities/dataplatform","docId":"docs/generated/metamodel/entities/dataPlatform"},{"type":"link","label":"Dataset","href":"/docs/generated/metamodel/entities/dataset","docId":"docs/generated/metamodel/entities/dataset"},{"type":"link","label":"DataJob","href":"/docs/generated/metamodel/entities/datajob","docId":"docs/generated/metamodel/entities/dataJob"},{"type":"link","label":"DataFlow","href":"/docs/generated/metamodel/entities/dataflow","docId":"docs/generated/metamodel/entities/dataFlow"},{"type":"link","label":"DataProcess","href":"/docs/generated/metamodel/entities/dataprocess","docId":"docs/generated/metamodel/entities/dataProcess"},{"type":"link","label":"DataProcessInstance","href":"/docs/generated/metamodel/entities/dataprocessinstance","docId":"docs/generated/metamodel/entities/dataProcessInstance"},{"type":"link","label":"Chart","href":"/docs/generated/metamodel/entities/chart","docId":"docs/generated/metamodel/entities/chart"},{"type":"link","label":"Dashboard","href":"/docs/generated/metamodel/entities/dashboard","docId":"docs/generated/metamodel/entities/dashboard"},{"type":"link","label":"Notebook","href":"/docs/generated/metamodel/entities/notebook","docId":"docs/generated/metamodel/entities/notebook"},{"type":"link","label":"Corpuser","href":"/docs/generated/metamodel/entities/corpuser","docId":"docs/generated/metamodel/entities/corpuser"},{"type":"link","label":"CorpGroup","href":"/docs/generated/metamodel/entities/corpgroup","docId":"docs/generated/metamodel/entities/corpGroup"},{"type":"link","label":"Domain","href":"/docs/generated/metamodel/entities/domain","docId":"docs/generated/metamodel/entities/domain"},{"type":"link","label":"Container","href":"/docs/generated/metamodel/entities/container","docId":"docs/generated/metamodel/entities/container"},{"type":"link","label":"Tag","href":"/docs/generated/metamodel/entities/tag","docId":"docs/generated/metamodel/entities/tag"},{"type":"link","label":"GlossaryTerm","href":"/docs/generated/metamodel/entities/glossaryterm","docId":"docs/generated/metamodel/entities/glossaryTerm"},{"type":"link","label":"GlossaryNode","href":"/docs/generated/metamodel/entities/glossarynode","docId":"docs/generated/metamodel/entities/glossaryNode"},{"type":"link","label":"Assertion","href":"/docs/generated/metamodel/entities/assertion","docId":"docs/generated/metamodel/entities/assertion"},{"type":"link","label":"MlModel","href":"/docs/generated/metamodel/entities/mlmodel","docId":"docs/generated/metamodel/entities/mlModel"},{"type":"link","label":"MlModelGroup","href":"/docs/generated/metamodel/entities/mlmodelgroup","docId":"docs/generated/metamodel/entities/mlModelGroup"},{"type":"link","label":"MlModelDeployment","href":"/docs/generated/metamodel/entities/mlmodeldeployment","docId":"docs/generated/metamodel/entities/mlModelDeployment"},{"type":"link","label":"MlFeatureTable","href":"/docs/generated/metamodel/entities/mlfeaturetable","docId":"docs/generated/metamodel/entities/mlFeatureTable"},{"type":"link","label":"MlFeature","href":"/docs/generated/metamodel/entities/mlfeature","docId":"docs/generated/metamodel/entities/mlFeature"},{"type":"link","label":"MlPrimaryKey","href":"/docs/generated/metamodel/entities/mlprimarykey","docId":"docs/generated/metamodel/entities/mlPrimaryKey"},{"type":"link","label":"Test","href":"/docs/generated/metamodel/entities/test","docId":"docs/generated/metamodel/entities/test"},{"type":"link","label":"InviteToken","href":"/docs/generated/metamodel/entities/invitetoken","docId":"docs/generated/metamodel/entities/inviteToken"},{"type":"link","label":"SchemaField","href":"/docs/generated/metamodel/entities/schemafield","docId":"docs/generated/metamodel/entities/schemaField"},{"type":"link","label":"DataHubRole","href":"/docs/generated/metamodel/entities/datahubrole","docId":"docs/generated/metamodel/entities/dataHubRole"},{"type":"link","label":"Post","href":"/docs/generated/metamodel/entities/post","docId":"docs/generated/metamodel/entities/post"},{"type":"link","label":"DataHubStepState","href":"/docs/generated/metamodel/entities/datahubstepstate","docId":"docs/generated/metamodel/entities/dataHubStepState"},{"type":"link","label":"DataHubView","href":"/docs/generated/metamodel/entities/datahubview","docId":"docs/generated/metamodel/entities/dataHubView"},{"type":"link","label":"Query","href":"/docs/generated/metamodel/entities/query","docId":"docs/generated/metamodel/entities/query"},{"type":"link","label":"DataHubPolicy","href":"/docs/generated/metamodel/entities/datahubpolicy","docId":"docs/generated/metamodel/entities/dataHubPolicy"},{"type":"link","label":"DataHubIngestionSource","href":"/docs/generated/metamodel/entities/datahubingestionsource","docId":"docs/generated/metamodel/entities/dataHubIngestionSource"},{"type":"link","label":"DataHubSecret","href":"/docs/generated/metamodel/entities/datahubsecret","docId":"docs/generated/metamodel/entities/dataHubSecret"},{"type":"link","label":"DataHubExecutionRequest","href":"/docs/generated/metamodel/entities/datahubexecutionrequest","docId":"docs/generated/metamodel/entities/dataHubExecutionRequest"},{"type":"link","label":"DataHubRetention","href":"/docs/generated/metamodel/entities/datahubretention","docId":"docs/generated/metamodel/entities/dataHubRetention"},{"type":"link","label":"DataPlatformInstance","href":"/docs/generated/metamodel/entities/dataplatforminstance","docId":"docs/generated/metamodel/entities/dataPlatformInstance"},{"type":"link","label":"Telemetry","href":"/docs/generated/metamodel/entities/telemetry","docId":"docs/generated/metamodel/entities/telemetry"},{"type":"link","label":"DataHubAccessToken","href":"/docs/generated/metamodel/entities/datahubaccesstoken","docId":"docs/generated/metamodel/entities/dataHubAccessToken"},{"type":"link","label":"DataHubUpgrade","href":"/docs/generated/metamodel/entities/datahubupgrade","docId":"docs/generated/metamodel/entities/dataHubUpgrade"},{"type":"link","label":"GlobalSettings","href":"/docs/generated/metamodel/entities/globalsettings","docId":"docs/generated/metamodel/entities/globalSettings"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Developing on DataHub","items":[{"type":"link","label":"Local Development","href":"/docs/developers","docId":"docs/developers"},{"type":"link","label":"Using Docker Images During Development","href":"/docs/docker/development","docId":"docs/docker/development"},{"type":"link","label":"Developing on Metadata Ingestion","href":"/docs/metadata-ingestion/developing","docId":"metadata-ingestion/developing"},{"type":"category","label":"Modules","items":[{"type":"link","label":"datahub-web-react","href":"/docs/datahub-web-react","docId":"datahub-web-react/README"},{"type":"link","label":"datahub-frontend","href":"/docs/datahub-frontend","docId":"datahub-frontend/README"},{"type":"link","label":"datahub-graphql-core","href":"/docs/datahub-graphql-core","docId":"datahub-graphql-core/README"},{"type":"link","label":"metadata-service","href":"/docs/metadata-service","docId":"metadata-service/README"},{"type":"link","label":"metadata-jobs:mae-consumer-job","href":"/docs/metadata-jobs/mae-consumer-job","docId":"metadata-jobs/mae-consumer-job/README"},{"type":"link","label":"metadata-jobs:mce-consumer-job","href":"/docs/metadata-jobs/mce-consumer-job","docId":"metadata-jobs/mce-consumer-job/README"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"link","label":"Quickstart Debugging Guide","href":"/docs/debugging","docId":"docs/debugging"},{"type":"link","label":"FAQ Using DataHub","href":"/docs/faq-using-datahub","docId":"docs/faq-using-datahub"},{"type":"link","label":"Plugins Guide","href":"/docs/plugins","docId":"docs/plugins"},{"type":"category","label":"Advanced","items":[{"type":"link","label":"Datahub\'s Reporting Framework for Ingestion Job Telemetry","href":"/docs/metadata-ingestion/docs/dev_guides/reporting_telemetry","docId":"metadata-ingestion/docs/dev_guides/reporting_telemetry"},{"type":"link","label":"MetadataChangeProposal & MetadataChangeLog Events","href":"/docs/advanced/mcp-mcl","docId":"docs/advanced/mcp-mcl"},{"type":"link","label":"Upgrade Docker Image","href":"/docs/docker/datahub-upgrade","docId":"docker/datahub-upgrade/README"},{"type":"link","label":"No Code Metadata","href":"/docs/advanced/no-code-modeling","docId":"docs/advanced/no-code-modeling"},{"type":"link","label":"React Analytics","href":"/docs/datahub-web-react/src/app/analytics","docId":"datahub-web-react/src/app/analytics/README"},{"type":"link","label":"Migrate Graph Service Implementation to Elasticsearch","href":"/docs/how/migrating-graph-service-implementation","docId":"docs/how/migrating-graph-service-implementation"},{"type":"link","label":"SchemaFieldPath Specification (Version 2)","href":"/docs/advanced/field-path-spec-v2","docId":"docs/advanced/field-path-spec-v2"},{"type":"link","label":"Adding a Metadata Ingestion Source","href":"/docs/metadata-ingestion/adding-source","docId":"metadata-ingestion/adding-source"},{"type":"link","label":"Using a Custom Ingestion Source","href":"/docs/how/add-custom-ingestion-source","docId":"docs/how/add-custom-ingestion-source"},{"type":"link","label":"Adding a custom Dataset Data Platform","href":"/docs/how/add-custom-data-platform","docId":"docs/how/add-custom-data-platform"},{"type":"link","label":"Browse Paths Upgrade (August 2022)","href":"/docs/advanced/browse-paths-upgrade","docId":"docs/advanced/browse-paths-upgrade"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Feature Guides","items":[{"type":"link","label":"Search","href":"/docs/how/search","docId":"docs/how/search"},{"type":"link","label":"Schema History","href":"/docs/schema-history","docId":"docs/schema-history"},{"type":"link","label":"Domains","href":"/docs/domains","docId":"docs/domains"},{"type":"link","label":"Business Glossary","href":"/docs/glossary/business-glossary","docId":"docs/glossary/business-glossary"},{"type":"link","label":"Tags","href":"/docs/tags","docId":"docs/tags"},{"type":"link","label":"Browse","href":"/docs/browse","docId":"docs/browse"},{"type":"link","label":"Access Policies","href":"/docs/authorization/access-policies-guide","docId":"docs/authorization/access-policies-guide"},{"type":"link","label":"Dataset Usage & Query History","href":"/docs/features/dataset-usage-and-query-history","docId":"docs/features/dataset-usage-and-query-history"},{"type":"link","label":"Posts","href":"/docs/posts","docId":"docs/posts"},{"type":"link","label":"Sync Status","href":"/docs/sync-status","docId":"docs/sync-status"},{"type":"link","label":"[Stemming and Synonyms Support]","href":"/docs/architecture/stemming_and_synonyms","docId":"docs/architecture/stemming_and_synonyms"},{"type":"link","label":"Lineage","href":"/docs/lineage/lineage-feature-guide","docId":"docs/lineage/lineage-feature-guide"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Join the Community","items":[{"type":"link","label":"Slack","href":"/docs/slack","docId":"docs/slack"},{"type":"link","label":"Town Halls","href":"/docs/townhalls","docId":"docs/townhalls"},{"type":"link","label":"Town Hall History","href":"/docs/townhall-history","docId":"docs/townhall-history"},{"type":"link","label":"Contributor Covenant Code of Conduct","href":"/docs/code_of_conduct","docId":"docs/CODE_OF_CONDUCT"},{"type":"link","label":"Contributing","href":"/docs/contributing","docId":"docs/CONTRIBUTING"},{"type":"link","label":"Articles & Talks","href":"/docs/links","docId":"docs/links"},{"type":"link","label":"RFC Process","href":"/docs/rfc","docId":"docs/rfc"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Managed DataHub","items":[{"type":"link","label":"Managed DataHub Exclusives","href":"/docs/managed-datahub/managed-datahub-overview","docId":"docs/managed-datahub/managed-datahub-overview"},{"type":"link","label":"Getting Started with Acryl DataHub","href":"/docs/managed-datahub/welcome-acryl","docId":"docs/managed-datahub/welcome-acryl"},{"type":"category","label":"Metadata Ingestion With Acryl","items":[{"type":"link","label":"Ingestion","href":"/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion","docId":"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion"}],"collapsed":true,"collapsible":true},{"type":"category","label":"DataHub API","items":[{"type":"link","label":"Entity Events API","href":"/docs/managed-datahub/datahub-api/entity-events-api","className":"saasOnly","docId":"docs/managed-datahub/datahub-api/entity-events-api"},{"type":"category","label":"GraphQL API","items":[{"type":"link","label":"Getting Started","href":"/docs/managed-datahub/datahub-api/graphql-api/getting-started","docId":"docs/managed-datahub/datahub-api/graphql-api/getting-started"},{"type":"link","label":"Incidents API (Beta)","href":"/docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta","className":"saasOnly","docId":"docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Integrations","items":[{"type":"link","label":"AWS PrivateLink","href":"/docs/managed-datahub/integrations/aws-privatelink","className":"saasOnly","docId":"docs/managed-datahub/integrations/aws-privatelink"},{"type":"link","label":"OIDC SSO Integration","href":"/docs/managed-datahub/integrations/oidc-sso-integration","className":"saasOnly","docId":"docs/managed-datahub/integrations/oidc-sso-integration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Operator Guide","items":[{"type":"link","label":"Setting up Remote Ingestion Executor on AWS","href":"/docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws","className":"saasOnly","docId":"docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws"},{"type":"link","label":"Setting up Events API on AWS EventBridge","href":"/docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge","className":"saasOnly","docId":"docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Local Chrome Extension","href":"/docs/managed-datahub/chrome-extension","className":"saasOnly","docId":"docs/managed-datahub/chrome-extension"},{"type":"category","label":"Managed DataHub Release History","items":[{"type":"link","label":"v0.2.5","href":"/docs/managed-datahub/release-notes/v_0_2_5","docId":"docs/managed-datahub/release-notes/v_0_2_5"},{"type":"link","label":"v0.2.4","href":"/docs/managed-datahub/release-notes/v_0_2_4","docId":"docs/managed-datahub/release-notes/v_0_2_4"},{"type":"link","label":"v0.2.3","href":"/docs/managed-datahub/release-notes/v_0_2_3","docId":"docs/managed-datahub/release-notes/v_0_2_3"},{"type":"link","label":"v0.2.2","href":"/docs/managed-datahub/release-notes/v_0_2_2","docId":"docs/managed-datahub/release-notes/v_0_2_2"},{"type":"link","label":"v0.2.1","href":"/docs/managed-datahub/release-notes/v_0_2_1","docId":"docs/managed-datahub/release-notes/v_0_2_1"},{"type":"link","label":"v0.2.0","href":"/docs/managed-datahub/release-notes/v_0_2_0","docId":"docs/managed-datahub/release-notes/v_0_2_0"},{"type":"link","label":"v0.1.73","href":"/docs/managed-datahub/release-notes/v_0_1_73","docId":"docs/managed-datahub/release-notes/v_0_1_73"},{"type":"link","label":"v0.1.72","href":"/docs/managed-datahub/release-notes/v_0_1_72","docId":"docs/managed-datahub/release-notes/v_0_1_72"},{"type":"link","label":"v0.1.70","href":"/docs/managed-datahub/release-notes/v_0_1_70","docId":"docs/managed-datahub/release-notes/v_0_1_70"},{"type":"link","label":"v0.1.69","href":"/docs/managed-datahub/release-notes/v_0_1_69","docId":"docs/managed-datahub/release-notes/v_0_1_69"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Release History","items":[{"type":"link","label":"Releases","href":"/docs/releases","docId":"releases"}],"collapsed":true,"collapsible":true}]},"docs":{"datahub-frontend/README":{"id":"datahub-frontend/README","title":"datahub-frontend","description":"DataHub frontend is a Play service written in Java. It is served as a mid-tier","sidebar":"overviewSidebar"},"datahub-graphql-core/README":{"id":"datahub-graphql-core/README","title":"datahub-graphql-core","description":"DataHub GraphQL API is a shared lib module containing a GraphQL API on top of the GMS service layer. It exposes a graph-based representation","sidebar":"overviewSidebar"},"datahub-web-react/README":{"id":"datahub-web-react/README","title":"datahub-web-react","description":"About","sidebar":"overviewSidebar"},"datahub-web-react/src/app/analytics/README":{"id":"datahub-web-react/src/app/analytics/README","title":"DataHub React Analytics","description":"About","sidebar":"overviewSidebar"},"docker/airflow/local_airflow":{"id":"docker/airflow/local_airflow","title":"Running Airflow locally with DataHub","description":"This feature is currently unmaintained. As of 0.10.0 the container described is not published alongside the DataHub CLI. If you\'d like to use it, please reach out to us on the community slack."},"docker/datahub-upgrade/README":{"id":"docker/datahub-upgrade/README","title":"DataHub Upgrade Docker Image","description":"This container is used to automatically apply upgrades from one version of DataHub to another.","sidebar":"overviewSidebar"},"docker/README":{"id":"docker/README","title":"Deploying with Docker","description":"Prerequisites","sidebar":"overviewSidebar"},"docs/act-on-metadata":{"id":"docs/act-on-metadata","title":"Act on Metadata Overview","description":"DataHub\'s metadata infrastructure is stream-oriented, meaning that all changes in metadata are communicated and reflected within the platform within seconds.","sidebar":"overviewSidebar"},"docs/act-on-metadata/impact-analysis":{"id":"docs/act-on-metadata/impact-analysis","title":"About DataHub Lineage Impact Analysis","description":"Lineage Impact Analysis is a powerful workflow for understanding the complete set of upstream and downstream dependencies of a Dataset, Dashboard, Chart, and many other DataHub Entities.","sidebar":"overviewSidebar"},"docs/actions/actions/executor":{"id":"docs/actions/actions/executor","title":"Ingestion Executor","description":"Certified","sidebar":"overviewSidebar"},"docs/actions/actions/hello_world":{"id":"docs/actions/actions/hello_world","title":"Hello World","description":"Certified","sidebar":"overviewSidebar"},"docs/actions/actions/slack":{"id":"docs/actions/actions/slack","title":"Slack","description":"|  |  |","sidebar":"overviewSidebar"},"docs/actions/actions/teams":{"id":"docs/actions/actions/teams","title":"Microsoft Teams","description":"|  |  |","sidebar":"overviewSidebar"},"docs/actions/concepts":{"id":"docs/actions/concepts","title":"Concepts","description":"The Actions framework includes pluggable components for filtering, transforming, and reacting to important DataHub, such as","sidebar":"overviewSidebar"},"docs/actions/events/entity-change-event":{"id":"docs/actions/events/entity-change-event","title":"Entity Change Event V1","description":"Event Type","sidebar":"overviewSidebar"},"docs/actions/events/metadata-change-log-event":{"id":"docs/actions/events/metadata-change-log-event","title":"Metadata Change Log Event V1","description":"Event Type","sidebar":"overviewSidebar"},"docs/actions/guides/developing-a-transformer":{"id":"docs/actions/guides/developing-a-transformer","title":"Developing a Transformer","description":"In this guide, we will outline each step to developing a custom Transformer for the DataHub Actions Framework.","sidebar":"overviewSidebar"},"docs/actions/guides/developing-an-action":{"id":"docs/actions/guides/developing-an-action","title":"Developing an Action","description":"In this guide, we will outline each step to developing a Action for the DataHub Actions Framework.","sidebar":"overviewSidebar"},"docs/actions/quickstart":{"id":"docs/actions/quickstart","title":"Quickstart","description":"Prerequisites","sidebar":"overviewSidebar"},"docs/actions/README":{"id":"docs/actions/README","title":"Introduction","description":"Welcome to DataHub Actions! The Actions framework makes responding to realtime changes in your Metadata Graph easy, enabling you to seamlessly integrate DataHub into a broader events-based architecture.","sidebar":"overviewSidebar"},"docs/actions/sources/kafka-event-source":{"id":"docs/actions/sources/kafka-event-source","title":"Kafka Event Source","description":"Overview","sidebar":"overviewSidebar"},"docs/advanced/aspect-versioning":{"id":"docs/advanced/aspect-versioning","title":"Aspect Versioning","description":"As each version of metadata aspect is immutable, any update to an existing aspect results in the creation of a new version. Typically one would expect the version number increases sequentially with the largest version number being the latest version, i.e. v1 (oldest), v2 (second oldest), ..., vN (latest). However, this approach results in major challenges in both rest.li modeling & transaction isolation and therefore requires a rethinking.","sidebar":"overviewSidebar"},"docs/advanced/backfilling":{"id":"docs/advanced/backfilling","title":"Backfilling Search Index & Graph DB","description":"WIP"},"docs/advanced/browse-paths-upgrade":{"id":"docs/advanced/browse-paths-upgrade","title":"Browse Paths Upgrade (August 2022)","description":"Background","sidebar":"overviewSidebar"},"docs/advanced/db-retention":{"id":"docs/advanced/db-retention","title":"Configuring Database Retention","description":"Goal","sidebar":"overviewSidebar"},"docs/advanced/derived-aspects":{"id":"docs/advanced/derived-aspects","title":"Derived Aspects","description":"WIP"},"docs/advanced/entity-hierarchy":{"id":"docs/advanced/entity-hierarchy","title":"Entity Hierarchy","description":"WIP"},"docs/advanced/field-path-spec-v2":{"id":"docs/advanced/field-path-spec-v2","title":"SchemaFieldPath Specification (Version 2)","description":"This document outlines the formal specification for the fieldPath member of","sidebar":"overviewSidebar"},"docs/advanced/high-cardinality":{"id":"docs/advanced/high-cardinality","title":"High Cardinality Relationships","description":"As explained in What is a Relationship, the raw metadata for forming relationships is captured directly inside of a Metadata Aspect. The most natural way to model this is using an array, e.g. a group membership aspect contains an array of user URNs. However, this poses some challenges when the cardinality of the relationship is expected to be large (say, greater than 10,000). The aspect becomes large in size, which leads to slow update and retrieval. It may even exceed the underlying limit of the document store, which is often in the range of a few MBs. Furthermore, sending large messages (> 1MB) over Kafka requires special tuning and is generally discouraged."},"docs/advanced/mcp-mcl":{"id":"docs/advanced/mcp-mcl","title":"MetadataChangeProposal & MetadataChangeLog Events","description":"Overview & Vision","sidebar":"overviewSidebar"},"docs/advanced/monitoring":{"id":"docs/advanced/monitoring","title":"Monitoring DataHub","description":"Monitoring DataHub\'s system components is critical for operating and improving DataHub. This doc explains how to add","sidebar":"overviewSidebar"},"docs/advanced/no-code-modeling":{"id":"docs/advanced/no-code-modeling","title":"No Code Metadata","description":"Summary of changes","sidebar":"overviewSidebar"},"docs/advanced/no-code-upgrade":{"id":"docs/advanced/no-code-upgrade","title":"No Code Upgrade (In-Place Migration Guide)","description":"Summary of changes","sidebar":"overviewSidebar"},"docs/advanced/partial-update":{"id":"docs/advanced/partial-update","title":"Supporting Partial Aspect Update","description":"WIP"},"docs/advanced/pdl-best-practices":{"id":"docs/advanced/pdl-best-practices","title":"PDL Best Practices","description":"WIP"},"docs/api/datahub-apis":{"id":"docs/api/datahub-apis","title":"Which DataHub API is for me?","description":"DataHub supplys several APIs to manipulate metadata on the platform. These are our most-to-least recommended approaches:","sidebar":"overviewSidebar"},"docs/api/graphql/getting-started":{"id":"docs/api/graphql/getting-started","title":"Getting Started With GraphQL","description":"Reading an Entity: Queries","sidebar":"overviewSidebar"},"docs/api/graphql/how-to-set-up-graphql":{"id":"docs/api/graphql/how-to-set-up-graphql","title":"How To Set Up GraphQL","description":"Preparing Local Datahub Deployment","sidebar":"overviewSidebar"},"docs/api/graphql/overview":{"id":"docs/api/graphql/overview","title":"DataHub GraphQL API","description":"DataHub provides a rich graphql API for programmatically interacting with the Entities & Relationships comprising your organization\'s Metadata Graph.","sidebar":"overviewSidebar"},"docs/api/graphql/token-management":{"id":"docs/api/graphql/token-management","title":"Access Token Management","description":"DataHub provides the following graphql endpoints for managing Access Tokens. In this page you will see examples as well","sidebar":"overviewSidebar"},"docs/api/openapi/openapi-usage-guide":{"id":"docs/api/openapi/openapi-usage-guide","title":"DataHub OpenAPI Guide","description":"Why OpenAPI","sidebar":"overviewSidebar"},"docs/api/restli/restli-overview":{"id":"docs/api/restli/restli-overview","title":"Rest.li API","description":"You can access basic documentation on the API endpoints by opening the /restli/docs endpoint in the browser.","sidebar":"overviewSidebar"},"docs/api/restli/restore-indices":{"id":"docs/api/restli/restore-indices","title":"Restore Indices Endpoint","description":"You can do a HTTP POST request to /gms/aspects?action=restoreIndices endpoint with the urn as part of JSON Payload to restore indices for the particular URN, or with the urnLike regex to restore for batchSize URNs matching the pattern starting from start.","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-custom-properties":{"id":"docs/api/tutorials/modifying-dataset-custom-properties","title":"Modifying Custom Properties on Datasets","description":"Why Would You Use Custom Properties on Datasets?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-deprecation":{"id":"docs/api/tutorials/modifying-dataset-deprecation","title":"Modifying Deprecation","description":"Why Would You Use Deprecation?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-descriptions":{"id":"docs/api/tutorials/modifying-dataset-descriptions","title":"Modifying Description","description":"Why Would You Use Description on Dataset?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-domains":{"id":"docs/api/tutorials/modifying-dataset-domains","title":"Modifying Domains On Dataset","description":"Why Would You Use Domains?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-lineage":{"id":"docs/api/tutorials/modifying-dataset-lineage","title":"Modifying Lineages","description":"Why Would You Use Lineage?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-owners":{"id":"docs/api/tutorials/modifying-dataset-owners","title":"Modifying Owners","description":"Why Would You Use Users and Groups?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-tags":{"id":"docs/api/tutorials/modifying-dataset-tags","title":"Modifying Tags on Datasets","description":"Why Would You Use Tags on Datasets?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-dataset-terms":{"id":"docs/api/tutorials/modifying-dataset-terms","title":"Modifying Terms on Datasets","description":"Why Would You Use Terms on Datasets?","sidebar":"overviewSidebar"},"docs/api/tutorials/modifying-datasets":{"id":"docs/api/tutorials/modifying-datasets","title":"Modifying Dataset","description":"Why Would You Use Datasets?","sidebar":"overviewSidebar"},"docs/architecture/architecture":{"id":"docs/architecture/architecture","title":"Overview","description":"DataHub is a 3rd generation Metadata Platform that enables Data Discovery, Collaboration, Governance, and end-to-end Observability","sidebar":"overviewSidebar"},"docs/architecture/metadata-ingestion":{"id":"docs/architecture/metadata-ingestion","title":"Ingestion Framework","description":"DataHub supports an extremely flexible ingestion architecture that can support push, pull, asynchronous and synchronous models.","sidebar":"overviewSidebar"},"docs/architecture/metadata-serving":{"id":"docs/architecture/metadata-serving","title":"Serving Tier","description":"The figure below shows the high-level system diagram for DataHub\'s Serving Tier.","sidebar":"overviewSidebar"},"docs/architecture/stemming_and_synonyms":{"id":"docs/architecture/stemming_and_synonyms","title":"About DataHub [Stemming and Synonyms Support]","description":"This feature adds features to our current search implementation in an effort to make search results more relevant. Included improvements are:","sidebar":"overviewSidebar"},"docs/authentication/concepts":{"id":"docs/authentication/concepts","title":"Concepts & Key Components","description":"We introduced a few important concepts to the Metadata Service to make authentication work:","sidebar":"overviewSidebar"},"docs/authentication/guides/add-users":{"id":"docs/authentication/guides/add-users","title":"Onboarding Users to DataHub","description":"New user accounts can be provisioned on DataHub in 3 ways:","sidebar":"overviewSidebar"},"docs/authentication/guides/jaas":{"id":"docs/authentication/guides/jaas","title":"JaaS Authentication","description":"Overview","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react":{"id":"docs/authentication/guides/sso/configure-oidc-react","title":"Overview","description":"The DataHub React application supports OIDC authentication built on top of the Pac4j Play library.","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react-azure":{"id":"docs/authentication/guides/sso/configure-oidc-react-azure","title":"Configuring Azure Authentication for React App (OIDC)","description":"Authored on 21/12/2021","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react-google":{"id":"docs/authentication/guides/sso/configure-oidc-react-google","title":"Configuring Google Authentication for React App (OIDC)","description":"Authored on 3/10/2021","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react-okta":{"id":"docs/authentication/guides/sso/configure-oidc-react-okta","title":"Configuring Okta Authentication for React App (OIDC)","description":"Authored on 3/10/2021","sidebar":"overviewSidebar"},"docs/authentication/introducing-metadata-service-authentication":{"id":"docs/authentication/introducing-metadata-service-authentication","title":"Metadata Service Authentication","description":"Introduction","sidebar":"overviewSidebar"},"docs/authentication/personal-access-tokens":{"id":"docs/authentication/personal-access-tokens","title":"About DataHub Personal Access Tokens","description":"Personal Access Tokens, or PATs for short, allow users to represent themselves in code and programmatically use DataHub\'s APIs in deployments where security is a concern.","sidebar":"overviewSidebar"},"docs/authentication/README":{"id":"docs/authentication/README","title":"Overview","description":"Authentication is the process of verifying the identity of a user or service. There are two","sidebar":"overviewSidebar"},"docs/authorization/access-policies-guide":{"id":"docs/authorization/access-policies-guide","title":"About DataHub Access Policies","description":"Access Policies define who can do what to which resources. In conjunction with Roles, Access Policies determine what users are allowed to do on DataHub.","sidebar":"overviewSidebar"},"docs/authorization/groups":{"id":"docs/authorization/groups","title":"Authorization using Groups","description":"Introduction","sidebar":"overviewSidebar"},"docs/authorization/policies":{"id":"docs/authorization/policies","title":"Policies Guide","description":"Introduction","sidebar":"overviewSidebar"},"docs/authorization/README":{"id":"docs/authorization/README","title":"Overview","description":"Authorization specifies what accesses an authenticated user has within a system.","sidebar":"overviewSidebar"},"docs/authorization/roles":{"id":"docs/authorization/roles","title":"About DataHub Roles","description":"DataHub provides the ability to use Roles to manage permissions.","sidebar":"overviewSidebar"},"docs/browse":{"id":"docs/browse","title":"About DataHub Browse","description":"Browse is one of the primary entrypoints for discovering different Datasets, Dashboards, Charts and other DataHub Entities.","sidebar":"overviewSidebar"},"docs/cli":{"id":"docs/cli","title":"DataHub CLI","description":"DataHub comes with a friendly cli called datahub that allows you to perform a lot of common operations using just the command line. Acryl Data maintains the pypi package for datahub.","sidebar":"overviewSidebar"},"docs/CODE_OF_CONDUCT":{"id":"docs/CODE_OF_CONDUCT","title":"Contributor Covenant Code of Conduct","description":"Our Pledge","sidebar":"overviewSidebar"},"docs/components":{"id":"docs/components","title":"Components","description":"The DataHub platform consists of the components shown in the following diagram.","sidebar":"overviewSidebar"},"docs/CONTRIBUTING":{"id":"docs/CONTRIBUTING","title":"Contributing","description":"We always welcome contributions to help make DataHub better. Take a moment to read this document if you would like to contribute.","sidebar":"overviewSidebar"},"docs/datahub_lite":{"id":"docs/datahub_lite","title":"DataHub Lite (Experimental)","description":"What is it?","sidebar":"overviewSidebar"},"docs/debugging":{"id":"docs/debugging","title":"Quickstart Debugging Guide","description":"For when Quickstart did not work out smoothly.","sidebar":"overviewSidebar"},"docs/deploy/aws":{"id":"docs/deploy/aws","title":"Deploying to AWS","description":"The following is a set of instructions to quickstart DataHub on AWS Elastic Kubernetes Service (EKS). Note, the guide","sidebar":"overviewSidebar"},"docs/deploy/confluent-cloud":{"id":"docs/deploy/confluent-cloud","title":"Integrating with Confluent Cloud","description":"DataHub provides the ability to easily leverage Confluent Cloud as your Kafka provider. To do so, you\'ll need to configure DataHub to talk to a broker and schema registry hosted by Confluent.","sidebar":"overviewSidebar"},"docs/deploy/gcp":{"id":"docs/deploy/gcp","title":"Deploying to GCP","description":"The following is a set of instructions to quickstart DataHub on GCP Google Kubernetes Engine (GKE). Note, the guide","sidebar":"overviewSidebar"},"docs/deploy/kubernetes":{"id":"docs/deploy/kubernetes","title":"Deploying with Kubernetes","description":"Introduction","sidebar":"overviewSidebar"},"docs/deploy/telemetry":{"id":"docs/deploy/telemetry","title":"DataHub Telemetry","description":"Overview of DataHub Telemetry","sidebar":"overviewSidebar"},"docs/dev-guides/timeline":{"id":"docs/dev-guides/timeline","title":"Timeline API","description":"The Timeline API supports viewing version history of schemas, documentation, tags, glossary terms, and other updates","sidebar":"overviewSidebar"},"docs/developers":{"id":"docs/developers","title":"Local Development","description":"Pre-requirements","sidebar":"overviewSidebar"},"docs/docker/development":{"id":"docs/docker/development","title":"Using Docker Images During Development","description":"We\'ve created a special docker-compose.dev.yml override file that should configure docker images to be easier to use","sidebar":"overviewSidebar"},"docs/domains":{"id":"docs/domains","title":"About DataHub Domains","description":"Starting in version 0.8.25, DataHub supports grouping data assets into logical collections called Domains. Domains are curated, top-level folders or categories where related assets can be explicitly grouped. Management of Domains can be centralized, or distributed out to Domain owners Currently, an asset can belong to only one Domain at a time.","sidebar":"overviewSidebar"},"docs/enrich-metadata":{"id":"docs/enrich-metadata","title":"Enriching Metadata in DataHub","description":"Metadata Enrichment is a powerful way to annotate entities within DataHub, supercharging data discoverability and ensuring end-users have quick access to critical context for a given entity, such as:","sidebar":"overviewSidebar"},"docs/faq-using-datahub":{"id":"docs/faq-using-datahub","title":"FAQ Using DataHub","description":"Logo for my platform is not appearing on the Home Page or search results","sidebar":"overviewSidebar"},"docs/features":{"id":"docs/features","title":"Features","description":"DataHub is a modern data catalog built to enable end-to-end data discovery, data observability, and data governance. This extensible metadata platform is built for developers to tame the complexity of their rapidly evolving data ecosystems and for data practitioners to leverage the total value of data within their organization.","sidebar":"overviewSidebar"},"docs/features/dataset-usage-and-query-history":{"id":"docs/features/dataset-usage-and-query-history","title":"About DataHub Dataset Usage & Query History","description":"Dataset Usage & Query History can give dataset-level information about the top queries which referenced a dataset.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/athena":{"id":"docs/generated/ingestion/sources/athena","title":"Athena","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/azure-ad":{"id":"docs/generated/ingestion/sources/azure-ad","title":"Azure AD","description":"Extracting DataHub Users","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/bigquery":{"id":"docs/generated/ingestion/sources/bigquery","title":"BigQuery","description":"Ingesting metadata from BigQuery requires using the bigquery module.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/business-glossary":{"id":"docs/generated/ingestion/sources/business-glossary","title":"Business Glossary","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/clickhouse":{"id":"docs/generated/ingestion/sources/clickhouse","title":"ClickHouse","description":"There are 2 sources that provide integration with ClickHouse","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/csv":{"id":"docs/generated/ingestion/sources/csv","title":"CSV","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/databricks":{"id":"docs/generated/ingestion/sources/databricks","title":"Databricks","description":"DataHub supports integration with Databricks ecosystem using a multitude of connectors, depending on your exact setup.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/dbt":{"id":"docs/generated/ingestion/sources/dbt","title":"dbt","description":"There are 2 sources that provide integration with dbt","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/delta-lake":{"id":"docs/generated/ingestion/sources/delta-lake","title":"Delta Lake","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/demo-data":{"id":"docs/generated/ingestion/sources/demo-data","title":"Demo Data","description":"This source loads sample data into DataHub. It is intended for demo and testing purposes only.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/druid":{"id":"docs/generated/ingestion/sources/druid","title":"Druid","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/elasticsearch":{"id":"docs/generated/ingestion/sources/elasticsearch","title":"Elasticsearch","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/feast":{"id":"docs/generated/ingestion/sources/feast","title":"Feast","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/file":{"id":"docs/generated/ingestion/sources/file","title":"File","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/file-based-lineage":{"id":"docs/generated/ingestion/sources/file-based-lineage","title":"File Based Lineage","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/glue":{"id":"docs/generated/ingestion/sources/glue","title":"Glue","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/hana":{"id":"docs/generated/ingestion/sources/hana","title":"SAP HANA","description":"Testing","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/hive":{"id":"docs/generated/ingestion/sources/hive","title":"Hive","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/iceberg":{"id":"docs/generated/ingestion/sources/iceberg","title":"Iceberg","description":"Testing","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/json-schema":{"id":"docs/generated/ingestion/sources/json-schema","title":"JSON Schemas","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/kafka":{"id":"docs/generated/ingestion/sources/kafka","title":"Kafka","description":"Extract Topics & Schemas from Apache Kafka or Confluent Cloud.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/kafka-connect":{"id":"docs/generated/ingestion/sources/kafka-connect","title":"Kafka Connect","description":"Integration Details","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/ldap":{"id":"docs/generated/ingestion/sources/ldap","title":"LDAP","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/looker":{"id":"docs/generated/ingestion/sources/looker","title":"Looker","description":"There are 2 sources that provide integration with Looker","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mariadb":{"id":"docs/generated/ingestion/sources/mariadb","title":"MariaDB","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/metabase":{"id":"docs/generated/ingestion/sources/metabase","title":"Metabase","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mode":{"id":"docs/generated/ingestion/sources/mode","title":"Mode","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mongodb":{"id":"docs/generated/ingestion/sources/mongodb","title":"MongoDB","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mssql":{"id":"docs/generated/ingestion/sources/mssql","title":"Microsoft SQL Server","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mysql":{"id":"docs/generated/ingestion/sources/mysql","title":"MySQL","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/nifi":{"id":"docs/generated/ingestion/sources/nifi","title":"NiFi","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/okta":{"id":"docs/generated/ingestion/sources/okta","title":"Okta","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/openapi":{"id":"docs/generated/ingestion/sources/openapi","title":"OpenAPI","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/oracle":{"id":"docs/generated/ingestion/sources/oracle","title":"Oracle","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/postgres":{"id":"docs/generated/ingestion/sources/postgres","title":"Postgres","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/powerbi":{"id":"docs/generated/ingestion/sources/powerbi","title":"PowerBI","description":"There are 2 sources that provide integration with PowerBI","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/presto":{"id":"docs/generated/ingestion/sources/presto","title":"Presto","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/presto-on-hive":{"id":"docs/generated/ingestion/sources/presto-on-hive","title":"Presto on Hive","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/pulsar":{"id":"docs/generated/ingestion/sources/pulsar","title":"Pulsar","description":"Integration Details","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/redash":{"id":"docs/generated/ingestion/sources/redash","title":"Redash","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/redshift":{"id":"docs/generated/ingestion/sources/redshift","title":"Redshift","description":"There are 3 sources that provide integration with Redshift","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/s3":{"id":"docs/generated/ingestion/sources/s3","title":"S3 Data Lake","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/sagemaker":{"id":"docs/generated/ingestion/sources/sagemaker","title":"SageMaker","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/salesforce":{"id":"docs/generated/ingestion/sources/salesforce","title":"Salesforce","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/snowflake":{"id":"docs/generated/ingestion/sources/snowflake","title":"Snowflake","description":"Snowflake Ingestion through the UI","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/sqlalchemy":{"id":"docs/generated/ingestion/sources/sqlalchemy","title":"SQLAlchemy","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/superset":{"id":"docs/generated/ingestion/sources/superset","title":"Superset","description":"Certified","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/tableau":{"id":"docs/generated/ingestion/sources/tableau","title":"Tableau","description":"Incubating","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/trino":{"id":"docs/generated/ingestion/sources/trino","title":"Trino","description":"There are 2 sources that provide integration with Trino","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/vertica":{"id":"docs/generated/ingestion/sources/vertica","title":"Vertica","description":"Integration Details","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/assertion":{"id":"docs/generated/metamodel/entities/assertion","title":"Assertion","description":"Assertion entity represents a data quality rule applied on dataset.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/chart":{"id":"docs/generated/metamodel/entities/chart","title":"Chart","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/container":{"id":"docs/generated/metamodel/entities/container","title":"Container","description":"A container of related data assets.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/corpGroup":{"id":"docs/generated/metamodel/entities/corpGroup","title":"CorpGroup","description":"CorpGroup represents an identity of a group of users in the enterprise.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/corpuser":{"id":"docs/generated/metamodel/entities/corpuser","title":"Corpuser","description":"CorpUser represents an identity of a person (or an account) in the enterprise.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dashboard":{"id":"docs/generated/metamodel/entities/dashboard","title":"Dashboard","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataFlow":{"id":"docs/generated/metamodel/entities/dataFlow","title":"DataFlow","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubAccessToken":{"id":"docs/generated/metamodel/entities/dataHubAccessToken","title":"DataHubAccessToken","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubExecutionRequest":{"id":"docs/generated/metamodel/entities/dataHubExecutionRequest","title":"DataHubExecutionRequest","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubIngestionSource":{"id":"docs/generated/metamodel/entities/dataHubIngestionSource","title":"DataHubIngestionSource","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubPolicy":{"id":"docs/generated/metamodel/entities/dataHubPolicy","title":"DataHubPolicy","description":"DataHub Policies represent access policies granted to users or groups on metadata operations like edit, view etc.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubRetention":{"id":"docs/generated/metamodel/entities/dataHubRetention","title":"DataHubRetention","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubRole":{"id":"docs/generated/metamodel/entities/dataHubRole","title":"DataHubRole","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubSecret":{"id":"docs/generated/metamodel/entities/dataHubSecret","title":"DataHubSecret","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubStepState":{"id":"docs/generated/metamodel/entities/dataHubStepState","title":"DataHubStepState","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubUpgrade":{"id":"docs/generated/metamodel/entities/dataHubUpgrade","title":"DataHubUpgrade","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubView":{"id":"docs/generated/metamodel/entities/dataHubView","title":"DataHubView","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataJob":{"id":"docs/generated/metamodel/entities/dataJob","title":"DataJob","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataPlatform":{"id":"docs/generated/metamodel/entities/dataPlatform","title":"Data Platform","description":"Data Platforms are systems or tools that contain Datasets, Dashboards, Charts, and all other kinds of data assets modeled in the metadata graph.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataPlatformInstance":{"id":"docs/generated/metamodel/entities/dataPlatformInstance","title":"DataPlatformInstance","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataProcess":{"id":"docs/generated/metamodel/entities/dataProcess","title":"DataProcess","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataProcessInstance":{"id":"docs/generated/metamodel/entities/dataProcessInstance","title":"DataProcessInstance","description":"DataProcessInstance represents an instance of a datajob/jobflow run","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataset":{"id":"docs/generated/metamodel/entities/dataset","title":"Dataset","description":"The dataset entity is one the most important entities in the metadata model. They represent collections of data that are typically represented as Tables or Views in a database (e.g. BigQuery, Snowflake, Redshift etc.), Streams in a stream-processing environment (Kafka, Pulsar etc.), bundles of data found as Files or Folders in data lake systems (S3, ADLS, etc.).","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/domain":{"id":"docs/generated/metamodel/entities/domain","title":"Domain","description":"A data domain within an organization.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/globalSettings":{"id":"docs/generated/metamodel/entities/globalSettings","title":"GlobalSettings","description":"Global settings for an the platform","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/glossaryNode":{"id":"docs/generated/metamodel/entities/glossaryNode","title":"GlossaryNode","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/glossaryTerm":{"id":"docs/generated/metamodel/entities/glossaryTerm","title":"GlossaryTerm","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/inviteToken":{"id":"docs/generated/metamodel/entities/inviteToken","title":"InviteToken","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlFeature":{"id":"docs/generated/metamodel/entities/mlFeature","title":"MlFeature","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlFeatureTable":{"id":"docs/generated/metamodel/entities/mlFeatureTable","title":"MlFeatureTable","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlModel":{"id":"docs/generated/metamodel/entities/mlModel","title":"MlModel","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlModelDeployment":{"id":"docs/generated/metamodel/entities/mlModelDeployment","title":"MlModelDeployment","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlModelGroup":{"id":"docs/generated/metamodel/entities/mlModelGroup","title":"MlModelGroup","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlPrimaryKey":{"id":"docs/generated/metamodel/entities/mlPrimaryKey","title":"MlPrimaryKey","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/notebook":{"id":"docs/generated/metamodel/entities/notebook","title":"Notebook","description":"\u26a0\ufe0f Notice: The Notebook entity is under active community development and IS NOT YET fully supported on the DataHub web application.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/post":{"id":"docs/generated/metamodel/entities/post","title":"Post","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/query":{"id":"docs/generated/metamodel/entities/query","title":"Query","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/schemaField":{"id":"docs/generated/metamodel/entities/schemaField","title":"SchemaField","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/tag":{"id":"docs/generated/metamodel/entities/tag","title":"Tag","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/telemetry":{"id":"docs/generated/metamodel/entities/telemetry","title":"Telemetry","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/test":{"id":"docs/generated/metamodel/entities/test","title":"Test","description":"A DataHub test","sidebar":"overviewSidebar"},"docs/get-started-with-datahub":{"id":"docs/get-started-with-datahub","title":"Get Started With DataHub","description":"These guides are focused on helping you get up and running with DataHub as quickly as possible.","sidebar":"overviewSidebar"},"docs/glossary/business-glossary":{"id":"docs/glossary/business-glossary","title":"Business Glossary","description":"Introduction","sidebar":"overviewSidebar"},"docs/how/add-custom-data-platform":{"id":"docs/how/add-custom-data-platform","title":"Adding a custom Dataset Data Platform","description":"A Data Platform represents a 3rd party system from which Metadata Entities are ingested from. Each Dataset that is ingested is associated with a single platform, for example MySQL, Snowflake, Redshift, or BigQuery.","sidebar":"overviewSidebar"},"docs/how/add-custom-ingestion-source":{"id":"docs/how/add-custom-ingestion-source","title":"Using a Custom Ingestion Source","description":"Adding a custom ingestion source is the easiest way to extend Datahubs ingestion framework to support source systems","sidebar":"overviewSidebar"},"docs/how/add-new-aspect":{"id":"docs/how/add-new-aspect","title":"How to add a new metadata aspect?","description":"Adding a new metadata aspect is one of the most common ways to extend an existing entity."},"docs/how/add-user-data":{"id":"docs/how/add-user-data","title":"Adding user metadata in DataHub","description":"This guide shares how you can add user metadata in DataHub. Usually you would want to use one of our sources for ingesting user metadata. But if there is no connector for your use case then you would want to use this guide."},"docs/how/backup-datahub":{"id":"docs/how/backup-datahub","title":"Taking backup of DataHub","description":"Production","sidebar":"overviewSidebar"},"docs/how/configuring-authorization-with-apache-ranger":{"id":"docs/how/configuring-authorization-with-apache-ranger","title":"Configuring Authorization with Apache Ranger","description":"DataHub integration with Apache Ranger allows DataHub Authorization policies to be controlled inside Apache Ranger.","sidebar":"overviewSidebar"},"docs/how/delete-metadata":{"id":"docs/how/delete-metadata","title":"Removing Metadata from DataHub","description":"There are a two ways to delete metadata from DataHub:","sidebar":"overviewSidebar"},"docs/how/extract-container-logs":{"id":"docs/how/extract-container-logs","title":"How to Extract Logs from DataHub Containers","description":"DataHub containers, datahub GMS (backend server) and datahub frontend (UI server), write log files to the local container filesystem. To extract these logs, you\'ll need to get them from inside the container where the services are running.","sidebar":"overviewSidebar"},"docs/how/kafka-config":{"id":"docs/how/kafka-config","title":"Configuring Kafka","description":"DataHub requires Kafka to operate. Kafka is used as a durable log that can be used to store inbound","sidebar":"overviewSidebar"},"docs/how/migrating-graph-service-implementation":{"id":"docs/how/migrating-graph-service-implementation","title":"Migrate Graph Service Implementation to Elasticsearch","description":"We currently support either Elasticsearch or Neo4j as backend implementations for the graph service. We recommend","sidebar":"overviewSidebar"},"docs/how/restore-indices":{"id":"docs/how/restore-indices","title":"Restoring Search and Graph Indices from Local Database","description":"If search or graph services go down or you have made changes to them that require reindexing, you can restore them from","sidebar":"overviewSidebar"},"docs/how/search":{"id":"docs/how/search","title":"About DataHub Search","description":"\x3c!--","sidebar":"overviewSidebar"},"docs/how/ui-tabs-guide":{"id":"docs/how/ui-tabs-guide","title":"UI Tabs Guide","description":"Some of the tabs in the UI might not be enabled by default. This guide is supposed to tell Admins of DataHub how to enable those UI tabs."},"docs/how/updating-datahub":{"id":"docs/how/updating-datahub","title":"Updating DataHub","description":"This file documents any backwards-incompatible changes in DataHub and assists people when migrating to a new version.","sidebar":"overviewSidebar"},"docs/lineage/airflow":{"id":"docs/lineage/airflow","title":"Airflow Integration","description":"DataHub supports integration of","sidebar":"overviewSidebar"},"docs/lineage/lineage-feature-guide":{"id":"docs/lineage/lineage-feature-guide","title":"About DataHub Lineage","description":"Lineage is used to capture data dependencies within an organization. It allows you to track the inputs from which a data asset is derived, along with the data assets that depend on it downstream.","sidebar":"overviewSidebar"},"docs/links":{"id":"docs/links","title":"Articles & Talks","description":"Overviews","sidebar":"overviewSidebar"},"docs/managed-datahub/approval-workflows":{"id":"docs/managed-datahub/approval-workflows","title":"About DataHub Approval Workflows","description":"Overview","sidebar":"overviewSidebar"},"docs/managed-datahub/chrome-extension":{"id":"docs/managed-datahub/chrome-extension","title":"Local Chrome Extension","description":"Learn how to upload and use the Acryl DataHub Chrome extension (beta) locally before it\'s available on the Chrome store.","sidebar":"overviewSidebar"},"docs/managed-datahub/datahub-api/entity-events-api":{"id":"docs/managed-datahub/datahub-api/entity-events-api","title":"Entity Events API","description":"This guide details the Entity Events API, which allows you to take action when things change on DataHub.","sidebar":"overviewSidebar"},"docs/managed-datahub/datahub-api/graphql-api/getting-started":{"id":"docs/managed-datahub/datahub-api/graphql-api/getting-started","title":"Getting Started","description":"Getting started with the DataHub GraphQL API.","sidebar":"overviewSidebar"},"docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta":{"id":"docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta","title":"Incidents API (Beta)","description":"This page provides an overview of working with the DataHub Incidents API.","sidebar":"overviewSidebar"},"docs/managed-datahub/integrations/aws-privatelink":{"id":"docs/managed-datahub/integrations/aws-privatelink","title":"AWS PrivateLink","description":"If you require a private connection between the provisioned DataHub instance and your own existing AWS account, Acryl supports using AWS PrivateLink in order to complete this private connection.","sidebar":"overviewSidebar"},"docs/managed-datahub/integrations/oidc-sso-integration":{"id":"docs/managed-datahub/integrations/oidc-sso-integration","title":"OIDC SSO Integration","description":"This page will help you set up OIDC SSO with your identity provider to log into Acryl Data","sidebar":"overviewSidebar"},"docs/managed-datahub/managed-datahub-overview":{"id":"docs/managed-datahub/managed-datahub-overview","title":"Managed DataHub Exclusives","description":"Acryl DataHub offers a slew of additional features on top of the normal OSS project.","sidebar":"overviewSidebar"},"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion":{"id":"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion","title":"Ingestion","description":"Acryl Metadata Ingestion functions similarly to that in open source DataHub. Sources are configured via the UI Ingestion or via a Recipe, ingestion recipes can be scheduled using your system of choice, and metadata can be pushed from anywhere.","sidebar":"overviewSidebar"},"docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge":{"id":"docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge","title":"Setting up Events API on AWS EventBridge","description":"This guide will walk through the configuration required to start receiving Acryl DataHub events via AWS EventBridge.","sidebar":"overviewSidebar"},"docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws":{"id":"docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws","title":"Setting up Remote Ingestion Executor on AWS","description":"This page describes the steps required to configure a remote ingestion executor, which allows you to ingest metadata from private metadata sources using private credentials via the DataHub UI.","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_69":{"id":"docs/managed-datahub/release-notes/v_0_1_69","title":"v0.1.69","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_70":{"id":"docs/managed-datahub/release-notes/v_0_1_70","title":"v0.1.70","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_72":{"id":"docs/managed-datahub/release-notes/v_0_1_72","title":"v0.1.72","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_73":{"id":"docs/managed-datahub/release-notes/v_0_1_73","title":"v0.1.73","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_0":{"id":"docs/managed-datahub/release-notes/v_0_2_0","title":"v0.2.0","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_1":{"id":"docs/managed-datahub/release-notes/v_0_2_1","title":"v0.2.1","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_2":{"id":"docs/managed-datahub/release-notes/v_0_2_2","title":"v0.2.2","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_3":{"id":"docs/managed-datahub/release-notes/v_0_2_3","title":"v0.2.3","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_4":{"id":"docs/managed-datahub/release-notes/v_0_2_4","title":"v0.2.4","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_5":{"id":"docs/managed-datahub/release-notes/v_0_2_5","title":"v0.2.5","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/saas-slack-setup":{"id":"docs/managed-datahub/saas-slack-setup","title":"Configure Slack Notifications","description":"Install the DataHub Slack App into your Slack workspace","sidebar":"overviewSidebar"},"docs/managed-datahub/welcome-acryl":{"id":"docs/managed-datahub/welcome-acryl","title":"Getting Started with Acryl DataHub","description":"Welcome to the Acryl DataHub! We at Acryl are on a mission to make data reliable by bringing clarity to the who, what, when, & how of your data ecosystem. We\'re thrilled to be on this journey with you; and cannot wait to see what we build together!","sidebar":"overviewSidebar"},"docs/modeling/extending-the-metadata-model":{"id":"docs/modeling/extending-the-metadata-model","title":"Extending the Metadata Model","description":"You can extend the metadata model by either creating a new Entity or extending an existing one. Unsure if you need to","sidebar":"overviewSidebar"},"docs/modeling/metadata-model":{"id":"docs/modeling/metadata-model","title":"The Metadata Model","description":"DataHub takes a schema-first approach to modeling metadata. We use the open-source Pegasus schema language (PDL) extended with a custom set of annotations to model metadata. The DataHub storage, serving, indexing and ingestion layer operates directly on top of the metadata model and supports strong types all the way from the client to the storage layer.","sidebar":"overviewSidebar"},"docs/platform-instances":{"id":"docs/platform-instances","title":"Working With Platform Instances","description":"DataHub\'s metadata model for Datasets supports a three-part key currently:","sidebar":"overviewSidebar"},"docs/plugins":{"id":"docs/plugins","title":"Plugins Guide","description":"Plugins are way to enhance the basic DataHub functionality in a custom manner.","sidebar":"overviewSidebar"},"docs/posts":{"id":"docs/posts","title":"About DataHub Posts","description":"DataHub allows users to make Posts that can be displayed on the app. Currently, Posts are only supported on the Home Page, but may be extended to other surfaces of the app in the future. Posts can be used to accomplish the following:","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/bigquery/configuration":{"id":"docs/quick-ingestion-guides/bigquery/configuration","title":"Configuration","description":"Now that you have created a Service Account and Service Account Key in BigQuery in the prior step, it\'s now time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/bigquery/overview":{"id":"docs/quick-ingestion-guides/bigquery/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/bigquery/setup":{"id":"docs/quick-ingestion-guides/bigquery/setup","title":"Setup","description":"To configure ingestion from BigQuery, you\'ll need a Service Account configured with the proper permission sets and an associated Service Account Key.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/powerbi/configuration":{"id":"docs/quick-ingestion-guides/powerbi/configuration","title":"Configuration","description":"Now that you have created a DataHub specific Azure AD app with the relevant access in the prior step, it\'s now time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/powerbi/overview":{"id":"docs/quick-ingestion-guides/powerbi/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/powerbi/setup":{"id":"docs/quick-ingestion-guides/powerbi/setup","title":"Setup","description":"In order to configure ingestion from PowerBI, you\'ll first have to ensure you have an Azure AD app with permission to access the PowerBI resources.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/redshift/configuration":{"id":"docs/quick-ingestion-guides/redshift/configuration","title":"Configuration","description":"Now that you have created a DataHub user in Redshift in the prior step, it\'s time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/redshift/overview":{"id":"docs/quick-ingestion-guides/redshift/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/redshift/setup":{"id":"docs/quick-ingestion-guides/redshift/setup","title":"Setup","description":"To configure ingestion from Redshift, you\'ll need a User configured with the proper permission sets, and an associated.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/snowflake/configuration":{"id":"docs/quick-ingestion-guides/snowflake/configuration","title":"Configuration","description":"Now that you have created a DataHub-specific user with the relevant roles in Snowflake in the prior step, it\'s now time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/snowflake/overview":{"id":"docs/quick-ingestion-guides/snowflake/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/snowflake/setup":{"id":"docs/quick-ingestion-guides/snowflake/setup","title":"Setup","description":"In order to configure ingestion from Snowflake, you\'ll first have to ensure you have a Snowflake user with the ACCOUNTADMIN role or MANAGE GRANTS privilege.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/tableau/configuration":{"id":"docs/quick-ingestion-guides/tableau/configuration","title":"Configuration","description":"Now that you have created a DataHub-specific user with the relevant access in Tableau in the prior step, it\'s now time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/tableau/overview":{"id":"docs/quick-ingestion-guides/tableau/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/tableau/setup":{"id":"docs/quick-ingestion-guides/tableau/setup","title":"Setup","description":"In order to configure ingestion from Tableau, you\'ll first have to enable Tableau Metadata API and you should have a user with Site Administrator Explorer permissions.","sidebar":"overviewSidebar"},"docs/quickstart":{"id":"docs/quickstart","title":"DataHub Quickstart Guide","description":"Deploying DataHub","sidebar":"overviewSidebar"},"docs/rfc":{"id":"docs/rfc","title":"DataHub RFC Process","description":"What is an RFC?","sidebar":"overviewSidebar"},"docs/roadmap":{"id":"docs/roadmap","title":"DataHub Roadmap","description":"The DataHub Roadmap has a new home!"},"docs/saas":{"id":"docs/saas","title":"Managed DataHub","description":"Sign up for fully managed, hassle-free and secure SaaS service for DataHub, provided by Acryl Data.","sidebar":"overviewSidebar"},"docs/schema-history":{"id":"docs/schema-history","title":"About DataHub Schema History","description":"Schema History is a valuable tool for understanding how a Dataset changes over time and gives insight into the following cases,","sidebar":"overviewSidebar"},"docs/slack":{"id":"docs/slack","title":"Slack","description":"The DataHub Slack is a thriving and rapidly growing community - we can\'t wait for you to join us!","sidebar":"overviewSidebar"},"docs/sync-status":{"id":"docs/sync-status","title":"About DataHub Sync Status","description":"When looking at metadata in DataHub, it\'s useful to know if the information you\'re looking at is relevant.","sidebar":"overviewSidebar"},"docs/tags":{"id":"docs/tags","title":"About DataHub Tags","description":"Tags are informal, loosely controlled labels that help in search & discovery. They can be added to datasets, dataset schemas, or containers, for an easy way to label or categorize entities \u2013 without having to associate them to a broader business glossary or vocabulary.","sidebar":"overviewSidebar"},"docs/tests/metadata-tests":{"id":"docs/tests/metadata-tests","title":"About Metadata Tests","description":"DataHub includes a highly configurable, no-code framework that allows you to configure broad-spanning monitors & continuous actions","sidebar":"overviewSidebar"},"docs/townhall-history":{"id":"docs/townhall-history","title":"Town Hall History","description":"A list of previous Town Halls, their planned schedule, and the recording of the meeting.","sidebar":"overviewSidebar"},"docs/townhalls":{"id":"docs/townhalls","title":"DataHub Town Halls","description":"We hold regular virtual town hall meetings to meet with DataHub community.","sidebar":"overviewSidebar"},"docs/ui-ingestion":{"id":"docs/ui-ingestion","title":"UI Ingestion Guide","description":"Introduction","sidebar":"overviewSidebar"},"docs/what-is-datahub/datahub-concepts":{"id":"docs/what-is-datahub/datahub-concepts","title":"DataHub Concepts","description":"Explore key concepts of DataHub to take full advantage of its capabilities in managing your data.","sidebar":"overviewSidebar"},"docs/what/aspect":{"id":"docs/what/aspect","title":"What is a metadata aspect?","description":"A metadata aspect is a structured document, or more precisely a record in PDL,"},"docs/what/delta":{"id":"docs/what/delta","title":"What is a metadata delta?","description":"Rest.li supports partial update natively without needing explicitly defined models."},"docs/what/entity":{"id":"docs/what/entity","title":"Entities","description":"This page has been moved. Please refer to The Metadata Model for details on"},"docs/what/gma":{"id":"docs/what/gma","title":"What is Generalized Metadata Architecture (GMA)?","description":"GMA is the backend infrastructure for DataHub. Unlike existing architectures, GMA leverages multiple storage technologies to efficiently service the four most commonly used query patterns"},"docs/what/gms":{"id":"docs/what/gms","title":"What is Generalized Metadata Service (GMS)?","description":"Metadata for entities onboarded to GMA is served through microservices known as Generalized Metadata Service (GMS). GMS typically provides a Rest.li API and must access the metadata using GMA DAOs."},"docs/what/graph":{"id":"docs/what/graph","title":"What is GMA graph?","description":"All the entities and relationships are stored in a graph database, Neo4j."},"docs/what/mxe":{"id":"docs/what/mxe","title":"Metadata Events","description":"DataHub makes use a few important Kafka events for operation. The most notable of these include","sidebar":"overviewSidebar"},"docs/what/relationship":{"id":"docs/what/relationship","title":"What is a relationship?","description":"A relationship is a named associate between exactly two entities, a source and a destination."},"docs/what/search-document":{"id":"docs/what/search-document","title":"What is a search document?","description":"Search documents are also modeled using PDL explicitly."},"docs/what/search-index":{"id":"docs/what/search-index","title":"What is GMA search index?","description":"Each search document type (or entity type) will be mapped to an independent search index in Elasticsearch."},"docs/what/snapshot":{"id":"docs/what/snapshot","title":"What is a snapshot?","description":"A metadata snapshot models the current state of one or multiple metadata aspects associated with a particular entity."},"docs/what/urn":{"id":"docs/what/urn","title":"What is URN?","description":"URN (Uniform Resource Name) is the chosen scheme of URI to uniquely define any resource in DataHub. It has the following form"},"graphql/enums":{"id":"graphql/enums","title":"Enums","description":"AccessLevel","sidebar":"overviewSidebar"},"graphql/inputObjects":{"id":"graphql/inputObjects","title":"Input objects","description":"AcceptRoleInput","sidebar":"overviewSidebar"},"graphql/interfaces":{"id":"graphql/interfaces","title":"Interfaces","description":"Aspect","sidebar":"overviewSidebar"},"graphql/mutations":{"id":"graphql/mutations","title":"Mutations","description":"acceptRole","sidebar":"overviewSidebar"},"graphql/objects":{"id":"graphql/objects","title":"Objects","description":"AccessToken","sidebar":"overviewSidebar"},"graphql/queries":{"id":"graphql/queries","title":"Queries","description":"appConfig","sidebar":"overviewSidebar"},"graphql/scalars":{"id":"graphql/scalars","title":"Scalars","description":"Boolean","sidebar":"overviewSidebar"},"graphql/unions":{"id":"graphql/unions","title":"Unions","description":"AnalyticsChart","sidebar":"overviewSidebar"},"metadata-ingestion-modules/airflow-plugin/README":{"id":"metadata-ingestion-modules/airflow-plugin/README","title":"Datahub Airflow Plugin","description":"See the DataHub Airflow docs for details."},"metadata-ingestion/adding-source":{"id":"metadata-ingestion/adding-source","title":"Adding a Metadata Ingestion Source","description":"There are two ways of adding a metadata ingestion source.","sidebar":"overviewSidebar"},"metadata-ingestion/as-a-library":{"id":"metadata-ingestion/as-a-library","title":"Python Emitter","description":"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc.","sidebar":"overviewSidebar"},"metadata-ingestion/developing":{"id":"metadata-ingestion/developing","title":"Developing on Metadata Ingestion","description":"If you just want to use metadata ingestion, check the user-centric guide.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source":{"id":"metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source","title":"Adding Stateful Ingestion to a Source","description":"Currently, datahub supports the Stale Metadata Removal and","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/classification":{"id":"metadata-ingestion/docs/dev_guides/classification","title":"Classification","description":"The classification feature enables sources to be configured to automatically predict info types for columns and use them as glossary terms. This is an explicit opt-in feature and is not enabled by default.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/reporting_telemetry":{"id":"metadata-ingestion/docs/dev_guides/reporting_telemetry","title":"Datahub\'s Reporting Framework for Ingestion Job Telemetry","description":"The Datahub\'s reporting framework allows for configuring reporting providers with the ingestion pipelines to send","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/sql_profiles":{"id":"metadata-ingestion/docs/dev_guides/sql_profiles","title":"SQL Profiling","description":"SQL Profiling collects table level and column level statistics.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/stateful":{"id":"metadata-ingestion/docs/dev_guides/stateful","title":"Stateful Ingestion","description":"The stateful ingestion feature enables sources to be configured to save custom checkpoint states from their","sidebar":"overviewSidebar"},"metadata-ingestion/docs/transformer/dataset_transformer":{"id":"metadata-ingestion/docs/transformer/dataset_transformer","title":"Dataset","description":"The below table shows transformer which can transform aspects of entity Dataset.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/transformer/intro":{"id":"metadata-ingestion/docs/transformer/intro","title":"Introduction","description":"What\u2019s a transformer?","sidebar":"overviewSidebar"},"metadata-ingestion/examples/transforms/README":{"id":"metadata-ingestion/examples/transforms/README","title":"Custom transformer script","description":"This script sets up a transformer that reads in a list of owner URNs from a JSON file specified via owners_json and appends these owners to every MCE."},"metadata-ingestion/integration_docs/great-expectations":{"id":"metadata-ingestion/integration_docs/great-expectations","title":"Great Expectations","description":"This guide helps to setup and configure DataHubValidationAction in Great Expectations to send assertions(expectations) and their results to DataHub using DataHub\'s Python Rest emitter.","sidebar":"overviewSidebar"},"metadata-ingestion/README":{"id":"metadata-ingestion/README","title":"Introduction to Metadata Ingestion","description":"Integration Options","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/airflow":{"id":"metadata-ingestion/schedule_docs/airflow","title":"Using Airflow","description":"If you are using Apache Airflow for your scheduling then you might want to also use it for scheduling your ingestion recipes. For any Airflow specific questions you can go through Airflow docs for more details.","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/cron":{"id":"metadata-ingestion/schedule_docs/cron","title":"Using Cron","description":"Assume you have a recipe file /home/ubuntu/datahubingest/mysqlto_datahub.yml on your machine","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/datahub":{"id":"metadata-ingestion/schedule_docs/datahub","title":"Using DataHub","description":"UI Ingestion can be used to schedule metadata ingestion through DataHub."},"metadata-ingestion/schedule_docs/intro":{"id":"metadata-ingestion/schedule_docs/intro","title":"Introduction to Scheduling Metadata Ingestion","description":"Given a recipe file /home/ubuntu/datahubingest/mysqlto_datahub.yml.","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/kubernetes":{"id":"metadata-ingestion/schedule_docs/kubernetes","title":"Using Kubernetes","description":"If you have deployed DataHub using our official helm charts you can use the","sidebar":"overviewSidebar"},"metadata-ingestion/sink_docs/console":{"id":"metadata-ingestion/sink_docs/console","title":"Console","description":"For context on getting started with ingestion, check out our metadata ingestion guide.","sidebar":"overviewSidebar"},"metadata-ingestion/sink_docs/datahub":{"id":"metadata-ingestion/sink_docs/datahub","title":"DataHub","description":"DataHub Rest","sidebar":"overviewSidebar"},"metadata-ingestion/sink_docs/file":{"id":"metadata-ingestion/sink_docs/file","title":"File","description":"For context on getting started with ingestion, check out our metadata ingestion guide.","sidebar":"overviewSidebar"},"metadata-ingestion/source-docs-template":{"id":"metadata-ingestion/source-docs-template","title":"Source Name","description":"Certified"},"metadata-integration/java/as-a-library":{"id":"metadata-integration/java/as-a-library","title":"Java Emitter","description":"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc.","sidebar":"overviewSidebar"},"metadata-integration/java/datahub-protobuf/README":{"id":"metadata-integration/java/datahub-protobuf/README","title":"Protobuf Schemas","description":"The datahub-protobuf module is designed to be used with the Java Emitter, the input is a compiled protobuf binary .protoc files and optionally the corresponding .proto source code. In addition, you can supply the root message in cases where a single protobuf source file includes multiple non-nested messages.","sidebar":"overviewSidebar"},"metadata-integration/java/spark-lineage/README":{"id":"metadata-integration/java/spark-lineage/README","title":"Spark","description":"To integrate Spark with DataHub, we provide a lightweight Java agent that listens for Spark application and job events and pushes metadata out to DataHub in real-time. The agent listens to events such application start/end, and SQLExecution start/end to create pipelines (i.e. DataJob) and tasks (i.e. DataFlow) in Datahub along with lineage to datasets that are being read from and written to. Read on to learn how to configure this for different Spark scenarios.","sidebar":"overviewSidebar"},"metadata-jobs/mae-consumer-job/README":{"id":"metadata-jobs/mae-consumer-job/README","title":"metadata-jobs:mae-consumer-job","description":"The Metadata Audit Event Consumer is a Spring job which can be deployed by itself, or as part of the Metadata Service.","sidebar":"overviewSidebar"},"metadata-jobs/mce-consumer-job/README":{"id":"metadata-jobs/mce-consumer-job/README","title":"metadata-jobs:mce-consumer-job","description":"The Metadata Change Event Consumer is a Spring job which can be deployed by itself, or as part of the Metadata Service.","sidebar":"overviewSidebar"},"metadata-jobs/README":{"id":"metadata-jobs/README","title":"MXE Processing Jobs","description":"DataHub uses Kafka as the pub-sub message queue in the backend. There are 2 Kafka topics used by DataHub which are"},"metadata-models-custom/README":{"id":"metadata-models-custom/README","title":"A Custom Metadata Model","description":"This module hosts a gradle project where you can store your custom metadata model. It contains an example extension for you to follow."},"metadata-service/README":{"id":"metadata-service/README","title":"metadata-service","description":"DataHub Metadata Service is a service written in Java consisting of multiple servlets:","sidebar":"overviewSidebar"},"perf-test/README":{"id":"perf-test/README","title":"Load testing with Locust","description":"Locust is an open-source, python-based, easy-to-use load testing tool. It provides an interface to"},"python-sdk/builder":{"id":"python-sdk/builder","title":"Builder","description":"\\\\n\\\\n\\\\nThese classes and methods make it easier to construct MetadataChangeProposals and MetadataChangeEvents.\\\\n\\\\n\\\\nclass datahub.emitter.mcp.MetadataChangeProposalWrapper(entityType=\'ENTITYTYPEUNSET\', changeType=\'UPSERT\', entityUrn=None, entityKeyAspect=None, auditHeader=None, aspectName=None, aspect=None, systemMetadata=None)\\\\nBases\\\\n\\\\nentityType (str)\\\\nchangeType (Union[str, ChangeTypeClass])\\\\nentityUrn (Optional[str])\\\\nentityKeyAspect (Optional[Aspect])\\\\nauditHeader (Optional[KafkaAuditHeaderClass])\\\\naspectName (Optional[str])\\\\naspect (Optional[Aspect])\\\\nsystemMetadata (Optional[SystemMetadataClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nentityType Union[str, ChangeTypeClass] = \'UPSERT\'\\\\n\\\\n\\\\n\\\\nentityUrn Optional[Aspect] = None\\\\n\\\\n\\\\n\\\\nauditHeader Optional[str] = None\\\\n\\\\n\\\\n\\\\naspect Optional[SystemMetadataClass] = None\\\\n\\\\n\\\\n\\\\nclassmethod constructmany(entityUrn, aspects)\\\\n\\\\nParameters\\\\nList[MetadataChangeProposalWrapper]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nmakemcp()\\\\n\\\\nReturn type\\\\nbool\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntoobj(tuples=False, simplifiedstructure=False)\\\\n\\\\nParameters\\\\ndict\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod fromobj(obj, tuples=False)\\\\nAttempt to deserialize into an MCPW, but fall back\\\\nto a standard MCP if we\\\\u2019re missing codegen\\\\u2019d classes for the\\\\nentity key or aspect.\\\\n\\\\nParameters\\\\nUnion[MetadataChangeProposalWrapper, MetadataChangeProposalClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod fromobjrequirewrapper(obj, tuples=False)\\\\n\\\\nParameters\\\\nMetadataChangeProposalWrapper\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nasworkunit()\\\\n\\\\nReturn type\\\\nvalue (bool)\\\\n\\\\nReturn type Enum\\\\nAn enumeration.\\\\n\\\\n\\\\nUSER = \'corpuser\'\\\\n\\\\n\\\\n\\\\nGROUP = \'corpGroup\'\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcebuilder.getsystime()\\\\n\\\\nReturn type\\\\nplatform (str)\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\ninstance (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nplatforminstance (Optional[str])\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nparenturn (str)\\\\nfieldpath (str)\\\\n\\\\n\\\\nReturn type\\\\nschemafieldurn (str)\\\\n\\\\nReturn type\\\\ndataseturn (str)\\\\n\\\\nReturn type\\\\nkey (DatasetKeyClass)\\\\n\\\\nReturn type\\\\nguid (str)\\\\n\\\\nReturn type\\\\nguid (str)\\\\n\\\\nReturn type\\\\nobj (dict)\\\\n\\\\nReturn type\\\\nassertionid (str)\\\\n\\\\nReturn type\\\\nassertionurn (str)\\\\n\\\\nReturn type\\\\nusername (str)\\\\n\\\\nReturn type\\\\ngroupname (str)\\\\n\\\\nReturn type\\\\ntag (str)\\\\n\\\\nReturn type\\\\n\\\\nowner (str)\\\\nownertype (OwnerType)\\\\n\\\\n\\\\nReturn type\\\\nterm (str)\\\\n\\\\nReturn type\\\\n\\\\norchestrator (str)\\\\nflowid (str)\\\\ncluster (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nflowurn (str)\\\\njobid (str)\\\\n\\\\n\\\\nReturn type\\\\ndataProcessInstanceId (str)\\\\n\\\\nReturn type\\\\n\\\\norchestrator (str)\\\\nflowid (str)\\\\njobid (str)\\\\ncluster (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\ndashboardurn (str)\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\ncharturn (str)\\\\n\\\\nReturn type\\\\ndomain (str)\\\\n\\\\nReturn type\\\\n\\\\nfeaturetablename (str)\\\\nprimarykeyname (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nfeaturetablename (str)\\\\nfeaturename (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nfeaturetablename (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nmodelname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\ndeploymentname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\ngroupname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\nownershiptype (Optional[str])\\\\n\\\\nReturn type\\\\nownershiptype (Optional[str])\\\\n\\\\nReturn type\\\\n\\\\nupstreamurns (List[str])\\\\ndownstreamurn (str)\\\\nlineagetype (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\nAspectType (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\nAspectType (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\nAspectType (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\ndefault (TypeVar(Aspect, bound= Aspect))\\\\n\\\\n\\\\nReturn type\\\\ntags (List[str])\\\\n\\\\nReturn type\\\\n\\\\nownerurns (List[str])\\\\nsourcetype (Union[str, OwnershipSourceTypeClass, None])\\\\nownertype (Union[str, OwnershipTypeClass])\\\\n\\\\n\\\\nReturn type\\\\ntermurns (List[str])\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\naspect (Optional[TypeVar(Aspect, bound= Aspect)])\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type BaseModel\\\\n\\\\nParameters\\\\nDict[str, str]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nguid()\\\\n\\\\nReturn type DatahubKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\ninstance Optional[str]\\\\n\\\\n\\\\n\\\\nguiddict()\\\\n\\\\nReturn type PlatformKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcp_builder.SchemaKey(data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompatinstanceforguid (str | None)\\\\ndatabase (str)\\\\nschema (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndbschema PlatformKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.MetastoreKey(**data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompatinstanceforguid (str | None)\\\\nmetastore (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nmetastore MetastoreKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.UnitySchemaKey(**data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompatinstanceforguid (str | None)\\\\nmetastore (str)\\\\ncatalog (str)\\\\nunityschema (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nunityschema ProjectIdKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.FolderKey(data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompat_instance_for_guid (str | None)\\\\nfolder_abs_path (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nfolder_abs_path PlatformKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.DatahubKeyJSONEncoder(*, skipkeys=False, ensureascii=True, checkcircular=True, allownan=True, sortkeys=False, indent=None, separators=None, default=None)\\\\nBases\\\\ndef default(self, o)\\\\n        iterable = iter(o)\\\\n    except TypeError\\\\n        return list(iterable)\\\\n    # Let the base class default method raise the TypeError\\\\n    return JSONEncoder.default(self, o)\\\\n\\\\n\\\\n\\\\nParameters\\\\nAny\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.adddomaintoentitywu(entityurn, domainurn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.addownertoentitywu(entitytype, entityurn, ownerurn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.addtagstoentitywu(entitytype, entityurn, tags)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.wrapaspectasworkunit(entityName, entityUrn, aspectName, aspect)\\\\n\\\\nParameters\\\\nMetadataWorkUnit\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.gencontainers(containerkey, name, subtypes, parentcontainerkey=None, extraproperties=None, domainurn=None, description=None, ownerurn=None, externalurl=None, tags=None, qualifiedname=None, created=None, lastmodified=None)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.adddatasettocontainer(containerkey, dataseturn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.addentitytocontainer(containerkey, entitytype, entityurn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.mcpsfrommce(mce)\\\\n\\\\nParameters\\\\nIterable[MetadataChangeProposalWrapper]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.createembedmcp(urn, embedurl)\\\\n\\\\nParameters\\\\nMetadataChangeProposalWrapper\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"overviewSidebar"},"python-sdk/clients":{"id":"python-sdk/clients","title":"Client","description":"\\\\n\\\\n\\\\nThe Kafka emitter or Rest emitter can be used to push metadata to DataHub.\\\\nThe DataHub graph client extends the Rest emitter with additional functionality.\\\\n\\\\n\\\\nclass datahub.emitter.restemitter.DataHubRestEmitter(gmsserver, token=None, connecttimeoutsec=None, readtimeoutsec=None, retrystatuscodes=None, retrymethods=None, retrymaxtimes=None, extraheaders=None, cacertificatepath=None, servertelemetryid=None, disablesslverification=False)\\\\nBases\\\\n\\\\ngmsserver (str)\\\\ntoken (Optional[str])\\\\nconnecttimeoutsec (Optional[float])\\\\nreadtimeoutsec (Optional[float])\\\\nretrystatuscodes (Optional[List[int]])\\\\nretrymethods (Optional[List[str]])\\\\nretrymaxtimes (Optional[int])\\\\nextraheaders (Optional[Dict[str, str]])\\\\ncacertificatepath (Optional[str])\\\\nservertelemetryid (Optional[str])\\\\ndisablesslverification (bool)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntestconnection()\\\\n\\\\nReturn type\\\\n\\\\nitem (Union[MetadataChangeEventClass, MetadataChangeProposalClass, MetadataChangeProposalWrapper, UsageAggregationClass])\\\\ncallback (Optional[Callable[[Exception, str], None]])\\\\n\\\\n\\\\nReturn type\\\\nmce (MetadataChangeEventClass)\\\\n\\\\nReturn type\\\\nmcp (Union[MetadataChangeProposalClass, MetadataChangeProposalWrapper])\\\\n\\\\nReturn type\\\\nusageStats (UsageAggregationClass)\\\\n\\\\nReturn type\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.restemitter.DatahubRestEmitter\\\\nalias of DataHubRestEmitter\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.kafkaemitter.KafkaEmitterConfig(**data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nconnection (KafkaProducerConnectionConfig)\\\\ntopicroutes (Dict[str, str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nconnection Dict[str, str]\\\\n\\\\n\\\\n\\\\nclassmethod validatetopicroutes(v)\\\\n\\\\nParameters\\\\nDict[str, str]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.kafkaemitter.DatahubKafkaEmitter(config)\\\\nBases\\\\nconfig (KafkaEmitterConfig)\\\\n\\\\n\\\\n\\\\n\\\\nemit(item, callback=None)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nemitmceasync(mce, callback)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nemitmcpasync(mcp, callback)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nflush()\\\\n\\\\nReturn type\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.ingestion.graph.client.DatahubClientConfig(data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nserver (str)\\\\ntoken (str | None)\\\\ntimeoutsec (int | None)\\\\nretrystatuscodes (List[int] | None)\\\\nretrymaxtimes (int | None)\\\\nextraheaders (Dict[str, str] | None)\\\\ncacertificatepath (str | None)\\\\nmaxthreads (int)\\\\ndisablesslverification (bool)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nserver Optional[str]\\\\n\\\\n\\\\n\\\\ntimeoutsec Optional[List[int]]\\\\n\\\\n\\\\n\\\\nretrymaxtimes Optional[Dict[str, str]]\\\\n\\\\n\\\\n\\\\ncacertificatepath int\\\\n\\\\n\\\\n\\\\ndisablesslverification DataHubRestEmitter\\\\n\\\\nParameters\\\\n\\\\nentityurn (str) \\\\u2013 The urn of the entity\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)]) \\\\u2013 The type class of the aspect being requested (e.g. datahub.metadata.schemaclasses.DatasetProperties)\\\\nversion (int) \\\\u2013 The version of the aspect to retrieve. The default of 0 means latest. Versions &gt; 0 go from oldest to newest, so 1 is the oldest.\\\\n\\\\n\\\\nReturn type\\\\nthe Aspect as a dictionary if present, None if no aspect was found (HTTP status 404)\\\\n\\\\nRaises\\\\n\\\\nentityurn (str)\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)])\\\\naspect (str)\\\\naspecttypename (Optional[str])\\\\nversion (int)\\\\n\\\\n\\\\nReturn type\\\\nDict[str, Any]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetownership(entityurn)\\\\n\\\\nParameters\\\\nOptional[OwnershipClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetschemametadata(entityurn)\\\\n\\\\nParameters\\\\nOptional[SchemaMetadataClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetdomainproperties(entityurn)\\\\n\\\\nParameters\\\\nOptional[DomainPropertiesClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetdatasetproperties(entityurn)\\\\n\\\\nParameters\\\\nOptional[DatasetPropertiesClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngettags(entityurn)\\\\n\\\\nParameters\\\\nOptional[GlobalTagsClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetglossaryterms(entityurn)\\\\n\\\\nParameters\\\\nOptional[GlossaryTermsClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetdomain(entityurn)\\\\n\\\\nParameters\\\\nOptional[DomainsClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetbrowsepath(entityurn)\\\\n\\\\nParameters\\\\nOptional[BrowsePathsClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetusageaspectsfromurn(entityurn, starttimestamp, endtimestamp)\\\\n\\\\nParameters\\\\nOptional[List[DatasetUsageStatisticsClass]]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nlistallentityurns(entitytype, start, count)\\\\n\\\\nParameters\\\\nOptional[List[str]]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetlatesttimeseriesvalue(entityurn, aspecttype, filtercriteriamap)\\\\n\\\\nParameters\\\\nOptional[TypeVar(Aspect, bound= Aspect)]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetentityraw(entityurn, aspects=None)\\\\n\\\\nParameters\\\\nDict\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetaspectsforentity(entityurn, aspects, aspecttypes)\\\\nGet multiple aspects for an entity. To get a single aspect for an entity, use the getaspectv2 method.\\\\nWarning\\\\n\\\\nentityurn (str) \\\\u2013 The urn of the entity\\\\naspecttypelist (List[Type[Aspect]]) \\\\u2013 List of aspect type classes being requested (e.g. [datahub.metadata.schemaclasses.DatasetProperties])\\\\naspectslist (List[str]) \\\\u2013 List of aspect names being requested (e.g. [schemaMetadata, datasetProperties])\\\\nentityurn\\\\naspects (List[str])\\\\naspecttypes (List[Type[TypeVar(Aspect, bound= Aspect)]])\\\\n\\\\n\\\\nReturn type\\\\nOptionally, a map of aspectname to aspectvalue as a dictionary if present, aspectvalue will be set to None if that aspect was not found. Returns None on HTTP status 404.\\\\n\\\\nRaises\\\\ndomainname (str)\\\\n\\\\nReturn type\\\\n\\\\nenv (Optional[str])\\\\nsearchquery (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nbatchsize (int)\\\\n\\\\n\\\\nReturn type\\\\n\\\\npipelinename (str)\\\\nplatform (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nstart (int)\\\\ncount (int)\\\\nentity (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\naspect (str)\\\\nurnlike (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nquery (str)\\\\nvariables (Optional[Dict])\\\\n\\\\n\\\\nReturn type str, Enum\\\\nAn enumeration.\\\\n\\\\n\\\\nINCOMING = \'INCOMING\'\\\\n\\\\n\\\\n\\\\nOUTGOING = \'OUTGOING\'\\\\n\\\\n\\\\n\\\\n\\\\nclass RelatedEntity(urn, relationshiptype)\\\\nBases\\\\n\\\\nurn (str)\\\\nrelationshiptype (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nurn str\\\\n\\\\n\\\\n\\\\n\\\\ngetrelatedentities(entityurn, relationshiptypes, direction)\\\\n\\\\nParameters\\\\nIterable[RelatedEntity]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nsoftdeleteurn(urn, runid=\'soft-delete-urns\')\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.ingestion.graph.client.getdefault_graph()\\\\n\\\\nReturn type:\\\\nDataHubGraph\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"overviewSidebar"},"python-sdk/models":{"id":"python-sdk/models","title":"Models","description":"\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AccessLevelClass\\\\nBases DictWrapper\\\\nArray field type.\\\\n\\\\nParameters None | List[str]\\\\nList of types this array holds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionInfoClass(type, customProperties=None, externalUrl=None, datasetAssertion=None)\\\\nBases\\\\n\\\\ntype (Union[str, AssertionTypeClass])\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndatasetAssertion (Optional[DatasetAssertionInfoClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties None | DatasetAssertionInfoClass\\\\nDataset Assertion information when type is DATASET\\\\n\\\\n\\\\n\\\\nproperty externalUrl str | AssertionTypeClass\\\\nType of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionKeyClass(assertionId)\\\\nBases\\\\nassertionId (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty assertionId DictWrapper\\\\nThe result of running an assertion\\\\n\\\\nParameters None | float\\\\nObserved aggregate value for evaluated batch\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | int\\\\nNumber of rows with missing value for evaluated batch\\\\n\\\\n\\\\n\\\\nproperty nativeResults None | int\\\\nNumber of rows for evaluated batch\\\\n\\\\n\\\\n\\\\nproperty type None | int\\\\nNumber of rows with unexpected value for evaluated batch\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionResultTypeClass\\\\nBases Aspect\\\\nAn event representing the current status of evaluating an assertion on a batch.\\\\nAssertionRunEvent should be used for reporting the status of a run as an assertion evaluation progresses.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty asserteeUrn str\\\\n\\\\n\\\\n\\\\nproperty batchSpec None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty result str\\\\nNative (platform-specific) identifier for this run\\\\n\\\\n\\\\n\\\\nproperty runtimeContext str | AssertionRunStatusClass\\\\nThe status of the assertion run as per this timeseries event.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis object\\\\nThe Assertion Run has completed\\\\n\\\\n\\\\nCOMPLETE = \'COMPLETE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionStdAggregationClass\\\\nBases object\\\\nA boolean operator that is applied on the input to an assertion, after an aggregation function has been applied.\\\\n\\\\n\\\\nBETWEEN = \'BETWEEN\'\\\\nValue being asserted is less than a max value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nCONTAIN = \'CONTAIN\'\\\\nValue being asserted ends with value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nENDWITH = \'ENDWITH\'\\\\nValue being asserted starts with value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nEQUALTO = \'EQUALTO\'\\\\nValue being asserted is not null. Requires no parameters.\\\\n\\\\n\\\\n\\\\nGREATERTHAN = \'GREATERTHAN\'\\\\nValue being asserted is greater than or equal to some value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nGREATERTHANOREQUALTO = \'GREATERTHANOREQUALTO\'\\\\nValue being asserted is equal to value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nIN = \'IN\'\\\\nValue being asserted is not in one of the array values. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nLESSTHAN = \'LESSTHAN\'\\\\nValue being asserted is less than or equal to some value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nLESSTHANOREQUALTO = \'LESSTHANOREQUALTO\'\\\\nValue being asserted is greater than some value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nNOTIN = \'NOTIN\'\\\\nOther\\\\n\\\\n\\\\n\\\\nNOTNULL = \'NOTNULL\'\\\\nValue being asserted contains value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nREGEXMATCH = \'REGEXMATCH\'\\\\nValue being asserted is one of the array values. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nSTARTWITH = \'STARTWITH\'\\\\nValue being asserted matches the regex value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionStdParameterClass(value, type)\\\\nBases\\\\n\\\\nvalue (str)\\\\ntype (Union[str, AssertionStdParameterTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty type str\\\\nThe parameter value\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionStdParameterTypeClass\\\\nBases DictWrapper\\\\nParameters for AssertionStdOperators.\\\\n\\\\nParameters None | AssertionStdParameterClass\\\\nThe maxValue parameter of an assertion\\\\n\\\\n\\\\n\\\\nproperty minValue None | AssertionStdParameterClass\\\\nThe value parameter of an assertion\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionTypeClass\\\\nBases DictWrapper\\\\nData captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into a particular lifecycle stage, and who acted to move it into that specific lifecycle stage.\\\\n\\\\nParameters str\\\\nThe entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.\\\\n\\\\n\\\\n\\\\nproperty impersonator None | str\\\\nwas the change created by an automated process, or manually.\\\\n\\\\nType int\\\\nWhen did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AzkabanJobTypeClass\\\\nBases//azkaban.readthedocs.io/en/latest/jobTypes.html#java-job-type\\\\n\\\\n\\\\n\\\\nGLUE = \'GLUE\'\\\\n\\\\n\\\\n\\\\nHADOOPJAVA = \'HADOOPJAVA\'\\\\nIn large part, this is the same Command type. The difference is its ability to talk to a Hadoop cluster\\\\nsecurely, via Hadoop tokens.\\\\n\\\\n\\\\n\\\\nHADOOPSHELL = \'HADOOPSHELL\'\\\\nHive type is for running Hive jobs.\\\\n\\\\n\\\\n\\\\nHIVE = \'HIVE\'\\\\nPig type is for running Pig jobs.\\\\n\\\\n\\\\n\\\\nPIG = \'PIG\'\\\\nSQL is for running Presto, mysql queries etc\\\\n\\\\n\\\\n\\\\nSQL = \'SQL\'\\\\nGlue type is for running AWS Glue job transforms.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.BaseDataClass(dataset, motivation=None, preProcessing=None)\\\\nBases\\\\n\\\\ndataset (str)\\\\nmotivation (Optional[str])\\\\npreProcessing (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty dataset None | str\\\\nWhy was this dataset chosen?\\\\n\\\\n\\\\n\\\\nproperty preProcessing DictWrapper\\\\nA batch on which certain operations, e.g. data quality evaluation, is done.\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty limit None | str\\\\nThe native identifier as specified by the system operating on the batch.\\\\n\\\\n\\\\n\\\\nproperty query DictWrapper\\\\nSchema text of binary JSON schema.\\\\n\\\\nParameters str\\\\nThe native schema text for binary JSON file format.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.BooleanTypeClass\\\\nBases Aspect\\\\nShared aspect containing Browse Paths to be indexed for an entity.\\\\n\\\\nParameters List[str]\\\\nA list of valid browse paths for the entity.\\\\nBrowse paths are expected to be forward slash-separated strings. For example DictWrapper\\\\nBytes field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CalendarIntervalClass\\\\nBases DictWrapper\\\\nThis section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?\\\\n\\\\nParameters None | str\\\\nCaveat Description\\\\nFor ex None | List[str]\\\\nRelevant groups that were not represented in the evaluation dataset?\\\\n\\\\n\\\\n\\\\nproperty needsFurtherTesting Aspect\\\\nThis section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?\\\\n\\\\nParameters None | CaveatDetailsClass\\\\nThis section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\\\\n\\\\n\\\\n\\\\nproperty idealDatasetCharacteristics None | str\\\\nRecommendations on where this MLModel should be used.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChangeAuditStampsClass(created=None, lastModified=None, deleted=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty lastModified object\\\\nDescriptor for a change action\\\\n\\\\n\\\\nCREATE = \'CREATE\'\\\\nNOT SUPPORTED YET\\\\nupdate if exists. otherwise fail\\\\n\\\\n\\\\n\\\\nDELETE = \'DELETE\'\\\\nNOT SUPPORTED YET\\\\npatch the changes instead of full replace\\\\n\\\\n\\\\n\\\\nPATCH = \'PATCH\'\\\\nRestate an aspect, eg. in a index refresh.\\\\n\\\\n\\\\n\\\\nRESTATE = \'RESTATE\'\\\\n\\\\n\\\\n\\\\nUPDATE = \'UPDATE\'\\\\nNOT SUPPORTED YET\\\\ndelete action\\\\n\\\\n\\\\n\\\\nUPSERT = \'UPSERT\'\\\\nNOT SUPPORTED YET\\\\ninsert if not exists. otherwise fail\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartCellClass(cellId, changeAuditStamps, cellTitle=None)\\\\nBases\\\\n\\\\ncellId (str)\\\\nchangeAuditStamps (ChangeAuditStampsClass)\\\\ncellTitle (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cellId None | str\\\\nTitle of the cell\\\\n\\\\n\\\\n\\\\nproperty changeAuditStamps Aspect\\\\nInformation about a chart\\\\n\\\\nParameters None | str | AccessLevelClass\\\\nAccess level for the chart\\\\n\\\\n\\\\n\\\\nproperty chartUrl Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty inputEdges None | List[str]\\\\nData sources for the chart\\\\nDeprecated! Use inputEdges instead.\\\\n\\\\n\\\\n\\\\nproperty lastModified None | int\\\\nThe time when this chart last refreshed\\\\n\\\\n\\\\n\\\\nproperty title None | str | ChartTypeClass\\\\nType of the chart\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartKeyClass(dashboardTool, chartId)\\\\nBases\\\\n\\\\ndashboardTool (str)\\\\nchartId (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty chartId str\\\\nThe name of the dashboard tool such as looker, redash etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartQueryClass(rawQuery, type)\\\\nBases\\\\n\\\\nrawQuery (str)\\\\ntype (Union[str, ChartQueryTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty rawQuery str | ChartQueryTypeClass\\\\nChart query type\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartQueryTypeClass\\\\nBases DictWrapper\\\\nA metadata snapshot for a specific Chart entity.\\\\n\\\\nParameters List[ChartKeyClass | ChartInfoClass | ChartQueryClass | EditableChartPropertiesClass | OwnershipClass | StatusClass | GlobalTagsClass | BrowsePathsClass | GlossaryTermsClass | InstitutionalMemoryClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the chart. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn object\\\\nThe various types of charts\\\\n\\\\n\\\\nAREA = \'AREA\'\\\\n\\\\n\\\\n\\\\nBAR = \'BAR\'\\\\nChart showing a Pie chart\\\\n\\\\n\\\\n\\\\nBOXPLOT = \'BOXPLOT\'\\\\n\\\\n\\\\n\\\\nCOHORT = \'COHORT\'\\\\n\\\\n\\\\n\\\\nHISTOGRAM = \'HISTOGRAM\'\\\\n\\\\n\\\\n\\\\nLINE = \'LINE\'\\\\n\\\\n\\\\n\\\\nPIE = \'PIE\'\\\\nChart showing a Scatter plot\\\\n\\\\n\\\\n\\\\nSCATTER = \'SCATTER\'\\\\nChart showing a table\\\\n\\\\n\\\\n\\\\nTABLE = \'TABLE\'\\\\nChart showing Markdown formatted text\\\\n\\\\n\\\\n\\\\nTEXT = \'TEXT\'\\\\n\\\\n\\\\n\\\\nWORDCLOUD = \'WORDCLOUD\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartUsageStatisticsClass(timestampMillis, eventGranularity=None, partitionSpec=None, messageId=None, viewsCount=None, uniqueUserCount=None, userCounts=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nviewsCount (Optional[int])\\\\nuniqueUserCount (Optional[int])\\\\nuserCounts (Optional[List[ChartUserUsageCountsClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis None | int\\\\nUnique user count\\\\n\\\\n\\\\n\\\\nproperty userCounts None | int\\\\nThe total number of times chart has been viewed\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartUserUsageCountsClass(user, viewsCount=None)\\\\nBases\\\\n\\\\nuser (str)\\\\nviewsCount (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty user None | int\\\\nThe number of times the user has viewed the chart\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ConditionClass\\\\nBases\\\\nRepresent the relation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nENDWITH = \'ENDWITH\'\\\\nfield = value, e.g. platform = hdfs\\\\n\\\\nType\\\\nRepresent the relation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nEXISTS = \'EXISTS\'\\\\nRepresent the relation greater than, e.g. ownerCount &gt; 5\\\\n\\\\n\\\\n\\\\nGREATERTHAN = \'GREATERTHAN\'\\\\nRepresent the relation greater than or equal to, e.g. ownerCount &gt;= 5\\\\n\\\\n\\\\n\\\\nGREATERTHANOREQUALTO = \'GREATERTHANOREQUALTO\'\\\\nString field is one of the array values to, e.g. name in [\\\\u201cProfile\\\\u201d, \\\\u201cEvent\\\\u201d]\\\\n\\\\nType\\\\nRepresents the relation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLESSTHAN = \'LESSTHAN\'\\\\nRepresent the relation less than or equal to, e.g. ownerCount &lt;= 3\\\\n\\\\n\\\\n\\\\nLESSTHANOREQUALTO = \'LESSTHANOREQUALTO\'\\\\nString field starts with value, e.g. name starts with PageView\\\\n\\\\nType DictWrapper\\\\nA list of criterion and\\\\u2019d together.\\\\n\\\\nParameters List[CriterionClass]\\\\nA list of and criteria the filter applies to the query\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ContainerClass(container)\\\\nBases\\\\ncontainer (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty container Aspect\\\\nKey for an Asset Container\\\\n\\\\nParameters None | str\\\\nUnique guid for container\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ContainerPropertiesClass(name, customProperties=None, externalUrl=None, qualifiedName=None, description=None, created=None, lastModified=None)\\\\nBases\\\\n\\\\nname (str)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\nqualifiedName (Optional[str])\\\\ndescription (Optional[str])\\\\ncreated (Optional[TimeStampClass])\\\\nlastModified (Optional[TimeStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nDisplay name of the Asset Container\\\\n\\\\n\\\\n\\\\nproperty qualifiedName Aspect\\\\nGroup information that can be edited from UI\\\\n\\\\nParameters None | str\\\\nA description of the group\\\\n\\\\n\\\\n\\\\nproperty email str\\\\nA URL which points to a picture which user wants to set as the photo for the group\\\\n\\\\n\\\\n\\\\nproperty slack Aspect\\\\nInformation about a Corp Group ingested from a third party source\\\\n\\\\nParameters List[str]\\\\nowners of this group\\\\nDeprecated! Replaced by Ownership aspect.\\\\n\\\\n\\\\n\\\\nproperty created None | str\\\\nA description of the group.\\\\n\\\\n\\\\n\\\\nproperty displayName None | str\\\\nemail of this group\\\\n\\\\n\\\\n\\\\nproperty groups List[str]\\\\nList of ldap urn in this group.\\\\nDeprecated! Replaced by GroupMembership aspect.\\\\n\\\\n\\\\n\\\\nproperty slack Aspect\\\\nKey for a CorpGroup\\\\n\\\\nParameters str\\\\nThe URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpGroupSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[CorpGroupKeyClass, CorpGroupInfoClass, GlobalTagsClass, StatusClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserAppearanceSettingsClass(showSimplifiedHomepage=None)\\\\nBases\\\\nshowSimplifiedHomepage (Optional[bool])\\\\n\\\\n\\\\n\\\\n\\\\nproperty showSimplifiedHomepage Aspect\\\\nCorp user credentials\\\\n\\\\nParameters str\\\\nHashed password generated by concatenating salt and password, then hashing\\\\n\\\\n\\\\n\\\\nproperty passwordResetToken None | int\\\\nWhen the password reset token expires.\\\\n\\\\n\\\\n\\\\nproperty salt Aspect\\\\nLinkedin corp user information that can be edited from UI\\\\n\\\\nParameters None | str\\\\nAbout me section of the user\\\\n\\\\n\\\\n\\\\nproperty displayName None | str\\\\nEmail address to contact the user\\\\n\\\\n\\\\n\\\\nproperty phone str\\\\nA URL which points to a picture which user wants to set as a profile photo\\\\n\\\\n\\\\n\\\\nproperty skills None | str\\\\nSlack handle for the user\\\\n\\\\n\\\\n\\\\nproperty teams None | str\\\\nDataHub-native Title, e.g. \\\\u2018Software Engineer\\\\u2019\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserInfoClass(active, customProperties=None, displayName=None, email=None, title=None, managerUrn=None, departmentId=None, departmentName=None, firstName=None, lastName=None, fullName=None, countryCode=None)\\\\nBases\\\\n\\\\nactive (bool)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\ndisplayName (Optional[str])\\\\nemail (Optional[str])\\\\ntitle (Optional[str])\\\\nmanagerUrn (Optional[str])\\\\ndepartmentId (Optional[int])\\\\ndepartmentName (Optional[str])\\\\nfirstName (Optional[str])\\\\nlastName (Optional[str])\\\\nfullName (Optional[str])\\\\ncountryCode (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty active\\\\nDeprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref\\\\n\\\\nType None | str\\\\ntwo uppercase letters country code. e.g.  US\\\\n\\\\n\\\\n\\\\nproperty customProperties None | int\\\\ndepartment id this user belong to\\\\n\\\\n\\\\n\\\\nproperty departmentName None | str\\\\ndisplayName of this user ,  e.g.  Hang Zhang(DataHQ)\\\\n\\\\n\\\\n\\\\nproperty email None | str\\\\nfirst name of this user\\\\n\\\\n\\\\n\\\\nproperty fullName None | str\\\\nlast name of this user\\\\n\\\\n\\\\n\\\\nproperty managerUrn None | str\\\\ntitle of this user\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserKeyClass(username)\\\\nBases\\\\nusername (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty username Aspect\\\\nSettings that a user can customize through the datahub ui\\\\n\\\\nParameters CorpUserAppearanceSettingsClass\\\\nSettings for a user around the appearance of their DataHub U\\\\n\\\\n\\\\n\\\\nproperty views DictWrapper\\\\nA metadata snapshot for a specific CorpUser entity.\\\\n\\\\nParameters List[CorpUserKeyClass | CorpUserInfoClass | CorpUserEditableInfoClass | CorpUserStatusClass | GroupMembershipClass | GlobalTagsClass | StatusClass]\\\\nThe list of metadata aspects associated with the CorpUser. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nThe status of the user, e.g. provisioned, active, suspended, etc.\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who last modified the status and when.\\\\n\\\\n\\\\n\\\\nproperty status DictWrapper\\\\nSettings related to the \\\\u2018Views\\\\u2019 feature.\\\\n\\\\nParameters None | str\\\\nThe default View which is selected for the user.\\\\nIf none is chosen, then this value will be left blank.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CostClass(costType, cost)\\\\nBases\\\\n\\\\ncostType (Union[str, CostTypeClass])\\\\ncost (CostCostClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cost str | CostTypeClass\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CostCostClass(fieldDiscriminator, costId=None, costCode=None)\\\\nBases\\\\n\\\\nfieldDiscriminator (Union[str, CostCostDiscriminatorClass])\\\\ncostId (Optional[float])\\\\ncostCode (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty costCode None | float\\\\n\\\\n\\\\n\\\\nproperty fieldDiscriminator object\\\\n\\\\n\\\\ncostCode = \'costCode\'\\\\n\\\\n\\\\n\\\\ncostId = \'costId\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CostTypeClass\\\\nBases DictWrapper\\\\nA criterion for matching a field with given value\\\\n\\\\nParameters str | ConditionClass\\\\nThe condition for the criterion, e.g. EQUAL, STARTWITH\\\\n\\\\n\\\\n\\\\nproperty field bool\\\\nWhether the condition should be negated\\\\n\\\\n\\\\n\\\\nproperty value List[str]\\\\nValues. one of which the intended field should match\\\\nNote, if values is set, the above \\\\u201cvalue\\\\u201d field will be ignored\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DashboardInfoClass(title, description, lastModified, customProperties=None, externalUrl=None, charts=None, chartEdges=None, datasets=None, datasetEdges=None, dashboardUrl=None, access=None, lastRefreshed=None)\\\\nBases\\\\n\\\\ntitle (str)\\\\ndescription (str)\\\\nlastModified (ChangeAuditStampsClass)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ncharts (Optional[List[str]])\\\\nchartEdges (Optional[List[EdgeClass]])\\\\ndatasets (Optional[List[str]])\\\\ndatasetEdges (Optional[List[EdgeClass]])\\\\ndashboardUrl (Optional[str])\\\\naccess (Union[None, str, AccessLevelClass])\\\\nlastRefreshed (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty access None | List[EdgeClass]\\\\nCharts in a dashboard\\\\n\\\\n\\\\n\\\\nproperty charts Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty dashboardUrl None | List[EdgeClass]\\\\nDatasets consumed by a dashboard\\\\n\\\\n\\\\n\\\\nproperty datasets str\\\\nDetailed description about the dashboard\\\\n\\\\n\\\\n\\\\nproperty externalUrl ChangeAuditStampsClass\\\\nCaptures information about who created/last modified/deleted this dashboard and when\\\\n\\\\n\\\\n\\\\nproperty lastRefreshed str\\\\nTitle of the dashboard\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DashboardKeyClass(dashboardTool, dashboardId)\\\\nBases\\\\n\\\\ndashboardTool (str)\\\\ndashboardId (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty dashboardId str\\\\nThe name of the dashboard tool such as looker, redash etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DashboardSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DashboardKeyClass, DashboardInfoClass, EditableDashboardPropertiesClass, OwnershipClass, StatusClass, GlobalTagsClass, BrowsePathsClass, GlossaryTermsClass, InstitutionalMemoryClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DashboardUsageStatisticsClass(timestampMillis, eventGranularity=None, partitionSpec=None, messageId=None, viewsCount=None, executionsCount=None, uniqueUserCount=None, userCounts=None, favoritesCount=None, lastViewedAt=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nviewsCount (Optional[int])\\\\nexecutionsCount (Optional[int])\\\\nuniqueUserCount (Optional[int])\\\\nuserCounts (Optional[List[DashboardUserUsageCountsClass]])\\\\nfavoritesCount (Optional[int])\\\\nlastViewedAt (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty executionsCount None | int\\\\nThe total number of times that the dashboard has been favorited\\\\n\\\\n\\\\n\\\\nproperty lastViewedAt None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\nproperty uniqueUserCount None | List[DashboardUserUsageCountsClass]\\\\nUsers within this bucket, with frequency counts\\\\n\\\\n\\\\n\\\\nproperty viewsCount DictWrapper\\\\nRecords a single user\\\\u2019s usage counts for a given resource\\\\n\\\\nParameters None | int\\\\nThe number of times the user has executed (refreshed) the dashboard\\\\n\\\\n\\\\n\\\\nproperty usageCount str\\\\nThe unique id of the user.\\\\n\\\\n\\\\n\\\\nproperty userEmail None | int\\\\nThe number of times the user has viewed the dashboard\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataFlowInfoClass(name, customProperties=None, externalUrl=None, description=None, project=None, created=None, lastModified=None)\\\\nBases\\\\n\\\\nname (str)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndescription (Optional[str])\\\\nproject (Optional[str])\\\\ncreated (Optional[TimeStampClass])\\\\nlastModified (Optional[TimeStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nFlow name\\\\n\\\\n\\\\n\\\\nproperty project Aspect\\\\nKey for a Data Flow\\\\n\\\\nParameters str\\\\nCluster where the flow is executed\\\\n\\\\n\\\\n\\\\nproperty flowId str\\\\nWorkflow manager like azkaban, airflow which orchestrates the flow\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataFlowSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DataFlowKeyClass, DataFlowInfoClass, EditableDataFlowPropertiesClass, OwnershipClass, StatusClass, GlobalTagsClass, BrowsePathsClass, GlossaryTermsClass, InstitutionalMemoryClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubAccessTokenInfoClass(name, actorUrn, ownerUrn, createdAt, expiresAt=None, description=None)\\\\nBases\\\\n\\\\nname (str)\\\\nactorUrn (str)\\\\nownerUrn (str)\\\\ncreatedAt (int)\\\\nexpiresAt (Optional[int])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty actorUrn int\\\\nWhen the token was created.\\\\n\\\\n\\\\n\\\\nproperty description None | int\\\\nWhen the token expires.\\\\n\\\\n\\\\n\\\\nproperty name str\\\\nUrn of the actor which created this access token.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubAccessTokenKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id DictWrapper\\\\nInformation used to filter DataHub actors.\\\\n\\\\nParameters bool\\\\nWhether the filter should apply to all groups.\\\\n\\\\n\\\\n\\\\nproperty allUsers None | List[str]\\\\nA specific set of groups to apply the policy to (disjunctive)\\\\n\\\\n\\\\n\\\\nproperty resourceOwners None | List[str]\\\\nA specific set of roles to apply the policy to (disjunctive).\\\\n\\\\n\\\\n\\\\nproperty users DictWrapper\\\\n\\\\nParameters None | bool\\\\nWhether or not to run this ingestion source in debug mode\\\\n\\\\n\\\\n\\\\nproperty executorId str\\\\nThe JSON recipe to use for ingestion\\\\n\\\\n\\\\n\\\\nproperty version Aspect\\\\nInfo about a DataHub ingestion source\\\\n\\\\nParameters DataHubIngestionSourceConfigClass\\\\nParameters associated with the Ingestion Source\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nData Platform URN associated with the source\\\\n\\\\n\\\\n\\\\nproperty schedule str\\\\nThe type of the source itself, e.g. mysql, bigquery, bigquery-usage. Should match the recipe.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubIngestionSourceKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id DictWrapper\\\\nThe schedule associated with an ingestion source.\\\\n\\\\nParameters str\\\\nA cron-formatted execution interval, as a cron string, e.g. * * * * *\\\\n\\\\n\\\\n\\\\nproperty timezone Aspect\\\\nInformation about a DataHub (UI) access policy.\\\\n\\\\nParameters DataHubActorFilterClass\\\\nThe actors that the policy applies to.\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nDisplay name of the Policy\\\\n\\\\n\\\\n\\\\nproperty editable None | int\\\\nTimestamp when the policy was last updated\\\\n\\\\n\\\\n\\\\nproperty privileges None | DataHubResourceFilterClass\\\\nThe resource that the policy applies to. Not required for some \\\\u2018Platform\\\\u2019 privileges.\\\\n\\\\n\\\\n\\\\nproperty state str\\\\nThe type of policy\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubPolicyKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id DictWrapper\\\\nA metadata snapshot for DataHub Access Policy data.\\\\n\\\\nParameters List[DataHubPolicyKeyClass | DataHubPolicyInfoClass]\\\\nThe list of metadata aspects associated with the DataHub access policy.\\\\n\\\\n\\\\n\\\\nproperty urn DictWrapper\\\\nInformation used to filter DataHub resource.\\\\n\\\\nParameters bool\\\\nWhether the policy should be applied to all assets matching the filter.\\\\n\\\\n\\\\n\\\\nproperty filter None | List[str]\\\\nA specific set of resources to apply the policy to, e.g. asset urns\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\n\\\\nParameters RetentionClass\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubRetentionKeyClass(entityName, aspectName)\\\\nBases\\\\n\\\\nentityName (str)\\\\naspectName (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspectName str\\\\nEntity name to apply retention to. * (or empty) for applying defaults.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubRetentionSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DataHubRetentionKeyClass, DataHubRetentionConfigClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubRoleInfoClass(name, description, editable=None)\\\\nBases\\\\n\\\\nname (str)\\\\ndescription (str)\\\\neditable (Optional[bool])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty description bool\\\\nWhether the role should be editable via the UI\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nKey for a DataHub Role\\\\n\\\\nParameters str\\\\nA unique id for the DataHub role record. Generated on the server side at role creation time.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubSecretKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nThe value of a DataHub Secret\\\\n\\\\nParameters None | AuditStampClass\\\\nCreated Audit stamp\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nThe display name for the secret\\\\n\\\\n\\\\n\\\\nproperty value Aspect\\\\nKey for a DataHub Step State\\\\n\\\\nParameters str\\\\nA unique id for the state\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubStepStatePropertiesClass(lastModified, properties=None)\\\\nBases\\\\n\\\\nlastModified (AuditStampClass)\\\\nproperties (Optional[Dict[str, str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty lastModified Dict[str, str]\\\\nDescription of the secret\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubUpgradeKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nInformation collected when kicking off a DataHubUpgrade\\\\n\\\\nParameters int\\\\nTimestamp when we started this DataHubUpgrade\\\\n\\\\n\\\\n\\\\nproperty version Aspect\\\\nInformation collected when a DataHubUpgrade successfully finishes\\\\n\\\\nParameters None | Dict[str, str]\\\\nResult map to place helpful information about this upgrade job\\\\n\\\\n\\\\n\\\\nproperty timestampMs DictWrapper\\\\nA View definition.\\\\n\\\\nParameters List[str]\\\\nThe Entity Types in the scope of the View.\\\\n\\\\n\\\\n\\\\nproperty filter Aspect\\\\nInformation about a DataHub View. \\\\u2013 TODO\\\\n\\\\nname (str)\\\\ntype (Union[str, DataHubViewTypeClass])\\\\ndefinition (DataHubViewDefinitionClass)\\\\ncreated (AuditStampClass)\\\\nlastModified (AuditStampClass)\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created DataHubViewDefinitionClass\\\\nThe view itself\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAudit stamp capturing the time and actor who last modified the View.\\\\n\\\\n\\\\n\\\\nproperty name str | DataHubViewTypeClass\\\\nThe type of View\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubViewKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id object\\\\nA view private for a specific person.\\\\n\\\\n\\\\nGLOBAL = \'GLOBAL\'\\\\n\\\\n\\\\n\\\\nPERSONAL = \'PERSONAL\'\\\\nA global view, which all users can see and use.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataJobInfoClass(name, type, customProperties=None, externalUrl=None, description=None, flowUrn=None, created=None, lastModified=None, status=None)\\\\nBases\\\\n\\\\nname (str)\\\\ntype (Union[str, AzkabanJobTypeClass])\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndescription (Optional[str])\\\\nflowUrn (Optional[str])\\\\ncreated (Optional[TimeStampClass])\\\\nlastModified (Optional[TimeStampClass])\\\\nstatus (Union[None, str, JobStatusClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty flowUrn None | TimeStampClass\\\\nA timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty name None | str | JobStatusClass\\\\nStatus of the job - Deprecated for Data Process Instance model.\\\\n\\\\n\\\\n\\\\nproperty type AzkabanJobType is deprecated. Please use strings instead.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataJobInputOutputClass(inputDatasets, outputDatasets, inputDatasetEdges=None, outputDatasetEdges=None, inputDatajobs=None, inputDatajobEdges=None, inputDatasetFields=None, outputDatasetFields=None, fineGrainedLineages=None)\\\\nBases\\\\n\\\\ninputDatasets (List[str])\\\\noutputDatasets (List[str])\\\\ninputDatasetEdges (Optional[List[EdgeClass]])\\\\noutputDatasetEdges (Optional[List[EdgeClass]])\\\\ninputDatajobs (Optional[List[str]])\\\\ninputDatajobEdges (Optional[List[EdgeClass]])\\\\ninputDatasetFields (Optional[List[str]])\\\\noutputDatasetFields (Optional[List[str]])\\\\nfineGrainedLineages (Optional[List[FineGrainedLineageClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty fineGrainedLineages None | List[EdgeClass]\\\\nInput datajobs that this data job depends on\\\\n\\\\n\\\\n\\\\nproperty inputDatajobs None | List[EdgeClass]\\\\nInput datasets consumed by the data job during processing\\\\n\\\\n\\\\n\\\\nproperty inputDatasetFields List[str]\\\\nInput datasets consumed by the data job during processing\\\\nDeprecated! Use inputDatasetEdges instead.\\\\n\\\\n\\\\n\\\\nproperty outputDatasetEdges None | List[str]\\\\nFields of the output datasets this job writes to\\\\n\\\\n\\\\n\\\\nproperty outputDatasets Aspect\\\\nKey for a Data Job\\\\n\\\\nParameters str\\\\nStandardized data processing flow urn representing the flow for the job\\\\n\\\\n\\\\n\\\\nproperty jobId DictWrapper\\\\nA metadata snapshot for a specific DataJob entity.\\\\n\\\\nParameters List[DataJobKeyClass | DataJobInfoClass | DataJobInputOutputClass | EditableDataJobPropertiesClass | OwnershipClass | StatusClass | GlobalTagsClass | BrowsePathsClass | GlossaryTermsClass | InstitutionalMemoryClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the data job. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nInformation about a data platform\\\\n\\\\nParameters str\\\\nThe delimiter in the dataset names on the data platform, e.g. \\\\u2018/\\\\u2019 for HDFS and \\\\u2018.\\\\u2019 for Oracle\\\\n\\\\n\\\\n\\\\nproperty displayName None | str\\\\nThe URL for a logo associated with the platform\\\\n\\\\n\\\\n\\\\nproperty name str | PlatformTypeClass\\\\nPlatform type this data platform describes\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataPlatformInstanceClass(platform, instance=None)\\\\nBases\\\\n\\\\nplatform (str)\\\\ninstance (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty instance str\\\\nData Platform\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataPlatformInstanceKeyClass(platform, instance)\\\\nBases\\\\n\\\\nplatform (str)\\\\ninstance (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty instance str\\\\nData platform urn associated with the instance\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataPlatformInstancePropertiesClass(customProperties=None, externalUrl=None, name=None, description=None)\\\\nBases\\\\n\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\nname (Optional[str])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDocumentation of the Data Platform Instance\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | str\\\\nDisplay name of the Data Platform Instance\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataPlatformKeyClass(platformName)\\\\nBases\\\\nplatformName (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty platformName DictWrapper\\\\nA metadata snapshot for a specific dataplatform entity.\\\\n\\\\nParameters List[DataPlatformKeyClass | DataPlatformInfoClass]\\\\nThe list of metadata aspects associated with the data platform. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nThe inputs and outputs of this data process\\\\n\\\\nParameters None | List[str]\\\\nthe inputs of the data process\\\\n\\\\n\\\\n\\\\nproperty outputs Aspect\\\\nInformation about the inputs datasets of a Data process\\\\n\\\\nParameters List[str]\\\\nInput datasets to be consumed\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstanceKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nInformation about the outputs of a Data process\\\\n\\\\nParameters List[str]\\\\nOutput datasets to be produced\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstancePropertiesClass(name, created, customProperties=None, externalUrl=None, type=None)\\\\nBases\\\\n\\\\nname (str)\\\\ncreated (AuditStampClass)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ntype (Union[None, str, DataProcessTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty externalUrl str\\\\nProcess name\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nInformation about Data process relationships\\\\n\\\\nParameters None | str\\\\nThe parent DataProcessInstance where it belongs to.\\\\nIf it is a Airflow Task then it should belong to an Airflow Dag run as well\\\\nwhich will be another DataProcessInstance\\\\n\\\\n\\\\n\\\\nproperty parentTemplate List[str]\\\\nInput DataProcessInstance which triggered this dataprocess instance\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstanceRunEventClass(timestampMillis, status, eventGranularity=None, partitionSpec=None, messageId=None, externalUrl=None, attempt=None, result=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\nstatus (Union[str, DataProcessRunStatusClass])\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nexternalUrl (Optional[str])\\\\nattempt (Optional[int])\\\\nresult (Optional[DataProcessInstanceRunResultClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | int\\\\nReturn the try number that this Instance Run is in\\\\n\\\\n\\\\n\\\\nproperty eventGranularity None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty result str | DataProcessRunStatusClass\\\\n\\\\n\\\\n\\\\nproperty timestampMillis DictWrapper\\\\n\\\\nParameters str\\\\nIt identifies the system where the native result comes from like Airflow, Azkaban, etc..\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nKey for a Data Process\\\\n\\\\nParameters str\\\\nProcess name i.e. an ETL job name\\\\n\\\\n\\\\n\\\\nproperty orchestrator Migrate towards something that can be validated like DataPlatform urn\\\\n\\\\n\\\\n\\\\nproperty origin object\\\\nThe status where the Data processing run is in.\\\\n\\\\n\\\\nCOMPLETE = \'COMPLETE\'\\\\n\\\\n\\\\n\\\\nSTARTED = \'STARTED\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DataProcessKeyClass, OwnershipClass, DataProcessInfoClass, StatusClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessTypeClass\\\\nBases Aspect\\\\nCheckpoint of a datahub ingestion run for a given job.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty config None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty pipelineName str\\\\nThe id of the instance against which the ingestion pipeline ran.\\\\ne.g. str\\\\nThe run identifier of this job.\\\\n\\\\n\\\\n\\\\nproperty state int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatahubIngestionRunSummaryClass(timestampMillis, pipelineName, platformInstanceId, runId, runStatus, eventGranularity=None, partitionSpec=None, messageId=None, numWorkUnitsCommitted=None, numWorkUnitsCreated=None, numEvents=None, numEntities=None, numAspects=None, numSourceAPICalls=None, totalLatencySourceAPICalls=None, numSinkAPICalls=None, totalLatencySinkAPICalls=None, numWarnings=None, numErrors=None, numEntitiesSkipped=None, config=None, customsummary=None, softwareVersion=None, systemHostName=None, operatingSystemName=None, numProcessors=None, totalMemory=None, availableMemory=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\npipelineName (str)\\\\nplatformInstanceId (str)\\\\nrunId (str)\\\\nrunStatus (Union[str, JobStatusClass])\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nnumWorkUnitsCommitted (Optional[int])\\\\nnumWorkUnitsCreated (Optional[int])\\\\nnumEvents (Optional[int])\\\\nnumEntities (Optional[int])\\\\nnumAspects (Optional[int])\\\\nnumSourceAPICalls (Optional[int])\\\\ntotalLatencySourceAPICalls (Optional[int])\\\\nnumSinkAPICalls (Optional[int])\\\\ntotalLatencySinkAPICalls (Optional[int])\\\\nnumWarnings (Optional[int])\\\\nnumErrors (Optional[int])\\\\nnumEntitiesSkipped (Optional[int])\\\\nconfig (Optional[str])\\\\ncustomsummary (Optional[str])\\\\nsoftwareVersion (Optional[str])\\\\nsystemHostName (Optional[str])\\\\noperatingSystemName (Optional[str])\\\\nnumProcessors (Optional[int])\\\\ntotalMemory (Optional[int])\\\\navailableMemory (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | int\\\\nThe available memory on the host the ingestion pipeline ran on.\\\\n\\\\n\\\\n\\\\nproperty config None | str\\\\nCustom value.\\\\n\\\\n\\\\n\\\\nproperty eventGranularity None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty numAspects None | int\\\\nThe total number of entities produced (unique entity urns).\\\\n\\\\n\\\\n\\\\nproperty numEntitiesSkipped None | int\\\\nNumber of errors generated.\\\\n\\\\n\\\\n\\\\nproperty numEvents None | int\\\\nThe number of processors on the host the ingestion pipeline ran on.\\\\n\\\\n\\\\n\\\\nproperty numSinkAPICalls None | int\\\\nTotal number of source API calls.\\\\n\\\\n\\\\n\\\\nproperty numWarnings None | int\\\\nThe number of workunits written to sink.\\\\n\\\\n\\\\n\\\\nproperty numWorkUnitsCreated None | str\\\\nThe os the ingestion pipeline ran on.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec str\\\\nThe name of the pipeline that ran ingestion, a stable unique user provided identifier.\\\\ne.g. mysnowflake1-to-datahub.\\\\n\\\\n\\\\n\\\\nproperty platformInstanceId Bigquery project ids, MySQL hostnames etc.\\\\n\\\\n\\\\n\\\\nproperty runId str | JobStatusClass\\\\nRun Status - Succeeded/Skipped/Failed etc.\\\\n\\\\n\\\\n\\\\nproperty softwareVersion None | str\\\\nThe hostname the ingestion pipeline ran on.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis None | int\\\\nTotal latency across all sink API calls.\\\\n\\\\n\\\\n\\\\nproperty totalLatencySourceAPICalls None | int\\\\nThe total amount of memory on the host the ingestion pipeline ran on.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetAssertionInfoClass(dataset, scope, operator, fields=None, aggregation=None, parameters=None, nativeType=None, nativeParameters=None, logic=None)\\\\nBases\\\\n\\\\ndataset (str)\\\\nscope (Union[str, DatasetAssertionScopeClass])\\\\noperator (Union[str, AssertionStdOperatorClass])\\\\nfields (Optional[List[str]])\\\\naggregation (Union[None, str, AssertionStdAggregationClass])\\\\nparameters (Optional[AssertionStdParametersClass])\\\\nnativeType (Optional[str])\\\\nnativeParameters (Optional[Dict[str, str]])\\\\nlogic (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aggregation str\\\\nThe dataset targeted by this assertion.\\\\n\\\\n\\\\n\\\\nproperty fields None | str\\\\n\\\\n\\\\n\\\\nproperty nativeParameters None | str\\\\nNative assertion type\\\\n\\\\n\\\\n\\\\nproperty operator None | AssertionStdParametersClass\\\\nStandard parameters required for the assertion. e.g. minvalue, maxvalue, value, columns\\\\n\\\\n\\\\n\\\\nproperty scope object\\\\nThis assertion applies to dataset columns\\\\n\\\\n\\\\nDATASETCOLUMN = \'DATASETCOLUMN\'\\\\nThis assertion applies to entire rows of the dataset\\\\n\\\\n\\\\n\\\\nDATASETROWS = \'DATASETROWS\'\\\\nThis assertion applies to the schema of the dataset\\\\n\\\\n\\\\n\\\\nDATASETSCHEMA = \'DATASETSCHEMA\'\\\\nThe scope of the assertion is unknown\\\\n\\\\n\\\\n\\\\nUNKNOWN = \'UNKNOWN\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetDeprecationClass(deprecated, note, decommissionTime=None, actor=None)\\\\nBases\\\\n\\\\ndeprecated (bool)\\\\nnote (str)\\\\ndecommissionTime (Optional[int])\\\\nactor (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty actor None | int\\\\nThe time user plan to decommission this dataset.\\\\n\\\\n\\\\n\\\\nproperty deprecated str\\\\nAdditional information about the dataset deprecation plan, such as the wiki, doc, RB.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetFieldForeignKeyClass(parentDataset, currentFieldPaths, parentField)\\\\nBases\\\\n\\\\nparentDataset (str)\\\\ncurrentFieldPaths (List[str])\\\\nparentField (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty currentFieldPaths str\\\\ndataset that stores the resource.\\\\n\\\\n\\\\n\\\\nproperty parentField DictWrapper\\\\nRepresentation of mapping between fields in source dataset to the field in destination dataset\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who reported the field mapping and when\\\\n\\\\n\\\\n\\\\nproperty destinationField List[str]\\\\nSource fields from which the fine grained lineage is derived\\\\n\\\\n\\\\n\\\\nproperty transformation DictWrapper\\\\nStats corresponding to fields in a dataset\\\\n\\\\nParameters None | List[ValueFrequencyClass]\\\\n\\\\n\\\\n\\\\nproperty fieldPath None | HistogramClass\\\\n\\\\n\\\\n\\\\nproperty max None | str\\\\n\\\\n\\\\n\\\\nproperty median None | str\\\\n\\\\n\\\\n\\\\nproperty nullCount None | float\\\\n\\\\n\\\\n\\\\nproperty quantiles None | List[str]\\\\n\\\\n\\\\n\\\\nproperty stdev None | int\\\\n\\\\n\\\\n\\\\nproperty uniqueProportion DictWrapper\\\\nRecords field-level usage counts for a given dataset\\\\n\\\\nParameters int\\\\nNumber of times the field has been used.\\\\n\\\\n\\\\n\\\\nproperty fieldPath Aspect\\\\nKey for a Dataset\\\\n\\\\nParameters str\\\\nUnique guid for dataset\\\\n\\\\n\\\\n\\\\nproperty origin str\\\\nData platform urn associated with the dataset\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetLineageTypeClass\\\\nBases Aspect\\\\nStats corresponding to datasets\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty columnCount None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty fieldProfiles None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec None | int\\\\nThe total number of rows\\\\n\\\\n\\\\n\\\\nproperty sizeInBytes int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetPropertiesClass(customProperties=None, externalUrl=None, name=None, qualifiedName=None, description=None, uri=None, created=None, lastModified=None, tags=None)\\\\nBases\\\\n\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\nname (Optional[str])\\\\nqualifiedName (Optional[str])\\\\ndescription (Optional[str])\\\\nuri (Optional[str])\\\\ncreated (Optional[TimeStampClass])\\\\nlastModified (Optional[TimeStampClass])\\\\ntags (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty lastModified None | str\\\\nDisplay name of the Dataset\\\\n\\\\n\\\\n\\\\nproperty qualifiedName List[str]\\\\n[Legacy] Unstructured tags for the dataset. Structured tags can be applied via the GlobalTags aspect.\\\\nThis is now deprecated.\\\\n\\\\n\\\\n\\\\nproperty uri\\\\nThe abstracted URI such as hdfs\\\\n\\\\nType DictWrapper\\\\nA metadata snapshot for a specific dataset entity.\\\\n\\\\nParameters List[DatasetKeyClass | DatasetPropertiesClass | EditableDatasetPropertiesClass | DatasetDeprecationClass | DatasetUpstreamLineageClass | UpstreamLineageClass | InstitutionalMemoryClass | OwnershipClass | StatusClass | SchemaMetadataClass | EditableSchemaMetadataClass | GlobalTagsClass | GlossaryTermsClass | BrowsePathsClass | DataPlatformInstanceClass | ViewPropertiesClass]\\\\nThe list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nFine Grained upstream lineage for fields in a dataset\\\\n\\\\nParameters List[DatasetFieldMappingClass]\\\\nUpstream to downstream field level lineage mappings\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetUsageStatisticsClass(timestampMillis, eventGranularity=None, partitionSpec=None, messageId=None, uniqueUserCount=None, totalSqlQueries=None, topSqlQueries=None, userCounts=None, fieldCounts=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nuniqueUserCount (Optional[int])\\\\ntotalSqlQueries (Optional[int])\\\\ntopSqlQueries (Optional[List[str]])\\\\nuserCounts (Optional[List[DatasetUserUsageCountsClass]])\\\\nfieldCounts (Optional[List[DatasetFieldUsageCountsClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty fieldCounts None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\nproperty topSqlQueries None | int\\\\nTotal SQL query count\\\\n\\\\n\\\\n\\\\nproperty uniqueUserCount None | List[DatasetUserUsageCountsClass]\\\\nUsers within this bucket, with frequency counts\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetUserUsageCountsClass(user, count, userEmail=None)\\\\nBases\\\\n\\\\nuser (str)\\\\ncount (int)\\\\nuserEmail (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty count str\\\\nThe unique id of the user.\\\\n\\\\n\\\\n\\\\nproperty userEmail DictWrapper\\\\nDate field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DeploymentStatusClass\\\\nBases Aspect\\\\nDeprecation status of an entity\\\\n\\\\nParameters str\\\\nThe user URN which will be credited for modifying this deprecation content.\\\\n\\\\n\\\\n\\\\nproperty decommissionTime bool\\\\nWhether the entity is deprecated.\\\\n\\\\n\\\\n\\\\nproperty note Aspect\\\\nKey for an Asset Domain\\\\n\\\\nParameters str\\\\nA unique id for the domain. Should be separate from the name used for displaying a Domain.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DomainPropertiesClass(name, description=None, created=None)\\\\nBases\\\\n\\\\nname (str)\\\\ndescription (Optional[str])\\\\ncreated (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | str\\\\nDescription of the Domain\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nLinks from an Asset to its Domains\\\\n\\\\nParameters List[str]\\\\nThe Domains attached to an Asset\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EdgeClass(sourceUrn, destinationUrn, created, lastModified, properties=None)\\\\nBases\\\\n\\\\nsourceUrn (str)\\\\ndestinationUrn (str)\\\\ncreated (AuditStampClass)\\\\nlastModified (AuditStampClass)\\\\nproperties (Optional[Dict[str, str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created str\\\\nUrn of the destination of this relationship edge.\\\\n\\\\n\\\\n\\\\nproperty lastModified None | Dict[str, str]\\\\nA generic properties bag that allows us to store specific information on this graph edge.\\\\n\\\\n\\\\n\\\\nproperty sourceUrn Aspect\\\\nStores editable changes made to properties. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted None | str\\\\nEdited documentation of the chart\\\\n\\\\n\\\\n\\\\nproperty lastModified Aspect\\\\nEditable information about an Asset Container as defined on the DataHub Platform\\\\n\\\\nParameters None | str\\\\nDescription of the Asset Container as its received on the DataHub Platform\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableDashboardPropertiesClass(created=None, lastModified=None, deleted=None, description=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableDataFlowPropertiesClass(created=None, lastModified=None, deleted=None, description=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableDataJobPropertiesClass(created=None, lastModified=None, deleted=None, description=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableDatasetPropertiesClass(created=None, lastModified=None, deleted=None, description=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableMLFeaturePropertiesClass(description=None)\\\\nBases\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty description Aspect\\\\nProperties associated with a MLFeatureTable editable from the ui\\\\n\\\\nParameters None | str\\\\nDocumentation of the MLFeatureTable\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableMLModelGroupPropertiesClass(description=None)\\\\nBases\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty description Aspect\\\\nProperties associated with a ML Model editable from the UI\\\\n\\\\nParameters None | str\\\\nDocumentation of the ml model\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableMLPrimaryKeyPropertiesClass(description=None)\\\\nBases\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty description Aspect\\\\nStores editable changes made to properties. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\\\\nNote\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableSchemaFieldInfoClass(fieldPath, description=None, globalTags=None, glossaryTerms=None)\\\\nBases\\\\n\\\\nfieldPath (str)\\\\ndescription (Optional[str])\\\\nglobalTags (Optional[GlobalTagsClass])\\\\nglossaryTerms (Optional[GlossaryTermsClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nFieldPath uniquely identifying the SchemaField this metadata is associated with\\\\n\\\\n\\\\n\\\\nproperty globalTags None | GlossaryTermsClass\\\\nGlossary terms associated with the field\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableSchemaMetadataClass(editableSchemaFieldInfo, created=None, lastModified=None, deleted=None)\\\\nBases\\\\n\\\\neditableSchemaFieldInfo (List[EditableSchemaFieldInfoClass])\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty editableSchemaFieldInfo AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EmbedClass(renderUrl=None)\\\\nBases\\\\nrenderUrl (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty renderUrl DictWrapper\\\\nShared fields for all entity change events.\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp of the operation\\\\n\\\\n\\\\n\\\\nproperty category str\\\\nThe type of the entity affected. Corresponds to the entity registry, e.g. \\\\u2018dataset\\\\u2019, \\\\u2018chart\\\\u2019, \\\\u2018dashboard\\\\u2019, etc.\\\\n\\\\n\\\\n\\\\nproperty entityUrn None | str\\\\nThe urn of the entity which was affected.\\\\n\\\\n\\\\n\\\\nproperty operation None | ParametersClass\\\\nArbitrary key-value parameters corresponding to the event.\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\nEnum field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EspressoSchemaClass(documentSchema, tableSchema)\\\\nBases\\\\n\\\\ndocumentSchema (str)\\\\ntableSchema (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty documentSchema str\\\\nThe espresso table schema definition.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EthicalConsiderationsClass(data=None, humanLife=None, mitigations=None, risksAndHarms=None, useCases=None)\\\\nBases\\\\n\\\\ndata (Optional[List[str]])\\\\nhumanLife (Optional[List[str]])\\\\nmitigations (Optional[List[str]])\\\\nrisksAndHarms (Optional[List[str]])\\\\nuseCases (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty data None | List[str]\\\\nIs the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?\\\\n\\\\n\\\\n\\\\nproperty mitigations None | List[str]\\\\nWhat risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown.\\\\n\\\\n\\\\n\\\\nproperty useCases Aspect\\\\nAll referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset.\\\\n\\\\nParameters List[BaseDataClass]\\\\nDetails on the dataset(s) used for the quantitative analyses in the MLModel\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ExecutionRequestInputClass(task, args, executorId, source, requestedAt)\\\\nBases Determine who is responsible for emitting execution request success or failure. Executor?\\\\n\\\\nParameters Dict[str, str]\\\\nArguments provided to the task\\\\n\\\\n\\\\n\\\\nproperty executorId\\\\nAdvanced\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty requestedAt ExecutionRequestSourceClass\\\\nSource which created the execution request\\\\n\\\\n\\\\n\\\\nproperty task Aspect\\\\nKey for an DataHub Execution Request\\\\n\\\\nParameters str\\\\nA unique id for the DataHub execution request.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ExecutionRequestResultClass(status, report=None, structuredReport=None, startTimeMs=None, durationMs=None)\\\\nBases\\\\n\\\\nstatus (str)\\\\nreport (Optional[str])\\\\nstructuredReport (Optional[StructuredExecutionReportClass])\\\\nstartTimeMs (Optional[int])\\\\ndurationMs (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty durationMs None | str\\\\nThe pretty-printed execution report.\\\\n\\\\n\\\\n\\\\nproperty startTimeMs str\\\\nThe status of the execution request\\\\n\\\\n\\\\n\\\\nproperty structuredReport Aspect\\\\nAn signal sent to a running execution request\\\\n\\\\nParameters AuditStampClass\\\\nAudit Stamp\\\\n\\\\n\\\\n\\\\nproperty executorId\\\\nAdvanced\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty signal DictWrapper\\\\n\\\\nParameters None | str\\\\nThe urn of the ingestion source associated with the ingestion request. Present if type is INGESTIONSOURCE\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nFabric group type\\\\n\\\\n\\\\nCORP = \'CORP\'\\\\n\\\\n\\\\n\\\\nDEV = \'DEV\'\\\\nDesignates testing fabrics\\\\n\\\\n\\\\n\\\\nEI = \'EI\'\\\\nDesignates pre-production fabrics\\\\n\\\\n\\\\n\\\\nNONPROD = \'NONPROD\'\\\\nDesignates production fabrics\\\\n\\\\n\\\\n\\\\nPRE = \'PRE\'\\\\nDesignates staging fabrics\\\\n\\\\n\\\\n\\\\nPROD = \'PROD\'\\\\nDesignates corporation fabrics\\\\n\\\\n\\\\n\\\\nQA = \'QA\'\\\\nDesignates user acceptance testing fabrics\\\\n\\\\n\\\\n\\\\nSTG = \'STG\'\\\\nDesignates non-production fabrics\\\\n\\\\n\\\\n\\\\nTEST = \'TEST\'\\\\nDesignates quality assurance fabrics\\\\n\\\\n\\\\n\\\\nUAT = \'UAT\'\\\\nDesignates early-integration fabrics\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FieldUsageCountsClass(fieldName, count)\\\\nBases\\\\n\\\\nfieldName (str)\\\\ncount (int)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty count str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FilterClass(or=None, criteria=None)\\\\nBases\\\\n\\\\nor (Optional[List[ConjunctiveCriterionClass]])\\\\ncriteria (Optional[List[CriterionClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty criteria None | List[ConjunctiveCriterionClass]\\\\nA list of disjunctive criterion for the filter. (or operation to combine filters)\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FineGrainedLineageClass(upstreamType, downstreamType, upstreams=None, downstreams=None, transformOperation=None, confidenceScore=None)\\\\nBases\\\\n\\\\nupstreamType (Union[str, FineGrainedLineageUpstreamTypeClass])\\\\ndownstreamType (Union[str, FineGrainedLineageDownstreamTypeClass])\\\\nupstreams (Optional[List[str]])\\\\ndownstreams (Optional[List[str]])\\\\ntransformOperation (Optional[str])\\\\nconfidenceScore (Optional[float])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty confidenceScore str | FineGrainedLineageDownstreamTypeClass\\\\nThe type of downstream field(s)\\\\n\\\\n\\\\n\\\\nproperty downstreams None | str\\\\nThe transform operation applied to the upstream entities to produce the downstream field(s)\\\\n\\\\n\\\\n\\\\nproperty upstreamType None | List[str]\\\\nUpstream entities in the lineage\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FineGrainedLineageDownstreamTypeClass\\\\nBases object\\\\nThe type of upstream entity in a fine-grained lineage\\\\n\\\\n\\\\nDATASET = \'DATASET\'\\\\nIndicates that there is no upstream lineage i.e. the downstream field is not a derived field\\\\n\\\\n\\\\n\\\\nFIELDSET = \'FIELDSET\'\\\\nIndicates that this lineage is originating from upstream dataset(s)\\\\n\\\\n\\\\n\\\\nNONE = \'NONE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FixedTypeClass\\\\nBases DictWrapper\\\\nDescription of a foreign key constraint in a schema.\\\\n\\\\nParameters str\\\\nReference to the foreign dataset for ease of lookup\\\\n\\\\n\\\\n\\\\nproperty foreignFields str\\\\nName of the constraint, likely provided from the source\\\\n\\\\n\\\\n\\\\nproperty sourceFields DictWrapper\\\\nDescription of a foreign key in a schema.\\\\n\\\\nParameters DatasetFieldForeignKeyClass | UrnForeignKeyClass\\\\nForeign key definition in metadata schema.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GenericAspectClass(value, contentType)\\\\nBases\\\\n\\\\nvalue (bytes)\\\\ncontentType (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty contentType bytes\\\\nThe value of the aspect, serialized as bytes.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GenericPayloadClass(value, contentType)\\\\nBases\\\\n\\\\nvalue (bytes)\\\\ncontentType (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty contentType bytes\\\\nThe value of the event, serialized as bytes.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlobalSettingsInfoClass(views=None)\\\\nBases\\\\nviews (Optional[GlobalViewsSettingsClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty views Aspect\\\\nKey for a Global Settings\\\\n\\\\nParameters str\\\\nli0\\\\n\\\\nType\\\\nurn\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlobalTagsClass(tags)\\\\nBases\\\\ntags (List[TagAssociationClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty tags DictWrapper\\\\nSettings for DataHub Views feature.\\\\n\\\\nParameters None | str\\\\nThe default View for the instance, or organization.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryNodeInfoClass(definition, parentNode=None, name=None, id=None)\\\\nBases\\\\n\\\\ndefinition (str)\\\\nparentNode (Optional[str])\\\\nname (Optional[str])\\\\nid (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty definition None | str\\\\nOptional id for the GlossaryNode\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nParent node of the glossary term\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryNodeKeyClass(name)\\\\nBases\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty name DictWrapper\\\\nA metadata snapshot for a specific GlossaryNode entity.\\\\n\\\\nParameters List[GlossaryNodeKeyClass | GlossaryNodeInfoClass | OwnershipClass | StatusClass]\\\\nThe list of metadata aspects associated with the GlossaryNode. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nHas A / Is A lineage information about a glossary Term reporting the lineage\\\\n\\\\nParameters None | List[str]\\\\nThe relationship Has A with glossary term\\\\n\\\\n\\\\n\\\\nproperty isRelatedTerms None | List[str]\\\\nThe relationship isRelatedTo with glossary term\\\\n\\\\n\\\\n\\\\nproperty values DictWrapper\\\\nProperties of an applied glossary term.\\\\n\\\\nParameters None | str\\\\nAdditional context about the association\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nProperties associated with a GlossaryTerm\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty definition None | str\\\\nOptional id for the term\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nParent node of the glossary term\\\\n\\\\n\\\\n\\\\nproperty rawSchema None | str\\\\nExternal Reference to the business-term\\\\n\\\\n\\\\n\\\\nproperty sourceUrl\\\\nThe abstracted URL such as https\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty termSource Aspect\\\\nKey for a GlossaryTerm\\\\n\\\\nParameters str\\\\nThe term name, which serves as a unique id\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryTermSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[GlossaryTermKeyClass, GlossaryTermInfoClass, OwnershipClass, StatusClass, BrowsePathsClass, GlossaryRelatedTermsClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryTermsClass(terms, auditStamp)\\\\nBases\\\\n\\\\nterms (List[GlossaryTermAssociationClass])\\\\nauditStamp (AuditStampClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty auditStamp List[GlossaryTermAssociationClass]\\\\nThe related business terms\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GroupMembershipClass(groups)\\\\nBases\\\\ngroups (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty groups DictWrapper\\\\n\\\\nParameters List[str]\\\\n\\\\n\\\\n\\\\nproperty heights DictWrapper\\\\nThe checkpoint state object of a datahub ingestion run for a given job.\\\\n\\\\nParameters str\\\\nThe version of the state format.\\\\n\\\\n\\\\n\\\\nproperty payload str\\\\nThe serialization/deserialization protocol.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.InputFieldClass(schemaFieldUrn, schemaField=None)\\\\nBases\\\\n\\\\nschemaFieldUrn (str)\\\\nschemaField (Optional[SchemaFieldClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty schemaField str\\\\nUrn of the schema being referenced for lineage purposes\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.InputFieldsClass(fields)\\\\nBases\\\\nfields (List[InputFieldClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty fields Aspect\\\\nInstitutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity.\\\\n\\\\nParameters List[InstitutionalMemoryMetadataClass]\\\\nList of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.InstitutionalMemoryMetadataClass(url, description, createStamp)\\\\nBases\\\\n\\\\nurl (str)\\\\ndescription (str)\\\\ncreateStamp (AuditStampClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty createStamp str\\\\nDescription of the link.\\\\n\\\\n\\\\n\\\\nproperty url Aspect\\\\nIntended Use for the ML Model\\\\n\\\\nParameters None | List[str]\\\\nHighlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to.\\\\n\\\\n\\\\n\\\\nproperty primaryUsers None | List[str]\\\\nPrimary Use cases for the MLModel.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.IntendedUserTypeClass\\\\nBases Aspect\\\\nAspect used to store invite tokens.\\\\n\\\\nParameters None | str\\\\nThe role that this invite token may be associated with\\\\n\\\\n\\\\n\\\\nproperty token Aspect\\\\nKey for an InviteToken.\\\\n\\\\nParameters str\\\\nA unique id for the invite token.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.JobStatusClass\\\\nBases DictWrapper\\\\nThis header records information about the context of an event as it is emitted into kafka and is intended to be used by the kafka audit application.  For more information see go/kafkaauditheader\\\\n\\\\nParameters str\\\\nThe name of the application from which the event is being emitted. see go/appname\\\\n\\\\n\\\\n\\\\nproperty auditVersion\\\\nThe version that is being used for auditing. In version 0, the audit trail buckets events into 10 minute audit windows based on the EventHeader timestamp. In version 1, the audit trail buckets events as follows\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty clusterConnectionString None | str\\\\nfabric\\\\nThe fabricUrn of the host from which the event is being emitted. Fabric Urn in the format of urn\\\\n\\\\nType None | str\\\\nThe instance on the server from which the event is being emitted. e.g. i001\\\\n\\\\n\\\\n\\\\nproperty messageId str\\\\nThe fully qualified name of the host from which the event is being emitted.\\\\n\\\\n\\\\n\\\\nproperty time DictWrapper\\\\nSchema holder for kafka schema.\\\\n\\\\nParameters str\\\\nThe native kafka document schema. This is a human readable avro document schema.\\\\n\\\\n\\\\n\\\\nproperty keySchema DictWrapper\\\\nSchema text of a key-value store schema.\\\\n\\\\nParameters str\\\\nThe raw schema for the key in the key-value store.\\\\n\\\\n\\\\n\\\\nproperty valueSchema object\\\\nMLFeature Data Type\\\\n\\\\n\\\\nAUDIO = \'AUDIO\'\\\\nText Data\\\\n\\\\n\\\\n\\\\nBINARY = \'BINARY\'\\\\nCount data is discrete whole number data - no negative numbers here.\\\\nCount data often has many small values, such as zero and one.\\\\n\\\\n\\\\n\\\\nBYTE = \'BYTE\'\\\\nUnknown data are data that we don\\\\u2019t know the type for.\\\\n\\\\n\\\\n\\\\nCONTINUOUS = \'CONTINUOUS\'\\\\nBytes data are binary-encoded values that can represent complex objects.\\\\n\\\\n\\\\n\\\\nCOUNT = \'COUNT\'\\\\nTime data is a cyclical, repeating continuous form of data.\\\\nThe relevant time features can be any period- daily, weekly, monthly, annual, etc.\\\\n\\\\n\\\\n\\\\nIMAGE = \'IMAGE\'\\\\nVideo Data\\\\n\\\\n\\\\n\\\\nINTERVAL = \'INTERVAL\'\\\\nImage Data\\\\n\\\\n\\\\n\\\\nMAP = \'MAP\'\\\\nlist, tuple, range\\\\n\\\\nType\\\\nSet Data Type ex\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSET = \'SET\'\\\\nContinuous data are made of uncountable values, often the result of a measurement such as height, weight, age etc.\\\\n\\\\n\\\\n\\\\nTEXT = \'TEXT\'\\\\ndict, map\\\\n\\\\nType Aspect\\\\nKey for an MLFeature\\\\n\\\\nParameters str\\\\nNamespace for the feature\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nProperties associated with a MLFeature\\\\n\\\\nParameters None | str | MLFeatureDataTypeClass\\\\nData Type of the MLFeature\\\\n\\\\n\\\\n\\\\nproperty description None | List[str]\\\\nSource of the MLFeature\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\n\\\\nParameters List[MLFeatureKeyClass | MLFeaturePropertiesClass | OwnershipClass | InstitutionalMemoryClass | StatusClass | DeprecationClass | BrowsePathsClass | GlobalTagsClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the MLFeature. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nKey for an MLFeatureTable\\\\n\\\\nParameters str\\\\nName of the feature table\\\\n\\\\n\\\\n\\\\nproperty platform Aspect\\\\nProperties associated with a MLFeatureTable\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | List[str]\\\\nList of features contained in the feature table\\\\n\\\\n\\\\n\\\\nproperty mlPrimaryKeys DictWrapper\\\\n\\\\nParameters List[MLFeatureTableKeyClass | MLFeatureTablePropertiesClass | OwnershipClass | InstitutionalMemoryClass | StatusClass | DeprecationClass | BrowsePathsClass | GlobalTagsClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the MLFeatureTable. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nProperties associated with an ML Hyper Param\\\\n\\\\nParameters None | int\\\\nDate when the MLHyperParam was developed\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nName of the MLHyperParam\\\\n\\\\n\\\\n\\\\nproperty value Aspect\\\\nProperties associated with an ML Metric\\\\n\\\\nParameters None | int\\\\nDate when the mlMetric was developed\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nName of the mlMetric\\\\n\\\\n\\\\n\\\\nproperty value Aspect\\\\nKey for an ML model deployment\\\\n\\\\nParameters str\\\\nName of the MLModelDeployment\\\\n\\\\n\\\\n\\\\nproperty origin str\\\\nStandardized platform urn for the model Deployment\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelDeploymentPropertiesClass(customProperties=None, externalUrl=None, description=None, createdAt=None, version=None, status=None)\\\\nBases\\\\n\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndescription (Optional[str])\\\\ncreatedAt (Optional[int])\\\\nversion (Optional[VersionTagClass])\\\\nstatus (Union[None, str, DeploymentStatusClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty createdAt Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty status None | VersionTagClass\\\\nVersion of the MLModelDeployment\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelDeploymentSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[MLModelDeploymentKeyClass, MLModelDeploymentPropertiesClass, OwnershipClass, StatusClass, DeprecationClass, GlobalTagsClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelFactorPromptsClass(relevantFactors=None, evaluationFactors=None)\\\\nBases\\\\n\\\\nrelevantFactors (Optional[List[MLModelFactorsClass]])\\\\nevaluationFactors (Optional[List[MLModelFactorsClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty evaluationFactors None | List[MLModelFactorsClass]\\\\nWhat are foreseeable salient factors for which MLModel performance may vary, and how were these determined?\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelFactorsClass(groups=None, instrumentation=None, environment=None)\\\\nBases\\\\n\\\\ngroups (Optional[List[str]])\\\\ninstrumentation (Optional[List[str]])\\\\nenvironment (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty environment None | List[str]\\\\nGroups refers to distinct categories with similar characteristics that are present in the evaluation data instances.\\\\nFor human-centric machine learning MLModels, groups are people who share one or multiple characteristics.\\\\n\\\\n\\\\n\\\\nproperty instrumentation Aspect\\\\nKey for an ML model group\\\\n\\\\nParameters str\\\\nName of the MLModelGroup\\\\n\\\\n\\\\n\\\\nproperty origin str\\\\nStandardized platform urn for the model group\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelGroupPropertiesClass(customProperties=None, description=None, createdAt=None, version=None)\\\\nBases\\\\n\\\\ncustomProperties (Optional[Dict[str, str]])\\\\ndescription (Optional[str])\\\\ncreatedAt (Optional[int])\\\\nversion (Optional[VersionTagClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty createdAt Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | VersionTagClass\\\\nVersion of the MLModelGroup\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelGroupSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[MLModelGroupKeyClass, MLModelGroupPropertiesClass, OwnershipClass, StatusClass, DeprecationClass, BrowsePathsClass, GlobalTagsClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelKeyClass(platform, name, origin)\\\\nBases\\\\n\\\\nplatform (str)\\\\nname (str)\\\\norigin (Union[str, FabricTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty name str | FabricTypeClass\\\\nFabric type where model belongs to or where it was generated\\\\n\\\\n\\\\n\\\\nproperty platform Aspect\\\\nProperties associated with a ML Model\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty date None | List[str]\\\\nDeployments for the MLModel\\\\n\\\\n\\\\n\\\\nproperty description None | List[str]\\\\nList of jobs (if any) that use the model\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | List[str]\\\\nGroups the model belongs to\\\\n\\\\n\\\\n\\\\nproperty hyperParameters these are deprecated in favor of hyperParams\\\\n\\\\n\\\\n\\\\nproperty hyperParams None | List[str]\\\\nList of features used for MLModel training\\\\n\\\\n\\\\n\\\\nproperty onlineMetrics List[str]\\\\nTags for the MLModel\\\\n\\\\n\\\\n\\\\nproperty trainingJobs None | List[MLMetricClass]\\\\nMetrics of the MLModel used in training\\\\n\\\\n\\\\n\\\\nproperty type None | VersionTagClass\\\\nVersion of the MLModel\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[MLModelKeyClass, OwnershipClass, MLModelPropertiesClass, IntendedUseClass, MLModelFactorPromptsClass, MetricsClass, EvaluationDataClass, TrainingDataClass, QuantitativeAnalysesClass, EthicalConsiderationsClass, CaveatsAndRecommendationsClass, InstitutionalMemoryClass, SourceCodeClass, StatusClass, CostClass, DeprecationClass, BrowsePathsClass, GlobalTagsClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLPrimaryKeyKeyClass(featureNamespace, name)\\\\nBases\\\\n\\\\nfeatureNamespace (str)\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty featureNamespace str\\\\nName of the primary key\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLPrimaryKeyPropertiesClass(sources, description=None, dataType=None, version=None)\\\\nBases\\\\n\\\\nsources (List[str])\\\\ndescription (Optional[str])\\\\ndataType (Union[None, str, MLFeatureDataTypeClass])\\\\nversion (Optional[VersionTagClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty dataType None | str\\\\nDocumentation of the MLPrimaryKey\\\\n\\\\n\\\\n\\\\nproperty sources None | VersionTagClass\\\\nVersion of the MLPrimaryKey\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLPrimaryKeySnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[MLPrimaryKeyKeyClass, MLPrimaryKeyPropertiesClass, OwnershipClass, InstitutionalMemoryClass, StatusClass, DeprecationClass, GlobalTagsClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MapTypeClass(keyType=None, valueType=None)\\\\nBases\\\\n\\\\nkeyType (Optional[str])\\\\nvalueType (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty keyType None | str\\\\nType of the value in a map\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MediaClass(type, location)\\\\nBases\\\\n\\\\ntype (Union[str, MediaTypeClass])\\\\nlocation (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty location str | MediaTypeClass\\\\nType of content the Media is storing, e.g. image, video, etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MediaTypeClass\\\\nBases DictWrapper\\\\nKafka event for proposing a metadata change for an entity. A corresponding MetadataAuditEvent is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeEvent will be emitted instead.\\\\n\\\\nParameters None | KafkaAuditHeaderClass\\\\nKafka audit header. See go/kafkaauditheader for more info.\\\\n\\\\n\\\\n\\\\nproperty proposedDelta ChartSnapshotClass | CorpGroupSnapshotClass | CorpUserSnapshotClass | DashboardSnapshotClass | DataFlowSnapshotClass | DataJobSnapshotClass | DatasetSnapshotClass | DataProcessSnapshotClass | DataPlatformSnapshotClass | MLModelSnapshotClass | MLPrimaryKeySnapshotClass | MLFeatureSnapshotClass | MLFeatureTableSnapshotClass | MLModelDeploymentSnapshotClass | MLModelGroupSnapshotClass | TagSnapshotClass | GlossaryTermSnapshotClass | GlossaryNodeSnapshotClass | DataHubPolicySnapshotClass | SchemaFieldSnapshotClass | DataHubRetentionSnapshotClass\\\\nSnapshot of the proposed metadata change. Include only the aspects affected by the change in the snapshot.\\\\n\\\\n\\\\n\\\\nproperty systemMetadata DictWrapper\\\\nKafka event for capturing update made to an entity\\\\u2019s metadata.\\\\n\\\\nParameters None | GenericAspectClass\\\\nThe value of the new aspect.\\\\n\\\\n\\\\n\\\\nproperty aspectName This is only valid for CREATE, UPSERT, and DELETE operations.\\\\n\\\\n\\\\n\\\\nproperty auditHeader str | ChangeTypeClass\\\\nType of change being proposed\\\\n\\\\n\\\\n\\\\nproperty created None | GenericAspectClass\\\\nKey aspect of the entity being written\\\\n\\\\n\\\\n\\\\nproperty entityType None | str\\\\nUrn of the entity being written\\\\n\\\\n\\\\n\\\\nproperty previousAspectValue None | SystemMetadataClass\\\\nThe previous value of the system metadata field that has changed.\\\\n\\\\n\\\\n\\\\nproperty systemMetadata DictWrapper\\\\nKafka event for proposing a metadata change for an entity. A corresponding MetadataChangeLog is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeProposal will be emitted instead.\\\\n\\\\nParameters None | GenericAspectClass\\\\nThe value of the new aspect.\\\\n\\\\n\\\\n\\\\nproperty aspectName This is only valid for CREATE, UPSERT, and DELETE operations.\\\\n\\\\n\\\\n\\\\nproperty auditHeader str | ChangeTypeClass\\\\nType of change being proposed\\\\n\\\\n\\\\n\\\\nproperty entityKeyAspect str\\\\nType of the entity being written to\\\\n\\\\n\\\\n\\\\nproperty entityUrn None | SystemMetadataClass\\\\nA string-&gt;string map of custom properties that one might want to attach to an event\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MetricsClass(performanceMeasures=None, decisionThreshold=None)\\\\nBases\\\\n\\\\nperformanceMeasures (Optional[List[str]])\\\\ndecisionThreshold (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty decisionThreshold None | List[str]\\\\nMeasures of MLModel performance\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MySqlDDLClass(tableSchema)\\\\nBases\\\\ntableSchema (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty tableSchema Aspect\\\\nCarries information about the native CorpGroups a user is in.\\\\n\\\\nParameters List[str]\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NotebookCellClass(type, textCell=None, queryCell=None, chartCell=None)\\\\nBases\\\\n\\\\ntype (Union[str, NotebookCellTypeClass])\\\\ntextCell (Optional[TextCellClass])\\\\nqueryCell (Optional[QueryCellClass])\\\\nchartCell (Optional[ChartCellClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty chartCell None | QueryCellClass\\\\nThe query cell content. The will be non-null only when all other cell field is null.\\\\n\\\\n\\\\n\\\\nproperty textCell str | NotebookCellTypeClass\\\\nThe type of this Notebook cell\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NotebookCellTypeClass\\\\nBases Aspect\\\\nContent in a Notebook\\\\nNote\\\\ncells (Optional[List[NotebookCellClass]])\\\\n\\\\n\\\\n\\\\n\\\\nproperty cells Aspect\\\\nInformation about a Notebook\\\\nNote\\\\n\\\\ntitle (str)\\\\nchangeAuditStamps (ChangeAuditStampsClass)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty changeAuditStamps Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty title Aspect\\\\nKey for a Notebook\\\\n\\\\nParameters str\\\\nUnique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as \\\\u2018querybook.com/notebook/773\\\\u2019\\\\n\\\\n\\\\n\\\\nproperty notebookTool DictWrapper\\\\nNull field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NumberTypeClass\\\\nBases long, integer, short, etc..\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OperationClass(timestampMillis, operationType, lastUpdatedTimestamp, eventGranularity=None, partitionSpec=None, messageId=None, actor=None, customOperationType=None, numAffectedRows=None, affectedDatasets=None, sourceType=None, customProperties=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\noperationType (Union[str, OperationTypeClass])\\\\nlastUpdatedTimestamp (int)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nactor (Optional[str])\\\\ncustomOperationType (Optional[str])\\\\nnumAffectedRows (Optional[int])\\\\naffectedDatasets (Optional[List[str]])\\\\nsourceType (Union[None, str, OperationSourceTypeClass])\\\\ncustomProperties (Optional[Dict[str, str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | str\\\\nActor who issued this operation.\\\\n\\\\n\\\\n\\\\nproperty affectedDatasets None | str\\\\nA custom type of operation. Required if operationType is CUSTOM.\\\\n\\\\n\\\\n\\\\nproperty customProperties None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty lastUpdatedTimestamp None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty numAffectedRows str | OperationTypeClass\\\\nOperation type of change.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec None | str | OperationSourceTypeClass\\\\nSource Type\\\\n\\\\n\\\\n\\\\nproperty timestampMillis object\\\\nThe source of an operation\\\\n\\\\n\\\\nDATAPLATFORM = \'DATAPLATFORM\'\\\\n\\\\n\\\\n\\\\nDATAPROCESS = \'DATAPROCESS\'\\\\nRows were updated\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OperationTypeClass\\\\nBases DictWrapper\\\\nSchema holder for oracle data definition language that describes an oracle table.\\\\n\\\\nParameters str\\\\nThe native schema in the dataset\\\\u2019s platform. This is a human readable (json blob) table schema.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OrcSchemaClass(schema)\\\\nBases\\\\nschema (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty schema Aspect\\\\nCarries information about where an entity originated from.\\\\n\\\\nParameters None | str\\\\nOnly populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider.\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nEnum to define where an entity originated from.\\\\n\\\\n\\\\nEXTERNAL = \'EXTERNAL\'\\\\n\\\\n\\\\n\\\\nNATIVE = \'NATIVE\'\\\\nThe entity is external to DataHub.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OtherSchemaClass(rawSchema)\\\\nBases\\\\nrawSchema (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty rawSchema DictWrapper\\\\nOwnership information\\\\n\\\\nParameters str\\\\ncorpuserligroupname, and urnmultiProduct only corpuser is currently supported in the frontend.)\\\\n\\\\nType\\\\nli\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty source str | OwnershipTypeClass\\\\nThe type of the ownership\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OwnershipClass(owners, lastModified=None)\\\\nBases\\\\n\\\\nowners (List[OwnerClass])\\\\nlastModified (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty lastModified List[OwnerClass]\\\\nList of owners of the entity.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OwnershipSourceClass(type, url=None)\\\\nBases\\\\n\\\\ntype (Union[str, OwnershipSourceTypeClass])\\\\nurl (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty type None | str\\\\nA reference URL for the source\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OwnershipSourceTypeClass\\\\nBases object\\\\nAsset owner types\\\\n\\\\n\\\\nBUSINESSOWNER = \'BUSINESSOWNER\'\\\\nA steward, expert, or delegate responsible for the asset.\\\\n\\\\n\\\\n\\\\nCONSUMER = \'CONSUMER\'\\\\nA person or a group that has direct business interest\\\\nDeprecated! Use TECHNICALOWNER, BUSINESSOWNER, or STEWARD instead.\\\\n\\\\n\\\\n\\\\nDATAOWNER = \'DATAOWNER\'\\\\nA person or a group that overseas the operation, e.g. a DBA or SRE.\\\\nDeprecated! Use TECHNICALOWNER instead.\\\\n\\\\n\\\\n\\\\nDATASTEWARD = \'DATASTEWARD\'\\\\nNo specific type associated to the owner.\\\\n\\\\n\\\\n\\\\nDELEGATE = \'DELEGATE\'\\\\nA person, group, or service that produces/generates the data\\\\nDeprecated! Use TECHNICALOWNER instead.\\\\n\\\\n\\\\n\\\\nDEVELOPER = \'DEVELOPER\'\\\\nA person or group that is owning the data\\\\nDeprecated! Use TECHNICALOWNER instead.\\\\n\\\\n\\\\n\\\\nNONE = \'NONE\'\\\\nA person or group that is in charge of developing the code\\\\nDeprecated! Use TECHNICALOWNER instead.\\\\n\\\\n\\\\n\\\\nPRODUCER = \'PRODUCER\'\\\\nA person, group, or service that consumes the data\\\\nDeprecated! Use TECHNICALOWNER or BUSINESSOWNER instead.\\\\n\\\\n\\\\n\\\\nSTAKEHOLDER = \'STAKEHOLDER\'\\\\n\\\\n\\\\n\\\\nTECHNICALOWNER = \'TECHNICALOWNER\'\\\\nA person or group who is responsible for logical, or business related, aspects of the asset.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ParametersClass\\\\nBases DictWrapper\\\\nDefines how the data is partitioned\\\\n\\\\nParameters str\\\\nString representation of the partition\\\\n\\\\n\\\\n\\\\nproperty timePartition str | PartitionTypeClass\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PartitionTypeClass\\\\nBases DictWrapper\\\\nA DataHub Platform Event.\\\\n\\\\nParameters PlatformEventHeaderClass\\\\nHeader information stored with the event.\\\\n\\\\n\\\\n\\\\nproperty name GenericPayloadClass\\\\nThe event payload.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PlatformEventHeaderClass(timestampMillis)\\\\nBases\\\\ntimestampMillis (int)\\\\n\\\\n\\\\n\\\\n\\\\nproperty timestampMillis object\\\\nPlatform types available at LinkedIn\\\\n\\\\n\\\\nFILESYSTEM = \'FILESYSTEM\'\\\\nValue for a key value store, e.g. espresso, voldemort\\\\n\\\\n\\\\n\\\\nKEYVALUESTORE = \'KEYVALUESTORE\'\\\\nValue for a message broker, e.g. kafka\\\\n\\\\n\\\\n\\\\nMESSAGEBROKER = \'MESSAGEBROKER\'\\\\nValue for an object store, e.g. ambry\\\\n\\\\n\\\\n\\\\nOBJECTSTORE = \'OBJECTSTORE\'\\\\nValue for an OLAP datastore, e.g. pinot\\\\n\\\\n\\\\n\\\\nOLAPDATASTORE = \'OLAPDATASTORE\'\\\\nValue for other platforms, e.g salesforce, dovetail\\\\n\\\\n\\\\n\\\\nOTHERS = \'OTHERS\'\\\\nValue for a query engine, e.g. presto\\\\n\\\\n\\\\n\\\\nQUERYENGINE = \'QUERYENGINE\'\\\\nValue for a relational database, e.g. oracle, mysql\\\\n\\\\n\\\\n\\\\nRELATIONALDB = \'RELATIONALDB\'\\\\nValue for a search engine, e.g seas\\\\n\\\\n\\\\n\\\\nSEARCHENGINE = \'SEARCHENGINE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PolicyMatchConditionClass\\\\nBases DictWrapper\\\\nA criterion for matching a field with given value\\\\n\\\\nParameters str | PolicyMatchConditionClass\\\\nThe condition for the criterion\\\\n\\\\n\\\\n\\\\nproperty field List[str]\\\\nValues. Matches criterion if any one of the values matches condition (OR-relationship)\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PolicyMatchFilterClass(criteria)\\\\nBases\\\\ncriteria (List[PolicyMatchCriterionClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty criteria DictWrapper\\\\nContent stored inside a Post.\\\\n\\\\nParameters None | str\\\\nOptional description of the post.\\\\n\\\\n\\\\n\\\\nproperty link None | MediaClass\\\\nOptional media that the post is storing\\\\n\\\\n\\\\n\\\\nproperty title str | PostContentTypeClass\\\\nType of content held in the post.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PostContentTypeClass\\\\nBases Aspect\\\\nInformation about a DataHub Post.\\\\n\\\\nParameters PostContentClass\\\\nContent stored in the post.\\\\n\\\\n\\\\n\\\\nproperty created int\\\\nThe time at which the post was last modified\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nKey for a Post.\\\\n\\\\nParameters str\\\\nA unique id for the DataHub Post record. Generated on the server side at Post creation time.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PostTypeClass\\\\nBases DictWrapper\\\\nSchema holder for presto data definition language that describes a presto view.\\\\n\\\\nParameters str\\\\nThe raw schema in the dataset\\\\u2019s platform. This includes the DDL and the columns extracted from DDL.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QuantileClass(quantile, value)\\\\nBases\\\\n\\\\nquantile (str)\\\\nvalue (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty quantile str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QuantitativeAnalysesClass(unitaryResults=None, intersectionalResults=None)\\\\nBases\\\\n\\\\nunitaryResults (Optional[str])\\\\nintersectionalResults (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty intersectionalResults None | str\\\\nLink to a dashboard with results showing how the MLModel performed with respect to each factor\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QueryCellClass(cellId, changeAuditStamps, rawQuery, cellTitle=None, lastExecuted=None)\\\\nBases\\\\n\\\\ncellId (str)\\\\nchangeAuditStamps (ChangeAuditStampsClass)\\\\nrawQuery (str)\\\\ncellTitle (Optional[str])\\\\nlastExecuted (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cellId None | str\\\\nTitle of the cell\\\\n\\\\n\\\\n\\\\nproperty changeAuditStamps None | AuditStampClass\\\\nCaptures information about who last executed this query cell and when\\\\n\\\\n\\\\n\\\\nproperty rawQuery Aspect\\\\nKey for a Query\\\\n\\\\nParameters str\\\\nA unique id for the Query.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QueryLanguageClass\\\\nBases Aspect\\\\nInformation about a Query against one or more data assets (e.g. Tables or Views).\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp capturing the time and actor who created the Query.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAudit stamp capturing the time and actor who last modified the Query.\\\\n\\\\n\\\\n\\\\nproperty name str | QuerySourceClass\\\\nThe source of the Query\\\\n\\\\n\\\\n\\\\nproperty statement object\\\\nThe query was entered manually by a user (via the UI).\\\\n\\\\n\\\\nMANUAL = \'MANUAL\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QueryStatementClass(value, language=None)\\\\nBases\\\\n\\\\nvalue (str)\\\\nlanguage (Union[str, QueryLanguageClass, None])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty language str\\\\nThe query text\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QuerySubjectClass(entity)\\\\nBases\\\\nentity (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty entity Aspect\\\\nInformation about the subjects of a particular Query, i.e. the assets\\\\nbeing queried.\\\\n\\\\nParameters List[QuerySubjectClass]\\\\nOne or more subjects of the query.\\\\nIn single-asset queries (e.g. table select), this will contain the Table reference\\\\nand optionally schema field references.\\\\nIn multi-asset queries (e.g. table joins), this may contain multiple Table references\\\\nand optionally schema field references.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.RecordTypeClass\\\\nBases DictWrapper\\\\nBase class that encapsulates different retention policies.\\\\nOnly one of the fields should be set\\\\n\\\\nParameters None | TimeBasedRetentionClass\\\\n\\\\n\\\\n\\\\nproperty version Aspect\\\\nCarries information about which roles a user is assigned to.\\\\n\\\\nParameters List[str]\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.RunResultTypeClass\\\\nBases DictWrapper\\\\nSchemaField to describe metadata related to dataset schema.\\\\n\\\\nParameters None | AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this schema field.\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nFlattened name of the field. Field is computed from jsonPath field.\\\\n\\\\n\\\\n\\\\nproperty globalTags None | GlossaryTermsClass\\\\nGlossary terms associated with the field\\\\n\\\\n\\\\n\\\\nproperty isPartOfKey None | bool\\\\nFor Datasets which are partitioned, this determines the partitioning key.\\\\n\\\\n\\\\n\\\\nproperty jsonPath None | str\\\\nFor schema fields that have other properties that are not modeled explicitly,\\\\nuse this field to serialize those properties into a JSON string\\\\n\\\\n\\\\n\\\\nproperty label None | AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this schema field.\\\\n\\\\n\\\\n\\\\nproperty nativeDataType bool\\\\nIndicates if this field is optional or nullable\\\\n\\\\n\\\\n\\\\nproperty recursive SchemaFieldDataTypeClass\\\\nPlatform independent field type of the field.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SchemaFieldDataTypeClass(type)\\\\nBases\\\\ntype (Union[BooleanTypeClass, FixedTypeClass, StringTypeClass, BytesTypeClass, NumberTypeClass, DateTypeClass, TimeTypeClass, EnumTypeClass, NullTypeClass, MapTypeClass, ArrayTypeClass, UnionTypeClass, RecordTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nKey for a SchemaField\\\\n\\\\nParameters str\\\\nfieldPath identifying the schema field\\\\n\\\\n\\\\n\\\\nproperty parent DictWrapper\\\\nA metadata snapshot for a specific schema field entity.\\\\n\\\\nParameters List[SchemaFieldKeyClass]\\\\nThe list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nSchemaMetadata to describe metadata related to store schema\\\\n\\\\nParameters None | str\\\\nThe cluster this schema metadata resides from\\\\n\\\\n\\\\n\\\\nproperty created None | str\\\\nDataset this schema metadata is associated with.\\\\n\\\\n\\\\n\\\\nproperty deleted List[SchemaFieldClass]\\\\nClient provided a list of fields from document schema.\\\\n\\\\n\\\\n\\\\nproperty foreignKeys None | Dict[str, ForeignKeySpecClass]\\\\nMap captures all the references schema makes to external datasets. Map key is ForeignKeySpecName typeref.\\\\n\\\\n\\\\n\\\\nproperty hash AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty platform)\\\\n\\\\nType\\\\nli\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty platformSchema None | List[str]\\\\nClient provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath.\\\\n\\\\n\\\\n\\\\nproperty schemaName int\\\\nEvery change to SchemaMetadata in the resource results in a new version. Version is server assigned. This version is differ from platform native schema version.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SchemalessClass\\\\nBases Aspect\\\\nSiblings information of an entity.\\\\n\\\\nParameters bool\\\\nIf this is the leader entity of the set of siblings\\\\n\\\\n\\\\n\\\\nproperty siblings Aspect\\\\nSource Code\\\\n\\\\nParameters List[SourceCodeUrlClass]\\\\nSource Code along with types\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SourceCodeUrlClass(type, sourceCodeUrl)\\\\nBases\\\\n\\\\ntype (Union[str, SourceCodeUrlTypeClass])\\\\nsourceCodeUrl (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty sourceCodeUrl str | SourceCodeUrlTypeClass\\\\nSource Code Url Types\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SourceCodeUrlTypeClass\\\\nBases Aspect\\\\nThe lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.\\\\nThis aspect is used to represent soft deletes conventionally.\\\\n\\\\nParameters bool\\\\nWhether the entity has been removed (soft-deleted).\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.StringTypeClass\\\\nBases DictWrapper\\\\nA flexible carrier for structured results of an execution request.\\\\nThe goal is to allow for free flow of structured responses from execution tasks to the orchestrator or observer.\\\\nThe full spectrum of different execution report types is not intended to be modeled by this object.\\\\n\\\\nParameters str\\\\nThe content-type of the serialized value (e.g. application/json, application/json;gzip etc.)\\\\n\\\\n\\\\n\\\\nproperty serializedValue str\\\\nThe type of the structured report. (e.g. INGESTIONREPORT, TESTCONNECTIONREPORT, etc.)\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SubTypesClass(typeNames)\\\\nBases\\\\ntypeNames (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty typeNames DictWrapper\\\\nMetadata associated with each metadata change that is processed by the system\\\\n\\\\nParameters int | None\\\\nThe timestamp the metadata was observed at\\\\n\\\\n\\\\n\\\\nproperty properties None | str\\\\nThe model registry name that was used to process this event\\\\n\\\\n\\\\n\\\\nproperty registryVersion str | None\\\\nThe run id that produced the metadata. Populated in case of batch-ingestion.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TagAssociationClass(tag, context=None)\\\\nBases\\\\n\\\\ntag (str)\\\\ncontext (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty context str\\\\nUrn of the applied tag\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TagKeyClass(name)\\\\nBases\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nProperties associated with a Tag\\\\n\\\\nParameters None | str\\\\nThe color associated with the Tag in Hex. For example #FFFFFF.\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nDisplay name of the tag\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TagSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[TagKeyClass, OwnershipClass, TagPropertiesClass, StatusClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TelemetryClientIdClass(clientId)\\\\nBases\\\\nclientId (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty clientId Aspect\\\\nKey for the telemetry client ID, only one should ever exist\\\\n\\\\nParameters str\\\\nThe telemetry entity name, which serves as a unique id\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestDefinitionClass(type, json=None)\\\\nBases\\\\n\\\\ntype (Union[str, TestDefinitionTypeClass])\\\\njson (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty json str | TestDefinitionTypeClass\\\\nThe Test Definition Type\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestDefinitionTypeClass\\\\nBases Aspect\\\\nInformation about a DataHub Test\\\\n\\\\nParameters str\\\\nCategory of the test\\\\n\\\\n\\\\n\\\\nproperty definition None | str\\\\nDescription of the test\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nKey for a Test\\\\n\\\\nParameters str\\\\nUnique id for the test\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestResultClass(test, type)\\\\nBases\\\\n\\\\ntest (str)\\\\ntype (Union[str, TestResultTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty test str | TestResultTypeClass\\\\nThe type of the result\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestResultTypeClass\\\\nBases Aspect\\\\nInformation about a Test Result\\\\n\\\\nParameters List[TestResultClass]\\\\nResults that are failing\\\\n\\\\n\\\\n\\\\nproperty passing DictWrapper\\\\nText cell in a Notebook, which will present content in text format\\\\n\\\\nParameters str\\\\nUnique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as \\\\u2018querybook.com/notebook/773/?cellId=1234\\\\u2019\\\\n\\\\n\\\\n\\\\nproperty cellTitle ChangeAuditStampsClass\\\\nCaptures information about who created/last modified/deleted this Notebook cell and when\\\\n\\\\n\\\\n\\\\nproperty text DictWrapper\\\\nKeep records that are less than X seconds old\\\\n\\\\nParameters int\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TimeStampClass(time, actor=None)\\\\nBases\\\\n\\\\ntime (int)\\\\nactor (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty actor\\\\nOptional\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty time DictWrapper\\\\nTime field type. This should also be used for datetimes.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TimeWindowClass(startTimeMillis, length)\\\\nBases\\\\n\\\\nstartTimeMillis (int)\\\\nlength (TimeWindowSizeClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty length int\\\\nStart time as epoch at UTC.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TimeWindowSizeClass(unit, multiple=None)\\\\nBases\\\\n\\\\nunit (Union[str, CalendarIntervalClass])\\\\nmultiple (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty multiple str | CalendarIntervalClass\\\\nInterval unit such as minute/hour/day etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TrainingDataClass(trainingData)\\\\nBases\\\\ntrainingData (List[BaseDataClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty trainingData object\\\\nType of the transformation involved in generating destination fields from source fields.\\\\n\\\\n\\\\nBLACKBOX = \'BLACKBOX\'\\\\nField transformation expressed as Identity function.\\\\n\\\\n\\\\n\\\\nIDENTITY = \'IDENTITY\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UDFTransformerClass(udf)\\\\nBases\\\\nudf (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty udf DictWrapper\\\\nUnion field type.\\\\n\\\\nParameters None | List[str]\\\\nList of types in union type.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UpstreamClass(dataset, type, auditStamp=None, created=None, properties=None)\\\\nBases\\\\n\\\\ndataset (str)\\\\ntype (Union[str, DatasetLineageTypeClass])\\\\nauditStamp (Optional[AuditStampClass])\\\\ncreated (Optional[AuditStampClass])\\\\nproperties (Optional[Dict[str, str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty auditStamp None | AuditStampClass\\\\nAudit stamp containing who created the lineage and when.\\\\n\\\\n\\\\n\\\\nproperty dataset None | Dict[str, str]\\\\nA generic properties bag that allows us to store specific information on this graph edge.\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nUpstream lineage of a dataset\\\\n\\\\nParameters None | List[FineGrainedLineageClass]\\\\nList of fine-grained lineage information, including field-level lineage\\\\n\\\\n\\\\n\\\\nproperty upstreams DictWrapper\\\\nIf SchemaMetadata fields make any external references and references are of type com.linkedin.pegasus2avro.common.Urn or any children, this models can be used to mark it.\\\\n\\\\nParameters str\\\\nField in hosting(current) SchemaMetadata.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UsageAggregationClass(bucket, duration, resource, metrics)\\\\nBases\\\\n\\\\nbucket (int)\\\\nduration (Union[str, WindowDurationClass])\\\\nresource (str)\\\\nmetrics (UsageAggregationMetricsClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty bucket str | WindowDurationClass\\\\nBucket duration\\\\n\\\\n\\\\n\\\\nproperty metrics str\\\\nResource associated with these usage stats\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UsageAggregationMetricsClass(uniqueUserCount=None, users=None, totalSqlQueries=None, topSqlQueries=None, fields=None)\\\\nBases\\\\n\\\\nuniqueUserCount (Optional[int])\\\\nusers (Optional[List[UserUsageCountsClass]])\\\\ntotalSqlQueries (Optional[int])\\\\ntopSqlQueries (Optional[List[str]])\\\\nfields (Optional[List[FieldUsageCountsClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty fields None | List[str]\\\\nFrequent SQL queries; mostly makes sense for datasets in SQL databases\\\\n\\\\n\\\\n\\\\nproperty totalSqlQueries None | int\\\\nUnique user count\\\\n\\\\n\\\\n\\\\nproperty users DictWrapper\\\\nRecords a single user\\\\u2019s usage counts for a given resource\\\\n\\\\nParameters int\\\\n\\\\n\\\\n\\\\nproperty user None | str\\\\nIf useremail is set, we attempt to resolve the user\\\\u2019s urn upon ingest\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ValueFrequencyClass(value, frequency)\\\\nBases\\\\n\\\\nvalue (str)\\\\nfrequency (int)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty frequency str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.VersionBasedRetentionClass(maxVersions)\\\\nBases\\\\nmaxVersions (int)\\\\n\\\\n\\\\n\\\\n\\\\nproperty maxVersions Aspect\\\\nInformation about a Data processing job\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty externalUrl str\\\\nThe version which can indentify a job version like a commit hash or md5 hash\\\\n\\\\n\\\\n\\\\nproperty versionType DictWrapper\\\\nA resource-defined string representing the resource state for the purpose of concurrency control\\\\n\\\\nParameters None | str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ViewPropertiesClass(materialized, viewLogic, viewLanguage)\\\\nBases\\\\n\\\\nmaterialized (bool)\\\\nviewLogic (str)\\\\nviewLanguage (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty materialized str\\\\nThe view logic language / dialect\\\\n\\\\n\\\\n\\\\nproperty viewLogic object\\\\nEnum to define the length of a bucket when doing aggregations\\\\n\\\\n\\\\nDAY = \'DAY\'\\\\n\\\\n\\\\n\\\\nHOUR = \'HOUR\'\\\\n\\\\n\\\\n\\\\nMONTH = \'MONTH\'\\\\n\\\\n\\\\n\\\\nWEEK = \'WEEK\'\\\\n\\\\n\\\\n\\\\nYEAR = \'YEAR\'\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.metadata.schemaclasses.getschema_type(fullname)\\\\n\\\\nParameters\\\\nRecordSchema\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"overviewSidebar"},"README":{"id":"README","title":"Introduction","description":"DataHub is a data discovery application built on an extensible metadata platform that helps you tame the complexity of diverse data ecosystems."},"releases":{"id":"releases","title":"DataHub Releases","description":"Summary","sidebar":"overviewSidebar"}}}')}}]);