"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[6539],{3905:function(e,a,t){t.d(a,{Zo:function(){return u},kt:function(){return m}});var n=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),p=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},u=function(e){var a=p(e.components);return n.createElement(s.Provider,{value:a},e.children)},d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},c=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=p(t),m=r,h=c["".concat(s,".").concat(m)]||c[m]||d[m]||i;return t?n.createElement(h,o(o({ref:a},u),{},{components:t})):n.createElement(h,o({ref:a},u))}));function m(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=c;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=t[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}c.displayName="MDXCreateElement"},79024:function(e,a,t){t.r(a),t.d(a,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return u},default:function(){return c}});var n=t(87462),r=t(63366),i=(t(67294),t(3905)),o=["components"],l={title:"Airflow Integration",sidebar_label:"Airflow Integration",slug:"/lineage/airflow",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/lineage/airflow.md"},s="Airflow Integration",p={unversionedId:"docs/lineage/airflow",id:"docs/lineage/airflow",isDocsHomePage:!1,title:"Airflow Integration",description:"DataHub supports integration of",source:"@site/genDocs/docs/lineage/airflow.md",sourceDirName:"docs/lineage",slug:"/lineage/airflow",permalink:"/docs/lineage/airflow",editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/lineage/airflow.md",tags:[],version:"current",frontMatter:{title:"Airflow Integration",sidebar_label:"Airflow Integration",slug:"/lineage/airflow",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/lineage/airflow.md"},sidebar:"overviewSidebar",previous:{title:"Using DataHub",permalink:"/docs/metadata-ingestion/schedule_docs/datahub"},next:{title:"Running Airflow locally with DataHub",permalink:"/docs/docker/airflow/local_airflow"}},u=[{value:"Using Datahub&#39;s Airflow lineage plugin (new)",id:"using-datahubs-airflow-lineage-plugin-new",children:[{value:"How to validate installation",id:"how-to-validate-installation",children:[],level:3}],level:2},{value:"Using Datahub&#39;s Airflow lineage backend",id:"using-datahubs-airflow-lineage-backend",children:[],level:2},{value:"Setting up Airflow to use DataHub as Lineage Backend",id:"setting-up-airflow-to-use-datahub-as-lineage-backend",children:[],level:2},{value:"Emitting lineage via a separate operator",id:"emitting-lineage-via-a-separate-operator",children:[],level:2}],d={toc:u};function c(e){var a=e.components,t=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"airflow-integration"},"Airflow Integration"),(0,i.kt)("p",null,"DataHub supports integration of"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Airflow Pipeline (DAG) metadata"),(0,i.kt)("li",{parentName:"ul"},"DAG and Task run information as well as"),(0,i.kt)("li",{parentName:"ul"},"Lineage information when present")),(0,i.kt)("p",null,"There are a few ways to enable these integrations from Airflow into DataHub."),(0,i.kt)("h2",{id:"using-datahubs-airflow-lineage-plugin-new"},"Using Datahub's Airflow lineage plugin (new)"),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"We recommend you use the lineage plugin if you are on Airflow version >= 2.0.2 or on MWAA with an Airflow version >= 2.0.2"))),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"You need to install the required dependency in your airflow."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"pip install acryl-datahub-airflow-plugin\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Disable lazy plugin load in your airflow.cfg.\nOn MWAA you should add this config to your ",(0,i.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-env-variables.html#configuring-2.0-airflow-override"},"Apache Airflow configuration options"),"."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"core.lazy_load_plugins : False\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"You must configure an Airflow hook for Datahub. We support both a Datahub REST hook and a Kafka-based hook, but you only need one."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"# For REST-based:\nairflow connections add  --conn-type 'datahub_rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'\n# For Kafka-based (standard Kafka sink config can be passed via extras):\nairflow connections add  --conn-type 'datahub_kafka' 'datahub_kafka_default' --conn-host 'broker:9092' --conn-extra '{}'\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Add your ",(0,i.kt)("inlineCode",{parentName:"p"},"datahub_conn_id")," and/or ",(0,i.kt)("inlineCode",{parentName:"p"},"cluster")," to your ",(0,i.kt)("inlineCode",{parentName:"p"},"airflow.cfg")," file if it is not align with the default values. See configuration parameters below"),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Configuration options:")),(0,i.kt)("table",{parentName:"li"},(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Name"),(0,i.kt)("th",{parentName:"tr",align:null},"Default value"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"datahub.datahub_conn_id"),(0,i.kt)("td",{parentName:"tr",align:null},"datahub_rest_default"),(0,i.kt)("td",{parentName:"tr",align:null},"The name of the datahub connection you set in step 1.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"datahub.cluster"),(0,i.kt)("td",{parentName:"tr",align:null},"prod"),(0,i.kt)("td",{parentName:"tr",align:null},"name of the airflow cluster")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"datahub.capture_ownership_info"),(0,i.kt)("td",{parentName:"tr",align:null},"true"),(0,i.kt)("td",{parentName:"tr",align:null},"If true, the owners field of the DAG will be capture as a DataHub corpuser.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"datahub.capture_tags_info"),(0,i.kt)("td",{parentName:"tr",align:null},"true"),(0,i.kt)("td",{parentName:"tr",align:null},"If true, the tags field of the DAG will be captured as DataHub tags.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"datahub.graceful_exceptions"),(0,i.kt)("td",{parentName:"tr",align:null},"true"),(0,i.kt)("td",{parentName:"tr",align:null},"If set to true, most runtime errors in the lineage backend will be suppressed and will not cause the overall task to fail. Note that configuration issues will still throw exceptions."))))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Configure ",(0,i.kt)("inlineCode",{parentName:"p"},"inlets")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"outlets")," for your Airflow operators. For reference, look at the sample DAG in ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_backend_demo.py"},(0,i.kt)("inlineCode",{parentName:"a"},"lineage_backend_demo.py")),", or reference ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py"},(0,i.kt)("inlineCode",{parentName:"a"},"lineage_backend_taskflow_demo.py"))," if you're using the ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts/taskflow.html"},"TaskFlow API"),".")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"[optional]"," Learn more about ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/lineage.html"},"Airflow lineage"),", including shorthand notation and some automation."))),(0,i.kt)("h3",{id:"how-to-validate-installation"},"How to validate installation"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Go and check in Airflow at Admin -> Plugins menu if you can see the Datahub plugin")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Run an Airflow DAG and you should see in the task logs Datahub releated log messages like:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"Emitting Datahub ...\n")))),(0,i.kt)("h2",{id:"using-datahubs-airflow-lineage-backend"},"Using Datahub's Airflow lineage backend"),(0,i.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The Airflow lineage backend is only supported in Airflow 1.10.15+ and 2.0.2+.\nFor managed services like MWAA you should use the Datahub Airflow plugin as the lineage backend is not supported there"))),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"If you are looking to run Airflow and DataHub using docker locally, follow the guide ",(0,i.kt)("a",{parentName:"p",href:"/docs/docker/airflow/local_airflow"},"here"),". Otherwise proceed to follow the instructions below."))),(0,i.kt)("h2",{id:"setting-up-airflow-to-use-datahub-as-lineage-backend"},"Setting up Airflow to use DataHub as Lineage Backend"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"You need to install the required dependency in your airflow. See ",(0,i.kt)("a",{parentName:"li",href:"https://registry.astronomer.io/providers/datahub/modules/datahublineagebackend"},"https://registry.astronomer.io/providers/datahub/modules/datahublineagebackend"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"  pip install acryl-datahub[airflow]\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"You must configure an Airflow hook for Datahub. We support both a Datahub REST hook and a Kafka-based hook, but you only need one."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"# For REST-based:\nairflow connections add  --conn-type 'datahub_rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'\n# For Kafka-based (standard Kafka sink config can be passed via extras):\nairflow connections add  --conn-type 'datahub_kafka' 'datahub_kafka_default' --conn-host 'broker:9092' --conn-extra '{}'\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Add the following lines to your ",(0,i.kt)("inlineCode",{parentName:"p"},"airflow.cfg")," file."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-ini"},'[lineage]\nbackend = datahub_provider.lineage.datahub.DatahubLineageBackend\ndatahub_kwargs = {\n    "datahub_conn_id": "datahub_rest_default",\n    "cluster": "prod",\n    "capture_ownership_info": true,\n    "capture_tags_info": true,\n    "graceful_exceptions": true }\n# The above indentation is important!\n')),(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Configuration options:")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"datahub_conn_id")," (required): Usually ",(0,i.kt)("inlineCode",{parentName:"li"},"datahub_rest_default")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"datahub_kafka_default"),", depending on what you named the connection in step 1."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"cluster"),' (defaults to "prod"): The "cluster" to associate Airflow DAGs and tasks with.'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"capture_ownership_info")," (defaults to true): If true, the owners field of the DAG will be capture as a DataHub corpuser."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"capture_tags_info")," (defaults to true): If true, the tags field of the DAG will be captured as DataHub tags."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"capture_executions")," (defaults to false): If true, it captures task runs as DataHub DataProcessInstances. ",(0,i.kt)("strong",{parentName:"li"},"This feature only works with Datahub GMS version v0.8.33 or greater.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"graceful_exceptions")," (defaults to true): If set to true, most runtime errors in the lineage backend will be suppressed and will not cause the overall task to fail. Note that configuration issues will still throw exceptions."))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Configure ",(0,i.kt)("inlineCode",{parentName:"p"},"inlets")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"outlets")," for your Airflow operators. For reference, look at the sample DAG in ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_backend_demo.py"},(0,i.kt)("inlineCode",{parentName:"a"},"lineage_backend_demo.py")),", or reference ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py"},(0,i.kt)("inlineCode",{parentName:"a"},"lineage_backend_taskflow_demo.py"))," if you're using the ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts/taskflow.html"},"TaskFlow API"),".")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"[optional]"," Learn more about ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/lineage.html"},"Airflow lineage"),", including shorthand notation and some automation."))),(0,i.kt)("h2",{id:"emitting-lineage-via-a-separate-operator"},"Emitting lineage via a separate operator"),(0,i.kt)("p",null,"Take a look at this sample DAG:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub_provider/example_dags/lineage_emission_dag.py"},(0,i.kt)("inlineCode",{parentName:"a"},"lineage_emission_dag.py"))," - emits lineage using the DatahubEmitterOperator.")),(0,i.kt)("p",null,"In order to use this example, you must first configure the Datahub hook. Like in ingestion, we support a Datahub REST hook and a Kafka-based hook. See step 1 above for details."))}c.isMDXComponent=!0}}]);